{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Listed below are the collections of notes taken from the courses I've took.","title":"Preface"},{"location":"#introduction","text":"Listed below are the collections of notes taken from the courses I've took.","title":"Introduction"},{"location":"Term/F2021/","text":"MTH312 Differential Equations and Vector Calculus ELE302 Electronic Networks PCS224 Solid State Physics COE328 Engineering Algorithms and Data Structures","title":"Fall 2021"},{"location":"Term/W2021/","text":"ECN801 Principle of Engineering Economics MTH240 Calculus 2 ELE202 Electric Circuit Analysis PCS125 Physics: Waves and Fields","title":"Winter 2021"},{"location":"Term/W2022/","text":"MTH314 Discrete Mathematics for Engineers ELE404 Electronic Circuits I COE428 Engineering Algorithms and Data Structures COE528 Object Oriented Eng Analysis and Design","title":"Winter 2022"},{"location":"W2022/COE428/COE428/","text":"Introduction This will cover various topics for the course COE428: Algorithms and Data Structures, using the textbook, Introduction to Algorithms , by T. Cormen, C. Leiserson, R. Rive, and lectures notes provided by the professor, Dr. Reza Samavi. Other resources used: Abdul Bari - Algorithms Last Updated: 2022-04-16 Introduction to Algorithms \u00b6 Algorithm Analysis \u00b6 An algorithm is a sequence of computational step that transform the input into the output. You will typically see them displayed as pseudocode shown below: It is very convenient to classify algorithms based on the relative amount of time or relative amount of space they require and specify as a function of the input size. Thus, we have the notions of: Time Complexity: Running time of the program as a function of the size of input. Space Complexity: Amount of computer memory required during the program execution, as a function of the input size. Running Time \u00b6 The time complexity of an algorithm can be measured by characterizing running time as a function, \\(T(n)\\) , of the input size, \\(n\\) . We count the number of primitive operations that are executed: Assigning a value to a variable Calling a method Performing an arithmetic operation Comparing two values Indexing into an array Returning from a method ::: algorithm ::: algorithmic \\(temp = a\\) ; \\(a = b\\) ; \\(b = temp\\) ; ::: ::: For this example, we say the running time is \\(T(n) = 3\\) , since there's three primitive operations executed by an algorithm. When it comes to loops, it becomes a bit more complex. ::: algorithm ::: algorithmic \\(x = s[0]\\) \\(x = s[i]\\) ::: ::: We define the size of the input array to be \\(n\\) : In line 1, its indexing an array, \\(s[0]\\) , then assigning it to \\(x\\) In line 2, inside the for loop: It first assigns \\(i = 1\\) On each iteration, it makes a comparison, \\(<\\) , for \\(n\\) times In line 3, the if statement repeats for : Its indexing an array, \\(s[i]\\) , then makes a comparison, \\(>\\) In line 4, there's two possibilities that could occur: If true, then its indexing an array, \\(s[i]\\) , then assigning it to \\(x\\) If false, then it won't execute At the end, \\(i++\\) , is incremented, then assigned back to \\(i\\) The last line returns \\(x\\) The number of primitive operations executed by algorithm can be characterized in two ways: Best-Case Analysis: \\(T(n) = 2 + 1 + n + (n-1)(2 + 0 + 2) + 1 = 5n\\) Worst-Case Analysis: \\(T(n) = 2 + 1 + n + (n-1)(2 + 2 + 2) + 1 = 7n - 2\\) ... to which it is bounded by two linear functions, \\(5n \\leq T(n) \\leq 7n - 2\\) Classes of Functions \u00b6 Let's first go over some common functions that characterize the running time of an algorithm: Constant function: \\(f(n) = c\\) Linear function: \\(f(n) = n\\) n-log-n function: \\(f(n) = n\\log{n}\\) Quadratic function: \\(f(n) = n^2\\) Cubic function: \\(f(n) = n^3\\) Exponents: \\(f(n) = b^n\\) Logarithms: \\(f(n) = \\log_2{n}\\) A review from MTH is the summation formula, which will be useful when analyzing the time complexity of algorithms: Arithmetic series: \\(\\displaystyle\\sum_{k = 0}^{n} k = 1 + 2 + \\cdots + n = \\frac{n(n+1)}{2}\\) Geometric series: \\(\\displaystyle\\sum_{k = 0}^{n} x^k = 1 + x + x^2 + \\cdots + x^n = \\frac{1-x^{n+1}}{1-x}\\) ... and the following terms which you might recall from coding: \\(\\lfloor x \\rfloor\\) or floor(x) is largest integer less than to equal to \\(x\\) . \\(\\lceil x \\rceil\\) or ceiling(x) is least integer greater than to equal to \\(x\\) . Growth Rate of Running Time \u00b6 When choosing between algorithms, we care most about asymptotic performance; how the algorithm increases with the input size. ::: center ::: If you notice, as the input size increases, certain class of functions grows much more rapidly than others. ::: center ::: If we let the value of the growth-rate function represent the units of time, an algorithm with the function \\(f(n) = \\log_2{n}\\) would be much more efficient than an algorithm with the function \\(f(n) = 2^n\\) . In general, the order-of-growth can be classified to be: \\( \\(1 < \\log{n} < \\sqrt{n} < n < n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n\\) \\) Analyzing and Designing Algorithms \u00b6 In this lecture, we will go over three different types of sorting algorithm: insertion sort, merge sort, and selection sort. The purpose of sorting algorithms is to solve the following problem: Input: A sequence of \\(n\\) numbers \\(\\langle a_1, a_2, \\dots, a_n \\rangle\\) Output: A permutation (reordering) \\(\\langle a'_1, a'_2, \\dots, a'_n \\rangle\\) of the input sequence such that \\(a'_1 \\leq a'_2 \\leq \\dots \\leq a'_n\\) . ::: dBox ::: definitionT Definition 2.1 (Key). The sequence are typically stored in array. We also refer to the number as keys. ::: ::: Insertion Sort \u00b6 Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. ::: algorithm [Insertion-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: Iterate from [ A[1] ]{style=\"background-color: light-gray\"} to [ A[n] ]{style=\"background-color: light-gray\"} over the array. Compare the current element [ key = A[j] ]{style=\"background-color: light-gray\"} to its predecessor. If the element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element. ::: list Note that iterations starts at [ A[1] ]{style=\"background-color: light-gray\"}, not [ A[0] ]{style=\"background-color: light-gray\"}. For the sake of convenience, we assume a fictitious record [ A[0] ]{style=\"background-color: light-gray\"} as the sentinel value with key of \\(-\\infty\\) . ::: It maybe easier to visualize this using images to better understand the psuedocode written. Suppose we start out with the following array with 6 elements. ::: center ::: If we apply the insertion sort algorithm, the following array would be sorted as shown below. Let's denote the [ key ]{style=\"background-color: light-gray\"} in green. ::: center ::: Since we care most about the asymptotic performance, we are interested on finding the running time \\(T(n)\\) . ::: algorithm ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: The lectures note and textbook goes in-depth deriving the following running time of insertion sort: \\( \\(T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^n t_j + c_6\\sum_{j = 2}^n (t_j - 1) + c_7\\sum_{j = 2}^n (t_j - 1) + c_8(n - 1)\\) \\) where \\(t_j\\) is the number of times the while loop is executed for that value of \\(j\\) . The main takeaway is knowing how it sorts and the function for best and worst-case running time of the following algorithm is. In the next lecture, we will go more in-depth on analyzing the time complexity using asymptotic notations, which simplifies all of this stuff. Best-Case Complexity \u00b6 The best-case scenario is when the array is already sorted. ::: center ::: In this example, the while loop does the comparison but never enters the loop, since it always find that [ A[i] ]{style=\"background-color: light-gray\"} is always less than or equal to [ key ]{style=\"background-color: light-gray\"}. ::: center ::: Thus, \\(t_j = 1\\) , we can derive the number of comparisons for every outer loop iteration: \\( \\(\\sum_{j=2}^n t_j \\to \\sum_{j=2}^n 1 = (n - 2) + 1 = n - 1\\) \\) Substituting this in the equation to the running time simplifies \\(T(n)\\) to \\( \\({ \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5(n - 1) + c_8(n - 1) \\\\ &= (c_1 + c_2 + c_4 + c_5 + c_8)n - (c_2 + c_4 + 5 + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(c_n\\) simplify to some constants \\(a\\) and \\(b\\) then \\( \\(T(n) = an - b\\) \\) Worst-Case Complexity \u00b6 The worst-case scenario is when the array is sorted in reversefrom largest to smallest. ::: center ::: In this example, \\(t_j\\) has to compare with all elements to the left \\(j\\) -th positioncompare with \\(j - 1\\) elements. Thus, \\(t_j = j\\) , we can derive the number of comparisons for every outer loop iteration \\( \\(\\sum_{j = 2}^{n} t_j \\to \\sum_{j = 2}^{n} j = 2 + 3 + 4 + \\dots + n = \\bigg[\\sum_{j=1}^n j\\bigg] - 1 = \\frac{n(n+1)}{2} - 1\\) \\) and as well the number of moves inside the while loop: \\( \\(\\sum_{j = 2}^{n} (t_j - 1) \\to \\sum_{j = 2}^{n} (j - 1) = 1 + 2 + 3 + \\dots + n - 1 = \\frac{n(n-1)}{2}\\) \\) Substituting this in the equation to the running time simplifies \\(T(n)\\) to \\( \\({ \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + (c_6 + c_7)\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_8(n - 1) \\\\ &= \\bigg[\\frac{c_5}{2} + \\frac{c_6}{2} + \\frac{c_7}{2}\\bigg]n^2 + (c_1 + \\dots + c_8)n - (c_2 + \\dots + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(c_n\\) simplify to some constants \\(a\\) , \\(b\\) and \\(c\\) then \\( \\(T(n) = an^2 + bn - c\\) \\) Merge Sort \u00b6 Merge sort closely follows the divide-and-conquer paradigm. Intuitively, it operates as follows. Divide: Divide the \\(n\\) -element sequence to be sorted into two subsequences of \\(n = 2\\) elements each. Conquer: Sort the two subsequences recursively using merge sort. Combine: Merge the two sorted subsequences to produce the sorted answer. ::: algorithm [Merge-Sort]{.smallcaps} \\((A,p,r) \\to A[p \\dots r]\\) ::: algorithmic \\(q = \\lfloor (p + r)/2 \\rfloor\\) [Merge-Sort( \\(A,p,q\\) )]{.smallcaps} [Merge-Sort( \\(A,q + 1,r\\) )]{.smallcaps} [Merge( \\(A,p,q,r\\) )]{.smallcaps} ::: ::: Split the deck into two piles, until these become simple enoughan array of size \\(1\\) . Sort the left pile and sort the right pile using [ Merge-Sort() ]{style=\"background-color: light-gray\"}. Merge both piles into the final pile. ::: list In the [ Merge-Sort(A,p,r) ]{style=\"background-color: light-gray\"}, the floor function is used to determine [ q ]{style=\"background-color: light-gray\"}, so in the case there's a decimalit will result in an integer, ex. \\(\\lfloor 7.5\\rfloor = 7\\) . ::: It may also be helpful to use a diagram like before to fully understand what's happening. The number in red denotes the order in which steps are processed. ::: center ::: If you prefer are more concrete example, look at the code I wrote on the left-side, which demonstrate how recursion works in this sorting algorithm. Each indent indicates the recursion depth. Step 1 to 3: Calls [ Merge-Sort(A,p,q) ]{style=\"background-color: light-gray\"} to split the left children with different values of \\(q\\) and \\(r\\) (passing parameter by value). Step 4: Since left child can no longer split, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on right child. Step 5: If both left and right child are already split, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Step 6: Trace back to tree structure and find the node that does not complete the splitting, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on the right children. Step 7 to 8: The same process is done as for Step 3 and 4. Step 9: Like in Step 5, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Most of the steps are just repeated for the other half of the array, until all children have complete the splitting, then they are merged together. Merge Algorithm \u00b6 The key operation of the merge sort algorithm is the merging of two sorted sequences, after divide and conquer. ::: algorithm [Merge]{.smallcaps} \\((A,p,q,r) \\to A[p \\dots q]\\) and \\(A[q + 1 \\dots r]\\) where \\(p \\leq q \\leq r\\) ::: algorithmic \\(n_1 = q - p + 1\\) \\(n_2 = r - q\\) Let L[1 ... \\(n_1 + 1\\) ] and L[1 ... \\(n_2 + 1\\) ] be new arrays \\(L[i] = A[p + i - 1]\\) \\(R[j] = A[q + j]\\) \\(L[n_1 + 1] = \\infty\\) \\(R[n_2 + 1] = \\infty\\) \\(i = 1\\) \\(j = 1\\) \\(A[k] = L[i]\\) \\(i = i + 1\\) \\(j = j + 1\\) ::: ::: It may look like a lot, but it's pretty simple. Most of the code are explained in the comments listed in the right. The main focus here is the [ for ]{style=\"background-color: light-gray\"} loop in Line 12 to 17. ::: center ::: The heavily shaded elements in [ A ]{style=\"background-color: light-gray\"} contain values that will be copied over, and heavily shaded elements in [ L[] ]{style=\"background-color: light-gray\"} and [ R[] ]{style=\"background-color: light-gray\"} contain values that have already been copied back into [ A[] ]{style=\"background-color: light-gray\"}. The lightly shaded elements in [ A[] ]{style=\"background-color: light-gray\"} indicate their final value. Time Complexity \u00b6 Let's discuss the time complexity of the following algorithm, which we can break down to the divide-and-conquer paradigm. The time to split deck takes can be denoted by \\(c_1\\) , as it takes constant timedoes not depend on any input. The time to sort left pile and sort right pile can be denoted by \\(2T(n/2)\\) , due to recursion, where the size is now divided by two, \\(n/2\\) . The time to merge piles can be denoted by \\(c_2n + c_3\\) , as it takes linear timeonly the [ for ]{style=\"background-color: light-gray\"} loop depends on input size, while the rest take constant time, thus simplified to that. The time complexity results to \\( \\(T(n) = c_1 + T(n/2) + T(n/2) + c_2n + c_3\\) \\) Our goal is to determine the most rapidly growing term in \\(T(n)\\) and so we can set a few rules. We set constants \\(c_n\\) to either: \\(0\\) , if they will not be significant in the most rapidly growing term or ... \\(1\\) , if they will be For \\(T(n)\\) , when \\(n > 1\\) , we can set \\(c_1\\) and \\(c_3\\) to \\(0\\) and \\(c_2\\) to \\(1\\) , which simplifies to: \\( \\(T(n) = \\begin{cases}c_1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) ::: list Note that when \\(n = 1\\) , the code inside [ if ... else ]{style=\"background-color: light-gray\"} wont run, as there's only one element \\(p \\nless r\\) or \\(1 \\nless 1\\) , so we write it as a constant. ::: In order to solve the recurrence, when \\(n > 1\\) , we need a base case, so for simplicity, \\(T(1) = 0\\) and we can make a deduction from it. ::: tabu c | c c | c \\(n\\) & \\(T(n/2)\\) & \\(2T(n/2) + n\\) & \\(n\\log_2{n}\\) \\ \\(2\\) & \\(0\\) & \\(2(0) + 2 = 2\\) & \\(2\\) \\ \\(4\\) & \\(2\\) & \\(2(2) + 4 = 8\\) & \\(8\\) \\ \\(8\\) & \\(8\\) & \\(2(8) + 8 = 24\\) & \\(24\\) \\ \\(16\\) & \\(24\\) & \\(2(24) + 16 = 64\\) & \\(64\\) \\ \\(32\\) & \\(64\\) & \\(2(64) + 32 = 160\\) & \\(160\\) \\ ::: Examining the numbers allows us to form an educated guess it is growing by a function of \\(n\\log_2{n}\\) , which can also be deducted by drawing a recursion tree. We start by representing \\(T(n) = 2T(n/2) + n\\) as a graph where we put the non-recursive part ( \\(n\\) in this case) on the top row and put each recursive part on a row below. ::: center ::: Then expand downwards for the next level. ::: center ::: Repeat the same process. Eventually, it will reach a certain height which it reaches the base case and stop. ::: center ::: ::: {#height_avl} If you notice the sum of the non-recursive elements for each level is \\(n\\) . Let's denote the depth or height of the tree as \\(h\\) and so we can say the time complexity is ::: \\( \\(T(n) = n \\times h\\) \\) It will eventually reach the base case which we set to some constant when \\(n=1\\) , where \\(T(1) = 1\\) . We can rewrite the fraction in terms of the depth, \\(h\\) , where \\( \\(\\frac{n}{2^h} = 1 \\to n = 2^h \\to h = \\log_2{n}\\) \\) Thus, the time complexity is \\( \\(T(n) = n\\log_2{n}\\) \\) If you recall the order-of-growth from Lecture 2, we know that \\(n\\log{n} < n^2\\) , and so merge sorting beats insertion sort in the worst-case scenario, as it grows much more slowly. Selection Sort \u00b6 The final sorting algorithm will cover is selection sort. ::: algorithm [Selection-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Select the first element as [ min ]{style=\"background-color: light-gray\"} Compare [ min ]{style=\"background-color: light-gray\"} with the second element. If the second element is smaller than minimum, assign the second element as [ min ]{style=\"background-color: light-gray\"}. Repeat until last element. After each iteration, [ min ]{style=\"background-color: light-gray\"} is placed in the front of the unsorted list. For each iteration, indexing starts from the first unsorted element. The steps are repeated until sorted. Let's use the example as we did for insertion sort, which is the following array with 6 elements. ::: center ::: The first iteration would like something like this. Let's denote the [ min ]{style=\"background-color: light-gray\"} in green and the line to the element it's being compared to. The arrow indicates a swap to be made. ::: center ::: In the next iteration, the first unsorted element is [ A[2] ]{style=\"background-color: light-gray\"}, so it starts at \\(3\\) . ::: center ::: Then it is repeated until all the elements are placed at their correct positions. Time Complexity \u00b6 As we have covered for the other algorithm, let's analyze the time complexity of selection sort. ::: algorithm ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Combining each one of them, we get the following running time of selection sort: \\( \\(T(n) = c_1n + c_2(n-1) + c_3\\sum_{j=2}^n j + c_4\\sum_{j = 2}^n (j - 1) + c_5(n-1)\\) \\) As we have previously done with insertion sort, we can simplify the summation using the arithmetic series \\( \\({ \\begin{split} T(n) &= c_1n + c_2(n-1) + c_3\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + c_4\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_5(n-1) \\\\ &= \\bigg[\\frac{c_3}{2}+\\frac{c_4}{2}\\bigg]n^2 + (c_1 + \\dots + c_5)n - (c_2 + \\dots + c_5) \\end{split}}\\) \\) or equivalently, if we let \\(c_n\\) simplify to some constants \\(a\\) , \\(b\\) and \\(c\\) then \\( \\(T(n) = an^2 + bn - c\\) \\) Comparing it to the other two algorithms discussed, selection sort is on par with insertion sort in the worst-case scenario and so merge sorting is better than selection sort as well. Complexity Analysis \u00b6 Asymptotic Notations \u00b6 As covered briefly in the growth rate of running time, it's hard to determine which algorithm is better with no prior knowledge of the input size, so we consider the asymptotic behavior of the two functions for very large input size \\(n\\) . We use specific notations called asymptotic notations to express mathematical properties of asymptotic efficiency. ::: dBox ::: definitionT Definition 3.1 (Asymptotic efficiency). The study of how the running time of an algorithm increases as the size of the input increases without bound. ::: ::: There are three asymptotic notations, which will go over in this lecture: Big-Oh notation, \\(\\text{O}()\\) , for the upper bound or worst-case complexity Big-Omega notation, \\(\\Omega()\\) , for the lower bound or best-case complexity Theta notation, \\(\\Theta()\\) , for the average bound or average-case complexity We can apply these to the previous lecture, which we covered three different sorting algorithms with varying time complexity: Algorithm Time Complexity Best Worst Insertion Sort \\(\\Omega(n)\\) \\(\\text{O}(n^2)\\) Merge Sort \\(\\Omega(n\\log{n})\\) \\(\\text{O}(n\\log{n})\\) Selection Sort \\(\\Omega(n^2)\\) \\(\\text{O}(n^2)\\) -3ex -0.1ex -.4ex 0.5ex .2ex Big-Oh Notation (O-notation) The notation represents the upper bound of the running time of an algorithm. Thus, it gives the worst-case complexity of an algorithm. \\( \\({ \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Some tips for determining \\(\\text{O}()\\) complexity: Ignore the constants: \\( \\(5n \\to n\\) \\) Certain terms dominate other, which we ignore lower order terms: \\( \\(\\text{O}(1) < \\text{O}(\\log{n}) < \\text{O}(n) < \\text{O}(n\\log{n}) < \\text{O}(n^2) < \\cdots < \\text{O}(2^n) < \\cdots < \\text{O}(n!) < \\text{O}(n^n)\\) \\) It might be easier to understand if we have examples to determine the big-Oh notation. ::: exampleT Example 3.1 . Determine the upper bound \\(\\text{O}()\\) for \\(f(n)\\) : \\(f_A(n) = an^2 + bn + c\\) is \\(\\text{O}(n^2)\\) \\(f_B(n) = 2n + 3\\) is \\(\\text{O}(n)\\) \\(f_C(n) = 5 + (15 \\cdot 20)\\) is \\(\\text{O}(1)\\) \\(f_D(n) = n^2\\log{n} + n\\) is \\(\\text{O}(n^2\\log{n})\\) ::: ::: list When writing the big-Oh notation, try to write the closest function to the running time. While the function \\(\\text{O}(n^2)\\) is true for \\(f_B(n)\\) , the function \\(\\text{O}(n)\\) is the closest to \\(f_B(n)\\) . ::: The rules for determining the \\(\\text{O}()\\) complexity are as listed: If \\(g(n) = \\text{O}(G(n))\\) and \\(f(n) = \\text{O}(F(n))\\) , then: \\( \\(f(n) + g(n) = \\text{O}(F(n)) + \\text{O}(G(n)) = \\text{O}(\\text{max}[F(n), G(n)])\\) \\) \\( \\(f(n) \\cdot g(n) = \\text{O}(F(n)) \\cdot \\text{O}(G(n)) = \\text{O}(F(n) \\cdot G(n))\\) \\) If \\(g(n) = \\text{O}(kG(n))\\) , where \\(k\\) is a constant, then \\(g(n) = \\text{O}(G(n))\\) . If \\(f(n)\\) is a polynomial of degree \\(d\\ (P(n) = \\sum_{i=0}^d a_in^i\\) where \\(a_d \\neq 0)\\) , then \\(f(n)\\) is \\(\\text{O}(n^d)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Big-Omega Notation ( \\(\\Omega\\) -notation) The notation represents the lower bound of the running time of an algorithm. Thus, it provides the best-case complexity of an algorithm. \\( \\({ \\begin{aligned} \\Omega(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq cg(n) \\leq f(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: The rules for determining \\(\\text{O}()\\) complexity is also true for determining the \\(\\Omega()\\) complexity. Let's use an example from before. ::: exampleT Example 3.2 . Determine the lower bound \\(\\Omega()\\) for \\(f(n) = 2n + 3\\) : If we look at the order-of-growth for functions, \\(T(n)\\) belongs to the linear function, \\(n\\) and if we define our lower and upper bounds as such ( \\({\\overunderbraces{&\\br{2}{\\text{Lower bound}}}% {&1 < \\log{n} < \\sqrt{n} <& n &< n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n&} {& &\\br{2}{\\text{Upper bound}}}}\\) \\) So the lower bound can be defined by any of the following: ( \\(\\Omega(1) < \\Omega(\\log{n}) < \\Omega(n)\\) \\) Similar to the upper bound, we want the function closest to \\(f(n)\\) and so \\(f(n) = 2n + 3\\) is \\(\\Omega(n)\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Theta Notation ( \\(\\Theta\\) -notation) The next notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm, it is used for analyzing the average-case complexity of an algorithm. \\( \\({ \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c_1,\\ c_2, \\text{ and } n_0 \\text{ such that } \\\\ &\\ 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Equivalently, \\(f(n)\\) is \\(\\Theta(g(n))\\) if and only if \\(f(n)\\) is both \\(\\text{O}(g(n))\\) and \\(\\Omega(g(n))\\) . One notable example which we used previously is the function \\(f(n) = 2n + 3\\) , which as we demonstrated in previous notations are \\(\\text{O}(n)\\) and \\(\\Omega(n)\\) , thus \\(f(n)\\) is \\(\\Theta(n)\\) . Complexity of Code Structures \u00b6 Loops are considered as dynamic if they depend on input size, otherwise they are static statements, everything within a loop is considered as static statementtakes a constant amount of time, \\(\\text{O}(1)\\) . The complexity is determined by: number of iterations in the loops \\(\\times\\) number of static statement -3ex -0.1ex -.4ex 0.5ex .2ex For Loop The following example is a simple for loop: for (int i = 0; i < n; i++) { // statement } The for loop is a dynamic statement, as it depends on the size of \\(n\\) . We are interested in the amount of times [ statement ]{style=\"background-color: light-gray\"} runs, which determines the time complexity of the following loop. Suppose \\(n = 3\\) then let's determine how many iterations: ::: tabu c c c Iteration & \\(i\\) &\\ & i = 0 &\\ 2 & i = 1 &\\ 3 & i = 2 &\\ 4 & i = 3 &\\ ::: You can see that the loop executes \\(3\\) times or in general, we can say \\(n\\) times. Thus, the time complexity is: \\( \\(n \\cdot 1 = \\text{O}(n)\\) \\) Note that there might be few variations of the for loop. Suppose there are also consecutive statements: for (int i = 0; i < n; i++) { // statement } for (int i = 1; i <= n; i++) { // statement } In both examples, the loop executes for \\(n\\) times. When we have consecutive statements, we would just add them together. If you recall, we ignore any constants of lower order terms. Thus, the time complexity is: \\( \\(\\underbrace{n \\cdot 1}_{\\substack{\\text{The first} \\\\ \\text{for loop}}} + \\underbrace{n \\cdot 1}_{\\substack{\\text{The second} \\\\ \\text{for loop}}} = 2n = \\text{O}(n)\\) \\) Note that this is not always the case for every for loop, as it depends on the initialization, condition test, and update statement. Suppose we have the following for loop to analyze: for (int i = 1; i <= n; i = i * 2) { // statement } Let's list out each iterations of the loop, till \\(k\\) iterations, since we do not know how many times this loop will execute. ::: tabu c c c Iteration & \\(i\\) \\ & i = 1 & \\(2^0\\) \\ 2 & i = 2 & \\(2^1\\) \\ 3 & i = 4 & \\(2^2\\) \\ \u22ee& \u22ee\\ \\(k\\) & i = \\(2^{k - 1}\\) &\\ ::: From the condition, we know that the loop will terminate once [ i > n ]{style=\"background-color: light-gray\"}. So we assume \\(i = n\\) , when it has reach \\(k\\) iterations; our very last iteration. Then we will solve for \\(k\\) : \\( \\({ \\begin{split} 2^{k - 1} &= n \\\\ k - 1 &= \\log_2{n} \\\\ k &= \\log_2{n} + 1 \\end{split}}\\) \\) If you recall from earlier, we ignore lower order terms. Thus, the time complexity is: \\( \\((\\log_2{n} + 1) \\cdot 1 = \\text{O}(\\log_2{n})\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Nested Loop The following example is a nested for loop: for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement } } As we covered earlier, the following for loop executes \\(n\\) times. Suppose now we have an inner loop, which also executes \\(n\\) times, then the statement is run \\(n \\times n\\) times. Thus, the time complexity is: \\( \\((n \\cdot n) \\cdot 1 = \\text{O}(n^2)\\) \\) The general formula for a nested loop is the time complexity of the outer loop times the inner loops. This also applies if we have a outer while loop with an inner for loop. -3ex -0.1ex -.4ex 0.5ex .2ex If Else Statement The following example is an if else statement: if (n == 0) { // statement 1 } else { for (int i = 0; i < n; i++) { // statement 2 } } If you notice, there's two possibilities that could occur: the if part, where[ statement 1 ]{style=\"background-color: light-gray\"} will run once, \\(\\text{O}(1)\\) or the else part, where [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(n\\) times, \\(\\text{O}(n)\\) . In general, the time complexity of an if else statement is: \\( \\(\\text{O}(if-else) = \\text{O}\\Big(\\text{max}\\Big[\\text{O}(\\text{condition1}), \\text{O}(\\text{condition2}), \\dots, \\text{O}(\\text{branch}1), \\text{O}(\\text{branch2}), \\dots\\Big]\\Big)\\) \\) As we are typically interested in the worst-cases, we only consider the branch with the largest running time. The condition runs once and then we add whichever is larger, which is the else part, thus, the time complexity is: \\( \\(1 + n = \\text{O}(n)\\) \\) or equivalently \\( \\(\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(1), \\text{O}(n)\\Big]\\Big) = \\text{O}(n)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Switch Statement The following example is a switch statement: switch (key) { case 'a': for (int i = 0; i < n; i++) { // statement 1 } case 'b': for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement 2 } } default: // statement 3 break; } Similar to the if else statement, we only consider the case with the largest running time, including the default case. In this example, for [ case \u2019b\u2019 ]{style=\"background-color: light-gray\"}, [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(n^2\\) times, \\(\\text{O}(n^2)\\) . Thus, the time complexity is: \\( \\(1 + n^2 = \\text{O}(n^2)\\) \\) or equivalently \\( \\(\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(n), \\text{O}(n^2), \\text{O}(1)\\Big]\\Big) = \\text{O}(n^2)\\) \\) Recurrence Equations \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction In Lecture 2, we described the worst-case running time \\(T(n)\\) of merge-sort procedure by the recurrence: \\( \\(T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) whose solution we claimed to be \\(T(n) = \\Theta(n\\log n)\\) . Previously, we didn't really have a general method for finding the form of recurrences. Our goal for this lecture is to go in-depth in ways we can analyze recursive algorithms and form a general formula. ::: dBox ::: definitionT Definition 4.1 (Recursive algorithm). An algorithm which calls itself to solve smaller problems. ::: ::: Recurrence can be polymorphic, meaning it can take many forms: A recursive algorithm which divides to two problem with equal sizes. \\( \\(T(n) = 2T(n/2) + \\Theta(n)\\) \\) A recursive algorithm might divide subproblems into unequal sizes. \\( \\(T(n) = T(2n/3) + T(n/3) + \\Theta(n)\\) \\) They are not necessarily constrained to being a constant fraction of the original problem size. \\( \\(T(n) = T(n-1) + \\Theta(1)\\) \\) -4ex -1ex -.4ex 1ex .2ex Finding the Asymptotic Bounds There are three methods for solving recurrencesthat is, for obtaining asymptotic \" \\(\\Theta\\) \" or \" \\(\\text{O}\\) \" bounds on the solution: Substitution Method Recursion-Tree Method Master Method -3ex -0.1ex -.4ex 0.5ex .2ex Substitution Method This method is powerful, but we must be able to guess the form of the answer in order to apply it. It comprises of the following steps: Step 1: Try a few substitutions to find a pattern. Step 2: Guess the recurrence formula after \\(k\\) iterations (in terms of \\(k\\) and \\(n\\) ). Step 3: Set \\(k\\) so we get the base case. Step 4: Put \\(k\\) back into the formula to find a potential closed form. Step 5: Prove the potential closed form using induction. Using the merge-sort algorithm as an example, which has the following recurrence: \\( \\(T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) Let's go step-by-step, as described. The easiest way to find a pattern, is by simply writing out the first few iterations. Let's denote \\(k\\) as our number of iterations starting from \\(1\\) . \\( \\({\\begin{split} k &= 1 & T(n) &= 2T(n/2) + n \\\\ k &= 2 & T(n) &= 2\\Big[2T(n/4) + n/2\\Big] + n = 4 \\cdot T(n/4) + 2n \\\\ k &= 3 & T(n) &= 2\\bigg[2\\Big[2T(n/8) + n/4\\Big] + n/2\\bigg] + n = 8 \\cdot T(n/8) + 3n \\\\ \\end{split}}\\) \\) Our goal is to generalize this for \\(k\\) iterations. In other words, relating each of the constants to \\(k\\) . We can rewrite it as such \\( \\({\\begin{split} k &= 1 & T(n) &= 2^1 \\cdot T(n/2^1) + 1n \\\\ k &= 2 & T(n) &= 2^2 \\cdot T(n/2^2) + 2n \\\\ k &= 3 & T(n) &= 2^3 \\cdot T(n/2^3) + 3n \\\\ \\end{split}}\\) \\) Thus, we can form a general formula, using in terms of \\(k\\) and \\(n\\) \\( \\(T(n) = 2^k \\cdot T(n/2^k) + kn\\) \\) We know the base case is set to \\(T(1) = 1\\) . From our general formula, we can determine how many iterations there are in terms of \\(n\\) to reach the base case, by solving for \\(k\\) . \\( \\({\\begin{split} \\frac{n}{2^k} = 1 \\ \\to\\ n &= 2^k \\\\ k &= \\log_2{n} \\end{split}}\\) \\) Substituting \\(k = \\log_2{n}\\) back into the general formula, we get a potential closed form, as \\(T()\\) is no longer inside our formula. \\( \\({\\begin{split} T(n) &= 2^{\\log_2{n}} \\cdot T(n/2^{\\log_2{n}}) + n\\log_2{n} \\\\ &= n + n\\log_2{n} \\end{split}}\\) \\) We can guess that the solution is \\(T(n) = \\text{O}(n\\log{n})\\) . However, we need a definite proof that this is true, by using mathematical induction for the following statement. \\( \\(0 \\leq T(n) \\leq cn\\log{n} \\hspace{1cm} \\exists c > 0,\\ \\forall n \\geq n_0\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Mathematical Induction In order to prove something is true, we use mathematical induction. We must show that we can choose the constant \\(c\\) large enough so that \\(T(n) \\leq cn\\log{n}\\) is true. Remember, the base case is \\(T(1) = 1\\) . Then for \\(n = 1\\) , it yields \\(T(1) \\nleq c(1)\\log{1} = 0\\) . Consequently, the base case fails to hold, so what now? For asymptotic notation we can specify a specific bound, \\(\\forall n \\geq n_0\\) , where \\(n_0\\) is something we can choose. Thus \\(n_0 = 2\\) , removing it from consideration in the induction proof. The induction proof consists of three parts: the base case, inductive hypothesis and inductive step. Let's assume \\(n\\) is some power of \\(2\\) or \\(n = 2^k\\) , for sake of convenience. Base Case: Let \\(k = 1\\) or \\(n = 2\\) then: \\( \\(T(2) = 2T(1) + 2 = 2 + 2 = 4 \\leq c(2)\\log{2}\\) \\) We can see that the inequality holds true for the base case, such that there \\(c \\geq 2\\) . Inductive hypothesis: We will now assume that our proposition, \\(T(n) = \\text{O}(n\\log{n})\\) , holds true for \\(k -1\\) , which equivalently is \\(n/2\\) , therefore: \\( \\(T(n/2) \\leq c(n/2)\\log{(n/2)}\\) \\) To prove the inductive step, one assumes the induction hypothesis for \\(k-1\\) and then uses this assumption to prove that the statement holds for \\(k\\) . If instead, we assume our hypothesis to hold for \\(k\\) , then we must prove it holds for \\(k+1\\) . Inductive step: From our hypothesis, prove the guess of correct for \\(k\\) . Using the following: \\( \\(T(n) = 2T(n/2) + n\\) \\) Since we know \\(T(n/2) \\leq c(n/2)\\log{(n/2)}\\) , then we can rewrite it as such: \\( \\({\\begin{split} T(n) &\\leq 2\\Big[c(n/2)\\log{(n/2)}\\Big] + n \\\\ &\\leq cn\\log{(n/2)} + n = cn\\log{n} - cn\\log{2} + n \\\\ &\\leq cn\\log{n} + (1 - c)n \\\\ &\\leq cn\\log{n}\\qquad (\\forall c \\geq 1) \\end{split}}\\) \\) From the inductive step, we proved that proposition is true as we found that there exists some value of \\(c\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Recursion-Tree Method In a recursion tree, we sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to determine the total cost of all levels of the recursion. Step 1: Start by substituting the parent with non-recursive part of the formula and adding child nodes for each recursive part. Step 2: Expand each node repeating the step above, until you reach the base case. We already covered how to do this using merge-sort algorithm, so let's start off simple, by using the following recurrence: \\( \\(T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ T(n-1) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) As usual, let's go step-by-step. The non-recursive part, \\(n\\) , will be the parent node and the recursive part, \\(T(n-1)\\) , will be the child node. The costs within each level is displayed in the right-hand side. ::: center ::: Expand on \\(T(n-1)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(T(1) = 1\\) . The fully expanded tree has height \\(n\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the arithmetic series, thus: \\( \\(T(n) = 1 + \\cdots + (n-2) + (n-1) + n = \\frac{n(n+2)}{2} = \\Theta(n^2)\\) \\) Suppose you consider something a bit more complex, which divides the subproblems into unequal sizes, for the following recurrence: \\( \\(T(n) = T(n/4) + T(n/2) + n^2\\) \\) The non-recursive part, \\(n^2\\) , will be the parent node and the recursive part, \\(T(n/4)\\) and \\(T(n/2)\\) , will be the child nodes. ::: center ::: Expand on \\(T(n/4)\\) and \\(T(n/2)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(T(1)\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the geometric series, thus: \\( \\(T(n) = n^2\\bigg[1 + \\Big[\\frac{5}{16}\\Big] + \\Big[\\frac{5}{16}\\Big]^2 + \\cdots\\bigg] = \\Theta(n^2)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Master Method The master method provides a \"cookbook\" method for solving recurrences of the form: \\( \\(T(n) = aT(n/b) + f(n)\\) \\) where \\(a \\geq 1\\) , \\(b > 1\\) and \\(f(n)\\) be a function of \\( \\(f(n) = n^k\\log^p{n}\\) \\) Note that there are various variations of the master theorem, but this is definition is what I found the easiest to understand. It consists of memorizing these three cases: Case One: If \\(\\log_b{a} > k\\) , then \\(T(n) = \\Theta(n^{\\log_b{a}})\\) . Case Two: If \\(\\log_b{a} = k\\) and ... (a) \\(p > -1\\) , then \\(T(n) = \\Theta(n^k\\log^{p+1}{n})\\) . (b) \\(p = -1\\) , then \\(T(n) = \\Theta(n^k\\log({\\log{n}}))\\) . (c) \\(p < -1\\) , then \\(T(n) = \\Theta(n^k)\\) . Case Three: If \\(\\log_ba < k\\) and ... (a) \\(p \\geq 0\\) , then \\(T(n) = \\Theta(n^k\\log^{p}{n})\\) . (b) \\(p < 0\\) , then \\(T(n) =\\Theta(n^k)\\) . It looks a bit complicated at first glance, but once we get to the examples, it becomes quite easy. ::: exampleT Example 4.1 . Suppose we have the following recurrence: ( \\(T(n) = 2T(n/2) + 1\\) \\) We know \\(a = 2\\) and \\(b = 2\\) , but how do we get \\(k\\) and \\(p\\) ? We can rewrite it in the form of \\(n^k\\log^p{n}\\) : ( \\(f(n) = 1 = n^0\\log^0{n}\\) \\) You can confirm that both equations are identical, thus \\(k = 0\\) and \\(p = 0\\) . Since \\(\\log_2{2} > k\\) , then \\(T(n) = \\Theta(n^{\\log_b{a}})\\) . Substituting in the values for \\(a\\) and \\(b\\) , we get: ( \\(T(n) = \\Theta(n^{\\log_2{2}}) = \\Theta(n)\\) \\) ::: You can refer to this [video]{.underline} for more examples covering the three cases. Elementary Data Structures \u00b6 -4ex -1ex -.4ex 1ex .2ex Stacks Stacks are dynamic sets in which the element removed from the set by the delete operation is prespecified. What defines a stack is that it implements a last-in, first-out (LIFO) principle, so only the top element is accessible. ::: center ::: There are three main methods on a stack: [ push(S,x) ]{style=\"background-color: light-gray\"} - Inserts an object \\(\\colorbox{light-gray}{\\texttt{x}}\\) onto top of Stack [ S ]{style=\"background-color: light-gray\"}. [ pop(S) ]{style=\"background-color: light-gray\"} - Removes the top object of stack [ S ]{style=\"background-color: light-gray\"}; if the stack is empty, an error occurs. [ top(S) ]{style=\"background-color: light-gray\"} - Returns the top object of the stack [ S ]{style=\"background-color: light-gray\"}, without removing it; if the stack is empty, an error occurs. ::: center ::: The following support methods should also be defined: [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in stack [ S ]{style=\"background-color: light-gray\"}. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if stack [ S ]{style=\"background-color: light-gray\"} is empty. We can implement each of the stack operations with just a few lines of code: ::: algorithm [Stack-Empty]{.smallcaps} \\((S)\\) ::: algorithmic \\(\\textsc{True}\\) \\(\\textsc{False}\\) ::: ::: ::: algorithm [Push]{.smallcaps} \\((S,x)\\) ::: algorithmic \\(S\\,.\\,top = S\\,.\\,top + 1\\) \\(S[S\\,.\\,top ] = x\\) ::: ::: ::: algorithm [Pop]{.smallcaps} \\((S)\\) ::: algorithmic \\\"underflow\\\" \\(S\\,.\\,top = S\\,.\\,top - 1\\) \\(S[S\\,.\\,top + 1]\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a stack [ S ]{style=\"background-color: light-gray\"} with \\(7\\) elements. Let \\(S\\,.\\,top\\) be a pointer to keep track of the last element (or top). When \\(S\\,.\\,top = 0\\) , there is no elements and is empty, so stack [ S ]{style=\"background-color: light-gray\"} has \\(0\\) elements. ::: center ::: When we call [ push(S,15) ]{style=\"background-color: light-gray\"}, \\(S\\,.\\,top\\) moves up by \\(1\\) and inserts element \\(15\\) to the stack. ::: center ::: Suppose we call the following: [ push(S,6) ]{style=\"background-color: light-gray\"}, [ push(S,2) ]{style=\"background-color: light-gray\"} and [ push(S,3) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ pop(S) ]{style=\"background-color: light-gray\"}, \\(S\\,.\\,top\\) moves down by \\(1\\) and returns the element that was removed, which is element \\(3\\) . ::: center ::: Although element \\(3\\) still appears in the array, it is no longer in the stack. When we call [ push(S,9) ]{style=\"background-color: light-gray\"}, \\(S\\,.\\,top\\) moves up by \\(1\\) and overwrites element \\(3\\) with \\(9\\) . ::: center ::: If you notice, when pushing an element or popping an element off the stack, it takes a constant amount of time. Let \\(n\\) be the numbers of elements in the stack. Each operation runs in time \\(\\text{O}(1)\\) . The space used is \\(\\text{O}(n)\\) . There are a few limitations we must consider: The maximum size of the stack must be defined priority and cannot be changed. When pushing a new element into a full stack, it causes an implementation error. -4ex -1ex -.4ex 1ex .2ex Queue Queue are another type of dynamic sets, which implements first-in, first-out (FIFO) principle, so queue items are removed in exactly the same order as they were added to the queue. ::: center ::: There are exist the following operations on a queue: [ enqueue(Q,x) ]{style=\"background-color: light-gray\"} - Inserts an element \\(\\colorbox{light-gray}{\\texttt{x}}\\) at the rear of the queue [ Q ]{style=\"background-color: light-gray\"}. [ dequeue(Q) ]{style=\"background-color: light-gray\"} - Removes the element at the front of queue [ Q ]{style=\"background-color: light-gray\"}. [ front() ]{style=\"background-color: light-gray\"} - Returns the front element of the queue without removing it. [ new() ]{style=\"background-color: light-gray\"} - Creates an empty queue. [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in queue. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if queue is empty. ::: center ::: Assume \\(n = Q\\,.\\,length\\) . The pseudocode for enqueue and dequeue is shown below: ::: algorithm [Enqueue]{.smallcaps} \\((Q,x)\\) ::: algorithmic \\(Q[Q\\,.\\,tail] = x\\) \\(Q\\,.\\,tail = 1\\) ::: ::: ::: algorithm [Dequeue]{.smallcaps} \\((Q)\\) ::: algorithmic \\(x = Q[Q\\,.\\,head]\\) \\(Q\\,.\\,head = 1\\) ::: ::: Note that we didn't account for the error when underflow and overflow occurs. -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a queue [ Q ]{style=\"background-color: light-gray\"} with \\(7\\) elements. Let \\(Q\\,.\\,head\\) be a pointer for the front of the queue and \\(Q\\,.\\,tail\\) be the back of the queue. When \\(Q\\,.\\,head = Q\\,.\\,tail\\) , there is no elements, so queue [ Q ]{style=\"background-color: light-gray\"} has \\(0\\) elements. ::: center ::: When we call [ enqueue(Q,15) ]{style=\"background-color: light-gray\"}, element \\(15\\) is added to the queue then \\(Q\\,.\\,tail\\) moves up by \\(1\\) . ::: center ::: Suppose we call the following: [ enqueue(Q,6) ]{style=\"background-color: light-gray\"}, [ enqueue(Q,2) ]{style=\"background-color: light-gray\"} and [ enqueue(Q,9) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ dequeue(Q) ]{style=\"background-color: light-gray\"}, element \\(15\\) located at the front of queue indicated by \\(Q\\,.\\,head\\) , is removed then \\(Q\\,.\\,head\\) moves up by \\(1\\) to element \\(6\\) . ::: center ::: As the final scenario, suppose we filled the array from \\(Q[2 .. 7]\\) , as shown below. ::: center ::: When we call \\(\\colorbox{light-gray}{\\texttt{enqueue(Q,x)}}\\) or add one more element, \\(Q\\,.\\,tail\\) will have to move up by one where \\(Q\\,.\\,head = Q\\,.\\,tail\\) . ::: center ::: But, if you recall, this means the queue is empty, which is not the case and so the queue overflows. Similar to a stack, when enqueueing or dequeueing an element, it takes a constant amount of time. Let \\(n\\) be the numbers of elements in the queue. Each operation runs in time \\(\\text{O}(1)\\) . There are also a few limitations we must consider which carries over for queue: The maximum size of the stack must be defined priority and cannot be changed. If we attempt to dequeue an element from an empty queue, the queue underflows. If we attempt to enqueue an element from a full queue, the queue overflows and so we can only store \\(n - 1\\) elements. -4ex -1ex -.4ex 1ex .2ex Linked Lists A collection of nodes that together form a linear ordering. Unlike an array, however, in which the linear order is determined by the array indices, the order in a linked list is determined by a pointer in each object. It consists of: A sequence of nodes Each node contains a value and link reference to some other node The last node contains a null link -3ex -0.1ex -.4ex 0.5ex .2ex Singly Linked Lists The most basic of all linked data structures, which are used to implement stacks and queues. Each node has data and a pointer to the next node. ::: center ::: Searching a singly linked list. ::: algorithm [List-Search]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,head\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: To search a list of \\(n\\) elements, the [List-Search]{.smallcaps} procedure takes \\(\\Theta(n)\\) time in the worst-case, since it may have to search the entire listsimilar to insertion sort. Inserting into a singly linked list. The [List-Insert]{.smallcaps} procedure splices the inserted element, [ x ]{style=\"background-color: light-gray\"}, onto the front of the linked list. ::: center ::: The running time for [List-Insert]{.smallcaps} on a list of \\(n\\) elements is \\(\\text{O}(1)\\) . Deleting from a singly linked list. The [List-Delete]{.smallcaps} procedure removes an element, [ x ]{style=\"background-color: light-gray\"}, from a linked list by getting a pointer to \\(\\colorbox{light-gray}{\\texttt{x}}\\) and it splices [ x ]{style=\"background-color: light-gray\"} out of the list by updating pointers. ::: center ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\text{O}(n)\\) time is required in the worst case. Some applications of singly linked lists are: Implement stacks and queues, as shown below. Dynamic memory allocation, which will cover in the very end. -3ex -0.1ex -.4ex 0.5ex .2ex Doubly Linked Lists We add a pointer to the previous node. Thus, we can go in either direction: forward or backward. ::: center ::: Searching a doubly linked list. A singly and linked list uses the same algorithm for searching. Thus, both take \\(\\Theta(n)\\) times in the worst-case to search through a list of \\(n\\) elements. <!-- --> Inserting into a doubly linked list. The [List-Insert]{.smallcaps} procedure is also similar to the singly, but now we also have to account for the previous pointer. ::: center ::: ::: algorithm [List-Insert]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,head\\) \\(L\\,.\\,head\\,.\\,prev = x\\) \\(L\\,.\\,head = x\\) \\(x\\,.\\,prev =\\) [nil]{.smallcaps} ::: ::: The running time for [List-Insert]{.smallcaps} on a list of \\(n\\) elements is \\(\\text{O}(1)\\) . Deleting from a doubly linked list. Likewise, same thing can be said for the [List-Delete]{.smallcaps} procedure, in which we now have to also assign the previous pointer ::: center ::: ::: algorithm [List-Delete]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(L\\,.\\,head = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\text{O}(n)\\) time is required in the worst case. Some applications of doubly linked lists are: Browsers to implement backward and forward navigation of visited web pagesthe back and forward button. Various application to implement Undo and Redo functionality. -3ex -0.1ex -.4ex 0.5ex .2ex Circularly Linked Lists A circularly singly linked list is a variation of a linked list in which the last element is linked to the first element. This forms a circular loop. ::: center ::: A circularly doubly linked list, in which in addition to the one above, the first element is linked to the last element. ::: center ::: In a circularly linked list, we used a sentinelrepresented by the dark grey node [ L.nil ]{style=\"background-color: light-gray\"}. ::: center ::: It represents [nil]{.smallcaps} which lies between the head and tail. It functions like any other object in a doubly linked list, which it has a pointer from the previous and next node. Below are the procedures used for circularly doubly linked list with sentinel. ::: algorithm [List-Search/]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,nil\\,.\\,next\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: ::: algorithm [List-Insert']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,nil\\,.\\,next\\) \\(L\\,.\\,nil\\,.\\,next\\,.\\,prev = x\\) \\(L\\,.\\,nil\\,.\\,next = x\\) \\(x\\,.\\,prev = L\\,.\\,nil\\) ::: ::: ::: algorithm [List-Delete']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: Some applications of circularly linked lists are: Useful for implementation of queue. Circular lists are useful in applications to repeatedly go around the list. Circular doubly linked lists are used for implementation of advanced data structures like Fibonacci Heap. -3ex -0.1ex -.4ex 0.5ex .2ex Implementing Pointers and Objects We can implement pointers and objects in languages that do not provide them by synthesizing them from arrays and array indices. For this example, let's use the following doubly linked list: ::: center ::: Single-array representation of objects. Analogous to storing an object in the memory. ::: center ::: Each object is represented by a contiguous sub-array of length \\(3\\) . The three attributes [ key ]{style=\"background-color: light-gray\"}, [ next ]{style=\"background-color: light-gray\"}, and [ prev ]{style=\"background-color: light-gray\"} correspond to the offsets: \\(0\\) , \\(1\\) , and \\(2\\) of the sub-array. Multiple-array representation of objects. We can represent a collection of objects that have the same attributes by using an array for each attribute. ::: center ::: You can think of each column (or vertical slice) as a single object. The pointers resides in the [ next ]{style=\"background-color: light-gray\"} and [ prev ]{style=\"background-color: light-gray\"} array, which point to the index where the next object resides. Allocating and freeing objects. To insert a key into a dynamic set represented by a doubly linked list, we must allocate a pointer to a currently unused object in the linked-list representation. ::: center ::: We keep the free objects in a singly linked list (only [ next ]{style=\"background-color: light-gray\"} pointer), which we call the free list. The free list acts like a stackthe next object allocated is the last one freed. -4ex -1ex -.4ex 1ex .2ex Heaps The (binary) heap data structure is an array of object that we can view as a nearly complete binary tree. Each node of the tree corresponds to an element of the array. ::: center ::: There are two kinds of binary heap. In both kinds, the values in the nodes satisfy a heap property, the specifics of which depend on the kind of heap. Max-heap. The max-heap property is that for every node \\(i\\) other than the root: \\( \\(A[\\textsc{Parent}(i)] \\geq A[i]\\) \\) which means that a child node can't have a greater value than its parent. Min-heap. The min-heap property is the opposite, which for every node \\(i\\) other than the root: \\( \\(A[\\textsc{Parent}(i)] \\leq A[i]\\) \\) which means that a parent node can't have a greater value than its child nodes. If all the nodes satisfy the heap property, then a binary tree is a heap. However, if a node does not have the heap property, the node is swapped with the parent. This operation is called sifting up. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Constructing a Heap A heap can be stored as an array \\(A\\) , where the: Root of tree is \\(A[1]\\) . The cell at index \\(0\\) is not used, thus we start at index \\(1\\) . Parent of \\(A[i]\\) is \\(A[\\lfloor i/2 \\rfloor]\\) . Left child of \\(A[i]\\) is \\(A[2i]\\) . Right child of \\(A[i]\\) is \\(A[2i + 1]\\) . To construct a heap: Start with a single node. Add a node to the right of the rightmost node in the deepest level. If the deepest level is full, start a new level. Each time we add a node, we may destroy heap property of its parent node. To fix this, sift up until either: We reach nodes whose values don't need to be swappedthe parent node is larger than both children. We reach the root. Suppose we have an array \\(A = [8, 10, 5, 12, 14]\\) , we would construct the heap as such: ::: center ::: Our final heap should look like this: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Maintaining Heap Property To implement this: Represent an arbitrary array as a binary tree. Devise a [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm that maintains the heap property of any given node \\(i\\) in the heap with sub-trees \\(l\\) and \\(r\\) rooted at \\(i\\) th children, given to be heaps. Devise a [ Build-Max-Heap() ]{style=\"background-color: light-gray\"} algorithm that uses [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm to construct a heap. ::: algorithm [Max-Heapify]{.smallcaps} \\((A,n)\\) ::: algorithmic ::: ::: ::: algorithm [Build-Max-Heap]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The worst-case time complexity of: [Max-Heapify]{.smallcaps} is \\(\\text{O}(\\log{n})\\) [Build-Max-Heap]{.smallcaps} is \\(\\text{O}(n)\\) The heapsort algorithm is based on the heap data structure, which uses these two main parts: building a max-heap and sorting it, to sort ::: algorithm [Heapsort]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: Thus, heapsort has a worst-case time complexity of \\(\\text{O}(n\\log{n})\\) like merge sort, but heapsort has a space complexity of \\(\\text{O}(1)\\) , since it sorts in-place, taking a constant amount of memory. -3ex -0.1ex -.4ex 0.5ex .2ex Priority Queue One of the most popular implementations of a heap, a priority queue is a data structure for maintaining a set \\(S\\) of elements, each with an associated value called a key. As with heaps, there are two kinds of priority queues: max-priority queue and min-priority queue. ::: center ::: We will focus here on how to implement max-priority queues, which are in turn based on max-heaps. A max-priority queue supports dynamic-set operations: [ Insert(S,x) ]{style=\"background-color: light-gray\"} - Inserts element [ x ]{style=\"background-color: light-gray\"} into set [ S ]{style=\"background-color: light-gray\"}. [ Maximum(S) ]{style=\"background-color: light-gray\"} - Returns an element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Extract-Max(S) ]{style=\"background-color: light-gray\"} - Removes and returns element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Increase-Key(S,x,k) ]{style=\"background-color: light-gray\"} - Increases value of element [ x ]{style=\"background-color: light-gray\"}'s key to [ k ]{style=\"background-color: light-gray\"}. Assume [ k \\geq x ]{style=\"background-color: light-gray\"}'s current key value. The procedure [Heap-Maximum]{.smallcaps} has a running time of \\(\\Theta(1)\\) . ::: algorithm [Heap-Maximum]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Extract-Max]{.smallcaps} has a running time of \\(\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Extract-Max]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Increase-Key]{.smallcaps} has a running time of \\(\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Increase-Key]{.smallcaps} \\((A,i,key)\\) ::: algorithmic ::: ::: The procedure [Max-Heap-Insert]{.smallcaps} has a running time of \\(\\text{O}(\\log{n})\\) . ::: algorithm [Max-Heap-Insert]{.smallcaps} \\((A,key)\\) ::: algorithmic ::: ::: In summary, a heap can support any priority-queue operation on a set of size \\(n\\) in \\(\\text{O}(\\log{n})\\) time. Hash Tables \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction Many applications require a dynamic set that supports only the dictionary operations. ::: dBox ::: definitionT Definition 6.1 (Dictionary). A data structure that stores (key, value) pairs and supports the operations [Insert]{.smallcaps}, [Search]{.smallcaps}, and [Delete]{.smallcaps}. ::: ::: So far we have seen a couple ways to implement dictionaries, such as linked lists. Now we will learn how to use a hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex How It Works? A hash tables takes a key (typically a string of characters or numbers) and passes it through a hash function to convert it into an index of the array to store the associated value. ::: center ::: Suppose you need to find a value of the key, you do not need to iterate through all items in the collection, because you can just use the hash function to easily find the index. Using a hash table offers a very fast lookup for a value based on the key, which should be the \\(\\text{O}(1)\\) operation. It is a generalization of an ordinary array. -3ex -0.1ex -.4ex 0.5ex .2ex Sample Problem As an example, you can think of a phone book. In the phone book, a person's name can be considered as a key, by which we can find a phone number. Case One: The simple and straightforward way to lookup number is to check all names in the phone book until we find a matching name. The worst-case search time is \\(\\text{O}(n)\\) . Case Two: Use a hash function that helps us to lookup entries much faster. Suppose we have a person's name \\\"James Davis\\\" with the phone number \\\"416-999-1234\\\". A hash function takes the key and maps it to an integer that is within the size of the array: \\( \\(\\text{String}\\ \\Rightarrow\\ \\boxed{\\text{Hash Function}}\\ \\Rightarrow\\ \\text{Index}\\) \\) Then it stores the value of the phone number to an index of the array. If we continue to add more people, it would map each one to an index of the array. ::: center ::: If we wanna lookup a person's phone number, all we need is the person's name and we can easily find the index it is stored in the array, by passing it through a hash function. Obviously, this is a watered-down explanation and doesn't go in-depthlike the possibility when two or more keys hash to the same slot. But before moving further, let's understand how direct-address table works to see the benefits of using hash tables instead. -3ex -0.1ex -.4ex 0.5ex .2ex Direct Address Table With an ordinary array, we store element whose key is \\(k\\) in position \\(k\\) of the array. ::: center ::: ::: dBox ::: definitionT Definition 6.2 (Direct addressing). Given a key \\(k\\) , we find the element whose key is \\(k\\) by just looking in the \\(k\\) th position of the array. ::: ::: A direct-address table (DAT) uses the keys as indices of the array and stores the values at those bucket locations. ::: center ::: It does facilitate fast searching, fast inserting and fast deletion operations: Inserting or deleting an element in the table, is the same as you would do for an array, hence we can do that in \\(\\text{O}(1)\\) time as we already know the index (via key). Searching an element takes \\(\\text{O}(1)\\) times, as we can easily access an element in an array in linear time if we already know the index of that element. Direct addressing is applicable when we can afford to allocate an array with one position for every possible key, and so it comes at a cost: It cannot handle collisionstwo keys are equal and contain different values. It is not recommended using the direct address table if the key values are very large. It has serious disadvantages, making it not suitable for the practical usage of current world scenarios, which is why we make use of hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Table As a recap, from the introduction, instead of storing an element with key \\(k\\) in index \\(k\\) , we use a hash function \\(h\\) and store the element in index \\(h(k)\\) . ::: center ::: ::: dBox ::: definitionT Definition 6.3 (Hash function). A hash function \\(h\\) maps all possible keys to the slots of an array \\(T[0 \\dots n - 1]\\) . ::: ::: While hash table offer the same time complexity of \\(\\text{O}(1)\\) when we talk about insertion, deletion, or searching an element, the main focus is in its ability to maintains the size constraint. The problem with DAT is if the universe \\(U\\) of keys is large, storing a table of size of \\(|U|\\) may be impractical or impossible. Often, the set of keys \\(K\\) actually stored is small, compared to \\(U\\) . ::: problem Problem 6.1 . Suppose we have a key of \\(7898\\) , which in turn is a large number. ::: Case One: Using a DAT table, we would need a huge array, for the key in index \\(7898\\) to store the value at \\(T[7898]\\) . In turn, we are wasting too much space, as most of the allocated space for the array is wasted. Case Two: But, in the case of a hash table, we can process this key via a hash function. The hash function \\(h(7898)\\) maps it to an index within the hash table \\(T[0 \\dots n - 1]\\) . Regarding the size of the hash table \\(n\\) it typically varies, as it depends in part on choice of the hash function and collision resolution, where a situation might arise when two or more keys hash to the same slot. -4ex -1ex -.4ex 1ex .2ex Hash Function A good hash function should minimizes collision as mush as possible. It is usually specified as the composition of two functions \\(h(k) = h_2\\big(h_1(k)\\big)\\) : Hash code. \\(h_1: \\text{keys}\\ \\to\\ \\text{integers}\\) Compression function. \\(h_2: \\text{integers}\\ \\to\\ [0 \\dots n - 1]\\) ::: center ::: The goal of the hash function is to disperse the keys in an apparently random way. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Code As mentioned previously, keys can be a string of characters. Thus, if the keys are not natural numbers, we find a way to interpret them as natural numbers. Some popular hash code maps are: Summing hash code. By adding up the [ASCII values]{.underline} of each letters in a string, we get an integer in return. For example, if the key is \\\"stop\\\": \\( \\(h_1(\"stop\") = 115 + 116 + 111 + 112\\) \\) However, this is not suitable for strings cause two different strings can have the same set of letters, but have different meaning\\\"post\\\", \\\"tops\\\", and \\\"pots\\\" will have the same hash code. Polynomial hash code. A better hash code takes into account the position of each character. Using the example from before: \\( \\(h_1(\"stop\") = (115 \\times a^0) + (116 \\times a^1) + (111 \\times a^2) + (112 \\times a^3)\\) \\) where \\(a\\) is a non-zero constantcompared to \\\"post\\\", \\\"tops\\\", and \\\"pots\\\", all have unique hash codes, which is ideal. -3ex -0.1ex -.4ex 0.5ex .2ex Compression Function The hash code typically returns a large range of integers and so the compression functions maps it in the range \\([0 \\dots n - 1]\\) , the indices of the hash table. There's two methods: Division Method. A simple-modulo based compression rule: \\( \\(h_2(k) = k\\ \\text{mod}\\ n\\) \\) The size \\(n\\) of the hash table is usually chosen to be a prime number, to help spread out the distribution of hash values. MAD Method. The Multiply-Add-Divide method still use \\(\\text{mod}\\ n\\) to get the numbers in the range, but a little fancier by spreading the numbers out first: \\( \\(h_2(k) = [(ak + b)\\ \\text{mod}\\ p]\\ \\text{mod}\\ n\\) \\) The values \\(a\\) and \\(b\\) are chosen at random as positive integers and \\(p\\) is a prime number, where \\(p > n\\) . With the addition of \\((ak + b)\\ \\text{mod}\\ p\\) , it eliminates patterns provided by \\(k\\ \\text{mod}\\ n\\) . Both incorporate the modulo operator, as it guarantees the output to be within the size of the hash table. Suppose we have a key of \\(7898\\) from the previous example and a hash table with \\(23\\) slots: \\( \\(h_2(7898) = 7898\\ \\text{mod}\\ 23 = 9\\) \\) Then the key will be mapped to index \\(9\\) of the hash table. -4ex -1ex -.4ex 1ex .2ex Collision Handling Collision occurs when different elements are mapped to the same index of the arraywhen \\(h(k_1) = h(k_2)\\) , but \\(k_1 \\neq k_2\\) . ::: center ::: Avoiding collision is ideal, nonetheless, it is impossible, so we use closed or open addressing to overcome this problem. Each of them have their pros and cons. -3ex -0.1ex -.4ex 0.5ex .2ex Closed Addressing Closed addressing (or open hashing) is also known as separate chaining. When collision occurs, the index keeps a reference to a linked list or dynamic array that stores all items with the same index. Let \\(e_1\\) and \\(e_2\\) represent the values attached to \\(k_1\\) and \\(k_2\\) respectively. ::: center ::: Separate chaining is fairly simple to implement and faster than open addressing in general. However, it is memory inefficient as it requires a secondary data structure and longs chains can result in \\(\\text{O}(n)\\) times. -3ex -0.1ex -.4ex 0.5ex .2ex Open Addressing Instead of referencing to a list or an array, open addressing (or closed hashing) resolves collision by searching for another empty bucket. ::: center ::: There's three types of open addressing: Linear Probing. When collision occurs, we linearly probe for the next bucket by increasing the index linearly until it finds an empty bucket: \\( \\(\\text{Index} = \\big[h(k) + i\\big]\\ \\text{mod}\\ n\\) \\) where \\(i\\) increases by one each iteration, until it finds an empty bucket. Quadratic Probing. Similar to the previous one, but instead we increase the index quadratically until it finds an empty bucket: \\( \\(\\text{Index} = \\big[h(k) + i^2\\big]\\ \\text{mod}\\ n\\) \\) where \\(i\\) increases by one each iteration, until it finds an empty bucket. Double Hashing. Using a secondary hash function \\(h'(k)\\) , it places the colliding item in the first available cell by: \\( \\(\\text{Index} = \\big[h(k) + jh'(k)\\big]\\ \\text{mod}\\ n\\) \\) where \\(j\\) increases by one each iteration, until it finds an empty bucket. The secondary hash function cannot have zero values and is typically written as such: \\( \\(h'(k) = q -(k\\ \\text{mod}\\ q)\\) \\) where \\(q\\) is a prime number, such that \\(q > n\\) . Unlike separate chaining, open addressing is more memory efficient, as it stores element in empty indices. However, it can create cluster: Linear probing can result in primary clustering. Quadratic probing can result in secondary clustering. Compared to the two, double hashing distributes the keys more evenly and produces a uniform distribution of records throughout the hash table. Trees \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction A tree is a dynamic set of nodes storing elements in a parent-child relationship (edge) with the following properties: It has a special node called root. Each node different from the root has a parent node. There is a single unique path along the edges from the root to any particular nodedoesn't have any cycles. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Tree Terminology In a tree, we often refer to certain parts of tree, which are listed below. For reference: ::: center ::: Root: The top element with no parent ( \\(A\\) ). Siblings: Children of the same parent ( \\(G, H\\) both have the parent \\(C\\) ). External node: Also referred to as leave, ndoes with no children ( \\(E, I, J, K, G, H\\) ). Internal node: nodes with one or more children ( \\(A, B, C, F\\) ). Ancestors: A node that is connected to all lower-level node ( \\(A, B, F\\) are ancestors of \\(I, J, K\\) ). Descendants: The connected lower-level nodes ( \\(I\\) is a descendant of \\(A, B, F\\) ). Depth of a node: Number of ancestors ( \\(I\\) has a depth of \\(3\\) ). Height of a tree: The max node depth (The height of tree is \\(3\\) ). Sub-tree: A tree consisting of a node and all its descendants (Refer to the red triangle above). -3ex -0.1ex -.4ex 0.5ex .2ex Tree Traversals A traversal is defined as a systematic way of accessing or visiting all nodes of a tree. Let's use the following tree as an example: ::: center ::: There's three ways a tree can be traverse, but we'll only go over two of them. The last one will be covered in the next section. Preorder traversal. Root is visited first and then sub-trees rooted at its children are visited recursively ( \\(A \\to B \\to D \\to E \\to C \\to F \\to G\\) ). Postorder traversal. Recursively traverse the sub-trees rooted at children and then visit the root itself ( \\(D \\to E \\to F \\to G \\to B \\to C \\to A\\) ). -4ex -1ex -.4ex 1ex .2ex Binary Search Tree Search trees are designed to support efficient search operations, including [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps}. A binary tree is a tree with the following: Each internal node has at most two children. The children of a node are an ordered pairleft child, right child and left sub-tree, right sub-tree. The keys satisfy the binary-search tree property: \\(u.key \\leq v.key \\leq w.key\\) Node \\(u\\) is a node (any node) in the left sub-tree of node \\(v\\) . Node \\(w\\) is a node (any node) in the right sub-tree of node \\(v\\) . ::: center ::: In other words, the value of the key of the parent should be between the value of the key of the left child and right child. A binary search tree (BST) is organized, as the name suggests, in a binary tree, where [ root[T] ]{style=\"background-color: light-gray\"} points to the root of tree [ T ]{style=\"background-color: light-gray\"} and each node contains the fields: [ key ]{style=\"background-color: light-gray\"} (and possibly other satellite data) [ left ]{style=\"background-color: light-gray\"} which points to left child. [ right ]{style=\"background-color: light-gray\"} which points to right child. [ p ]{style=\"background-color: light-gray\"} which points to parent, where [ p[root[T]] = nil ]{style=\"background-color: light-gray\"} -3ex -0.1ex -.4ex 0.5ex .2ex Inorder Traversal The binary-search tree property allows us to print out all the keys in sorted tree by a simple recursive algorithm, called an inorder tree walk, which can be visualized as such: ::: center ::: \\( \\(D \\to B \\to E \\to A \\to F \\to C \\to G\\) \\) How [Inorder-Tree-Walk]{.smallcaps} works: Check to make sure that [ x ]{style=\"background-color: light-gray\"} is not [ nil ]{style=\"background-color: light-gray\"}. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s left sub-tree. Print [ x ]{style=\"background-color: light-gray\"}'s key. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. ::: algorithm [Inorder-Tree-Walk]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Querying Binary search tree can support such queries as [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} operations. The [Tree-Search]{.smallcaps} procedure starts at the root and traces a simple path downward in the tree. The running time is \\(\\text{O}(h)\\) , where \\(h\\) is the height of the tree. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: The [Iterative-Tree-Search]{.smallcaps} is more efficient in which works by \\\"unrolling\\\" the recursion into a while loop. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: The binary search-tree property guarantees that: the leftmost node is the minimum key of the binary search tree the rightmost node is the maximum key of the binary search tree Thus, the [Tree-Minimum]{.smallcaps} and [Tree-Maximum]{.smallcaps} procedure traverse the appropriate points until [nil]{.smallcaps} is reached. The running time for both is \\(\\text{O}(h)\\) . ::: algorithm [Tree-Minimum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: ::: algorithm [Tree-Maximum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: Before going over the procedure for successor and predecessor, let's define what it means. Assuming all keys are unique, if \\(x\\) has two children: The successor is the minimum value in its right sub-tree. The predecessor is the maximum value in its left sub-tree. Refer to this example using the key value of \\(25\\) : ::: center ::: If you recall from earlier, when we performed inorder traversal, we can find the successor and predecessor based entirely on the tree structure. ::: algorithm [Tree-Successor]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: We can break the code for [Tree-Successor]{.smallcaps} into two cases: If [ x.right ]{style=\"background-color: light-gray\"} is non-empty, then [ x ]{style=\"background-color: light-gray\"}'s successor is the minimum in [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. If [ x.right ]{style=\"background-color: light-gray\"} is empty, then go up the tree until the current node is a left child. If you cannot go up further and you reached root, then [ x ]{style=\"background-color: light-gray\"} is the largest element. For example, if we want to find the successor of the key value of \\(20\\) : The right sub-tree is empty, so we go up the tree to the key value of \\(19\\) . Since \\(20\\) is not a left child or located in the left sub-tree of \\(19\\) , go up the tree to the key value of \\(15\\) . Likewise, it is not a left child of \\(15\\) , so go up the tree to the key value of \\(25\\) . The key value of \\(25\\) has \\(20\\) as a left child, therefore, the successor of \\(20\\) is \\(25\\) . Refer to the diagram below: ::: center ::: The [Tree-Predecessor]{.smallcaps} procedure is symmetric to [Tree-Predecessor]{.smallcaps} procedure, which instead uses [ x\u2006.\u2006left ]{style=\"background-color: light-gray\"}. The running time for both is \\(\\text{O}(h)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion The operations of insertion and deletion cause the dynamic set represented by a binary search tree to change. Thus, the binary-search tree property must hold after this change. The [Tree-Insert]{.smallcaps} procedure works quite similar to [Tree-Search]{.smallcaps} and [Iterative-Tree-Search]{.smallcaps}, which begins at the root of the tree. ::: algorithm [Tree-Insert]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: In the code, we are trying to insert [ z ]{style=\"background-color: light-gray\"} to the tree [ T ]{style=\"background-color: light-gray\"}: The pointer [ x ]{style=\"background-color: light-gray\"} traces a simple path downward looking for a [nil]{.smallcaps} to replace with the input [ z ]{style=\"background-color: light-gray\"}. The trailing pointer [ y ]{style=\"background-color: light-gray\"} maintains the parent of [ x ]{style=\"background-color: light-gray\"}. Suppose we want to insert an item with key \\(9\\) . The while loop in lines 3-8 can be expressed as: ::: center ::: The [nil]{.smallcaps} occupies the position where we wish to place the input item [ z ]{style=\"background-color: light-gray\"}. The lines 10-15 set the pointers that cause [ z ]{style=\"background-color: light-gray\"} to be inserted. Deletion is somewhat more tricky than insertion. The process for deleting node [ z ]{style=\"background-color: light-gray\"} can be broken into three cases: Case One: If [ z ]{style=\"background-color: light-gray\"} has no children, then we simply remove it by modifying it's parent to replace [ z ]{style=\"background-color: light-gray\"} with [nil]{.smallcaps}. ::: center ::: Case Two: If [ z ]{style=\"background-color: light-gray\"} has one child, then delete [ z ]{style=\"background-color: light-gray\"} by making the parent of [ z ]{style=\"background-color: light-gray\"} point to [ z ]{style=\"background-color: light-gray\"}'s child, instead of [ z ]{style=\"background-color: light-gray\"}. ::: center ::: Case Three: If [ z ]{style=\"background-color: light-gray\"} has two children, then delete [ z ]{style=\"background-color: light-gray\"}'s successor, [ y ]{style=\"background-color: light-gray\"}, from the tree (via Case One or Case Two) and replace [ z ]{style=\"background-color: light-gray\"}'s key and satellite data with [ y ]{style=\"background-color: light-gray\"}. ::: center ::: The [Tree-Delete]{.smallcaps} procedure executes the three cases as follows. The running time is \\(\\text{O}(h)\\) . ::: algorithm [Tree-Delete]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: As you may have notice, the running time for these operations: [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps} all take \\(\\text{O}(h)\\) , where \\(h\\) is the height of the tree. These operations are fast if the height of the tree is small. For binary search trees given we have \\(n\\) items, the minimum height of a binary tree can be \\(\\log{n}\\) and the maximum be \\(n\\) . It can be depicted as such: ::: center ::: Ideally we want make sure the height of the binary tree is always \\(\\log{n}\\) , as it provides the worst-case running time of \\(\\text{O}(\\log{n})\\) , thus comes the motivation for the next topic. -3ex -0.1ex -.4ex 0.5ex .2ex Balanced Search Tree One way we can ensure our tree is always balanced is by implementing a self-balancing binary search tree. A search-tree data structure for which a height of \\(\\log{n}\\) is guaranteed when implementing dynamic set of \\(n\\) items. AVL Tree Red-Black Tree It ensures the \\(\\text{O}(\\log{n})\\) time complexity at all times, by maintaining the binary-search tree property and height-balance property of the tree, whenever insertion or deletion is performed. -4ex -1ex -.4ex 1ex .2ex Red-Black Trees Red-black trees are one of many search-tree schemes that are \"balanced\" in order to guarantee that basic dynamic-set operations take \\(\\text{O}(\\log{n})\\) time in the worst case. ::: center ::: It is a binary tree that satisfies the following red-black properties: Every node is either red or black. The root and leaves ([nil]{.smallcaps}) are black. If a node is red, then both of its children are black. For each node, all simple paths from the node to ([nil]{.smallcaps}) descendant leaves contain the same number of black nodes. To expand more on property 4, let's find the black-height of the key value of \\(7\\) . These are all the simple paths that can be taken indicated by the grey dashed arrow above: 7, 3, [nil]{.smallcaps} 7, 18, 10, 8, [nil]{.smallcaps} 7, 18, 10, 11, [nil]{.smallcaps} 7, 18, 22, 26, [nil]{.smallcaps} If we don't include the root node, notice how all simple paths consists of the (same number of) \\(2\\) black nodes. Likewise, the same can be said for every node in the tree. The red-black tree is a BST, so we can implement the dynamic-set operations [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} in \\(\\text{O}(\\log{n})\\) time. -3ex -0.1ex -.4ex 0.5ex .2ex Recoloring and Rotation However, it does not directly support the dynamic-set operations [Insert]{.smallcaps} and [Delete]{.smallcaps}. Because they modify the tree, the result may violate the red-black properties. We must change color of some nodes via recoloring Restructure the links of the tree via rotation For starters, let's go over the relationship in a binary tree: ::: center ::: There's two types of procedures called [Left-Rotate]{.smallcaps} and [Right-Rotate]{.smallcaps}. ::: center ::: The letters \\(\\alpha\\) , \\(\\beta\\) , and \\(\\gamma\\) represent an arbitrary sub-treeall of them have the same black-height. First, determine if recoloring needs to be done. Case One: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is red. Proceed with recoloring. Push \\(C\\) 's black onto \\(A\\) and \\(D\\) . Recurse and check for \\(C\\) 's uncle if it exists. ::: center ::: Case Two: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LR or RL imbalance. ::: center ::: There are four restructuring configurations depending on whether the double red nodes ( \\(A\\) and \\(B\\) ) are left or right children. ::: center ::: If there's a LR imbalance, perform [Left-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: If there's a RL imbalance, perform [Right-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: Case Three: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LL or RR imbalance. Preserve the color If there's a LL imbalance, perform [Right-Rotate]{.smallcaps} on top node. ::: center ::: If there's a RR imbalance, perform [Left-Rotate]{.smallcaps} on top node. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion To perform insertion, we insert [ x ]{style=\"background-color: light-gray\"} and color it red. The motivation for using the color red is that only property 2 and 3 might be brokenthese violation are fairly easy to fix. The [RB-Insert]{.smallcaps} procedure performs the following three cases describe in the previous section. The running time is \\(\\text{O}(\\log{n})\\) . Suppose we want to insert \\(15\\) using the red-black tree shown in the beginning, then we would insert as we normally would in a BST and color it red. Refer to the diagram below: ::: center ::: Just like deleting a node in a BST, it's just as complicated to delete a node in a red-black tree. The process for deleting node can be broken into three cases: Case One: If the deleted node is red, perform the deletion as you would in BST. No color changes should occur. ::: center ::: Case Two: If the deleted node is black and has one red child. Reattach the red child in place of the black node we removed, then recolor the red node as black to fix black-height of the tree. ::: center ::: Case Three: If the deleted node is black. Reattach a black child in place of the black node we removed, then recolor as a double black. ::: center ::: The double black is to keep track of where we violated the black depth property. Denoted as [ r ]{style=\"background-color: light-gray\"} and the sibling of [ r ]{style=\"background-color: light-gray\"} as [ y ]{style=\"background-color: light-gray\"}, we'll divide this into three sub-cases based on [ y ]{style=\"background-color: light-gray\"}: ::: list The color of [ x ]{style=\"background-color: light-gray\"}, parent of [ z ]{style=\"background-color: light-gray\"}, displayed can be black or red. These three sub-cases differ only on the color of [ y ]{style=\"background-color: light-gray\"}, sibling of [ r ]{style=\"background-color: light-gray\"}. ::: Case Three (a): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and has a red child [ z ]{style=\"background-color: light-gray\"}. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a left child, perform [Right-Rotate]{.smallcaps} on [ y ]{style=\"background-color: light-gray\"}, then proceed below. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a right child, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ x ]{style=\"background-color: light-gray\"} and [ z ]{style=\"background-color: light-gray\"} black, give [ y ]{style=\"background-color: light-gray\"} the former color of [ x ]{style=\"background-color: light-gray\"}, and color [ r ]{style=\"background-color: light-gray\"} black. ::: center ::: As you can see, we managed to achieve the same configuration as the original tree prior to the deletion of node. We basically converted the red node to be a black node, thus maintaining the red-black tree property. Case Three (b): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and both children of [ y ]{style=\"background-color: light-gray\"} are black. ::: center ::: If [ x ]{style=\"background-color: light-gray\"} is red, we color it black, then we color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: Otherwise, we only color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: In this case, we are essentially removing one black-height from the other sub-tree, to deal with the double black. Case Three (c): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is red. Perform an adjustment operation. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the right child of [ x ]{style=\"background-color: light-gray\"}, perform [Left-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the left child of [ x ]{style=\"background-color: light-gray\"}, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: The sibling of [ r ]{style=\"background-color: light-gray\"} should be black now, thus solve using Case Three (a) or Case Three (b). ::: list In Case Three (a) and Case Three (b), if [ r ]{style=\"background-color: light-gray\"} is in the right-side instead of the left-side, the direction of rotation changese.g. [Right-Rotate]{.smallcaps} instead of [Left-Rotate]{.smallcaps} and vice-versa, as we have done in Case Three (c). ::: -3ex -0.1ex -.4ex 0.5ex .2ex Comparing AVL and Red-Black Trees Since both provide dynamic-set operations in \\(\\text{O}(\\log{n})\\) time, which one to choose? AVL trees provide faster lookups than Red Black Trees because they are more strictly balanced. Red-Black Trees provide faster insertion and removal operations than AVL trees as fewer rotations are done due to relatively relaxed balancing. AVL trees store balance factors or heights with each node, thus requires storage for an integer per node whereas Red Black Tree requires only 1 bit of information per node. Red-Black Trees are used in most of the language libraries like map, multi-map, multi-set in C++ whereas AVL trees are used in databases where faster retrievals are required. Graph \u00b6 -4ex -1ex -.4ex 1ex .2ex Properties of a Graph A graph should consists of the following: Vertices (nodes), which specify some entities we are interested in. Edges (lines), which specify the relationship between entities. Weights (number in lines), which specify the weight the edge represent. The formal definition of a graph is a pair \\((V,E)\\) where: \\(V\\) is a collection of nodes, called vertices. \\(E\\) is a collection of pairs of vertices, called edges. ::: exampleT Example 8.1 . We can represent the following graph using the given vertices and edges: \\(V = \\{a,b,c,d,e,f\\}\\) \\(E = \\{(a,c),(b,c),(c,f),(b,d),(d,f),(c,d)\\}\\) ::: center ::: ::: A graph can be categorized into one of two types, depending on the edge type: Undirected Graph. Edges do not have a directionundirected edge are unordered pair of vertices, such that \\((u,v)\\) and \\((v,u)\\) are the same edge. ::: center ::: Directed Graph. Edges with directiondirected edges are ordered pair of vertices, such that \\(\\langle u,v \\rangle\\) and \\(\\langle v,u \\rangle\\) are two different edges. ::: center ::: To distinguish between the two edge types, we use round brackets \\(( )\\) for unordered pairs and angle brackets \\(\\langle \\rangle\\) for ordered pairs. -3ex -0.1ex -.4ex 0.5ex .2ex Graph Terminology We will go over a few graph terminologies, some of which you should be familiar with. The degree of a vertex is the number of incident edges of this vertex. Below are some examples. Pay close attention to the degree of vertex \\(z\\) . ::: center ::: Let \\(m\\) be the number of edges and \\(\\deg(a)\\) be the degree of vertex \\(a\\) , then \\( \\(\\sum_{a \\in V}\\deg(a) = 2m\\) \\) For undirected graphs, parallel edges are edges that have the same endpoints, whereas for directed graph, they are edges that have the same origin and destination. ::: center ::: Self-loop is an edge whose endpoints coincide, such as the edge \\((z,z)\\) ::: center ::: In this course, we will deal almost exclusively with simple graphs, which are graphs that do not have a parallel edge or self-loop. Let \\(n\\) be the number of vertices and \\(m\\) the number of edges, then \\( \\(m \\leq \\frac{n(n - 1)}{2}\\) \\) There are various definitions used to describe the movement in a graph: A path is a sequence of vertices, such that consecutive vertices are adjacent. A simple path is path such that all its vertices are distinct. ::: center ::: A cycle is a path on which the first vertex is equal to the last vertex. A simple cycle is a cycle such that all its vertices are distinct, except the first and last one. ::: center ::: Lastly, we'll cover the characteristics of a connected graph and the definition of a subgraph. A connected graph is a graph in which there is a path from any vertex to any other vertex in the graph. ::: center ::: We can say a tree is a connected graph without a cycleany two vertices are connected by exactly one path. ::: center ::: A subgraph of a graph \\((V,E)\\) is a pair \\((V', E')\\) where \\(V' \\subseteq V\\) and \\(E' \\subseteq E\\) . Both endpoints of edges in \\(E'\\) are in \\(V'\\) . ::: center ::: Then a spanning tree is a subgraph of a connected graph, which includes all vertices of the connected graph. ::: center ::: -4ex -1ex -.4ex 1ex .2ex Representations of Graphs We can choose between two standard ways to represent a graph \\(G = (V,E)\\) , as a collection of: Adjacency list Adjacency matrix Either way applies to both directed and undirected graph. They are useful in representing dense and sparse graphs: Sparse graphs. A graph with only a few edge. Dense graphs. The number of edges is close to the maximal number of edges. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency List The adjacency-list representation of a graph consists of an array \\(\\textit{Adj}\\) of \\(|V|\\) list, one for each vertex in \\(V\\) . For an undirected graph, the adjacency list \\(\\textit{Adj}[u]\\) contains all the vertices \\(v\\) such that there is an edge \\((u,v), (v,u) \\in E\\) . ::: center ::: Alternatively, you can think of it as a list of all the vertices adjacent to \\(u\\) . ::: center ::: <!-- --> - For a directed graph, the adjacency list \\(\\textit{Adj}[u]\\) contains all the vertices \\(v\\) such that there is an edge \\(\\langle u,v \\rangle \\in E\\) . ::: center ![image](Figure/8/Adjacency/adjacency_list2.png){height=\"3cm\"} ::: Alternatively, you can think of it as a list of destinations given the origin $u$. ::: center ![image](Figure/8/Adjacency/adjacency_list2a.png){height=\"2.5cm\"} ::: ::: list Note that in an adjacency list, the order doesn't matter, meaning we could have listed the vertex in any order. ::: A useful thing we could do with adjacency list is to represent weighted graphsedges with an associated weight to them. It can easily be done by storing it with vertex \\(v\\) in \\(u\\) 's adjacency list. ::: center ::: A potential disadvantage of the adjacency-list representation is that there is no quicker way to determine if a given edge \\((u,v)\\) is present in the graph. We would need to search for \\(v\\) in the adjacency list \\(\\textit{Adj}[u]\\) . If we want to check the edge \\((2,4)\\) , then we would need search through \\(\\textit{Adj}[2]\\) . ::: center ::: The worst-case running time would be the number of adjacent vertices, which is not ideal. A solution would be to use an adjacency-matrix representation, which requires a constant time \\(\\text{O}(1)\\) , but at the cost of using asymptotically more memory. -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency Matrix The adjacency-matrix representation of a graph consists of a \\(|V| \\times |V|\\) matrixassuming the vertices are numbered from \\(1\\) , \\(2\\) , ..., to \\(|V|\\) . We can represent the elements inside the matrix \\(A\\) as \\(a_{ij}\\) , where \\(i\\) and \\(j\\) indicate the row and column. For an undirected graph, if there is an edge \\((i,j), (j,i) \\in E\\) , then set \\(a_{ij} = 1\\) , otherwise, \\(a_{ij} = 0\\) . ::: center ::: Notice how \\(a_{22}\\) is \\(0\\) , since the edge \\((2,2)\\) does not exist. ::: center ::: For a directed graph, if there is an edge \\(\\langle i, j \\rangle \\in E\\) , then set \\(a_{ij} = 1\\) , otherwise, \\(a_{ij} = 0\\) . ::: center ::: Like the adjacency-list representation of a graph, an adjacency matrix can represent a weighted graph. Instead of storing \\(0\\) 's and \\(1\\) 's, we store the weight of the given edge. ::: center ::: If an edge does not exist, we can store a [nil]{.smallcaps} value, depicted as empty in the diagram above. -3ex -0.1ex -.4ex 0.5ex .2ex Comparison As we have demonstrated both are applicable to undirected and directed graphs, each with their own advantages and disadvantages. **Adjacency List** **Adjacency Matrix** **Space:** $\\Theta(|V + E|)$ $\\Theta(|V|^2)$ Time: List all vertices adjacent to \\(u\\) \\(\\Theta(\\deg(u))\\) \\(\\Theta(|V|)\\) Time: Determine if \\((u,v) \\in E\\) \\(\\Theta(\\deg(u))\\) \\(\\Theta(1)\\) The choice of which one to use comes down to the following criteria: The adjacency-list representation provides a compact way to represent sparse graphsthose for which \\(|E|\\) is much less than \\(|V|^2\\) . However, if \\(|E|\\) is close to \\(|V|^2\\) , then we may choose an adjacency-matrix representation since it almost have the same space complexity as the adjacency-list. Alternatively, if we need to be able to tell quickly if there is an edge connecting two given vertices, an adjacency-matrix representation is used. -4ex -1ex -.4ex 1ex .2ex Graph Traversals A traversal (or graph searching) is a systematic procedure for exploring a connected graph by examining all its vertices and/or edges. There's two types of traversal algorithms: Breadth-First Search (BFS) Depth-First Search (DFS) -3ex -0.1ex -.4ex 0.5ex .2ex Breadth-First Search Breadth-first search (BFS) is one of the simplest algorithms for searching a graph, which uses a queue data structure. For simplicity, we will use a tree to describe breadth-first search: Let's start at the root of tree. Let's add \\(A\\) to the queue. ::: center ::: We want to explore all the vertices that are adjacent to \\(A\\) , which are \\(B\\) and \\(C\\) . We will add them to the queue. ::: center ::: Note that the order they are placed in queue does not matter. We could have stored \\(C\\) first. Since we finished \\\"exploring\\\" \\(A\\) , we will move on, then \\(B\\) is next in queue. ::: center ::: Likewise, we add all vertices adjacent to \\(B\\) , which are \\(D\\) and \\(E\\) , to the queue. ::: center ::: Since we finished \\\"exploring\\\" \\(B\\) , we will move to \\(C\\) . ::: center ::: We will add the vertices adjacent to \\(C\\) . which are \\(F\\) and \\(G\\) to the queue. ::: center ::: When we move to vertex \\(D\\) , you will see there are no adjacent vertices, thus we don't add anything to the queue and move on to \\(E\\) . ::: center ::: The same can be said for \\(E\\) through \\(G\\) , in which we finish our breadth-first search. ::: center ::: This is a simplified explanation, but should provide a general idea of how it works. We associate the vertex colors to guide the algorithm: White vertices have not been discovered. All vertices start out white. Grey vertices are discovered but not fully explored. Black vertices are discovered and fully explored. The algorithm attaches several attributes to each vertex, such as color [ u.color ]{style=\"background-color: light-gray\"}, parent [ u.\\pi ]{style=\"background-color: light-gray\"}, and distance [ u.d ]{style=\"background-color: light-gray\"} which is computed by the algorithm. To keep track of progress, breadth-first search colors each vertex white, grey, or black. Distance is used to represent the smallest number of edges that must be traverse from the starting vertex [ s ]{style=\"background-color: light-gray\"} to end vertex [ v ]{style=\"background-color: light-gray\"}. ::: algorithm [BFS]{.smallcaps} \\((G,s)\\) ::: algorithmic ::: ::: We start off by painting every vertex [ white ]{style=\"background-color: light-gray\"} except our starting vertex [ s ]{style=\"background-color: light-gray\"} and setting the distance to [ \\infty ]{style=\"background-color: light-gray\"}, as we not sure how far it is from the starting vertex or whether it is even reachable. ::: algorithm ::: algorithmic ::: ::: Then we paint our starting vertex [ s ]{style=\"background-color: light-gray\"} to [ grey ]{style=\"background-color: light-gray\"}, set the distance to [ 0 ]{style=\"background-color: light-gray\"}, since it's our starting point. The parent of [ s ]{style=\"background-color: light-gray\"} is [ nil ]{style=\"background-color: light-gray\"}doesn't exist. ::: center ::: Note that vertex [ s ]{style=\"background-color: light-gray\"} is something that we chose, which in this example is [ 5 ]{style=\"background-color: light-gray\"}. ::: algorithm ::: algorithmic ::: ::: Then we initialize [ Q ]{style=\"background-color: light-gray\"} to the queue containing just the vertex [ s ]{style=\"background-color: light-gray\"} which is [ 5 ]{style=\"background-color: light-gray\"}. ::: center ::: ::: algorithm ::: algorithmic ::: ::: The [ while ]{style=\"background-color: light-gray\"} loop functions similarly to what we have demonstrated in the first example. ::: center ::: The total running time of the [BFS]{.smallcaps} procedure is \\(\\text{O}(V + E)\\) . As you may have notice, it is particularly useful for finding the shortest path from the starting vertex [ s ]{style=\"background-color: light-gray\"} to some vertex [ v ]{style=\"background-color: light-gray\"} in the graph. -3ex -0.1ex -.4ex 0.5ex .2ex Depth-First Search Depth-first search (DFS) as the name implies, searches \\\"deeper\\\" first until it cannot go further at which point it backtracks and continues, which uses a stack data structure. Let's use the same tree as we have used for BFS, to compare the difference: As before, we will start at the root of three. Let's add \\(A\\) to the stack. ::: center ::: Then we arbitrarily pick an edge outwards of \\(A\\) , which will choose \\(B\\) and add to the stack. ::: center ::: Note there's multiple ways, so we could have also chosen to go with vertex \\(C\\) instead. Continue to pick an edge outwards, which there is only one, so will choose \\(D\\) . ::: center ::: Since there's no more vertices to explore, we backtrack to vertex \\(B\\) . ::: center ::: Continue to pick an edge outwards that has not been visited yet, which is vertex \\(E\\) . ::: center ::: Then we backtrack all the way to vertex \\(A\\) , since all of vertex \\(B\\) and \\(E\\) has been explored. ::: center ::: Then, we repeat the same steps for the right sub-tree, by picking some arbitrary edge outwards until we have fully discovered every vertex. DFS uses the same color scheme as we previously described in BFS. However, one unique thing about the algorithm is the it uses two timestamps for: when it first discovers the vertex [ u.d ]{style=\"background-color: light-gray\"} and ... when it finishes exploring the vertex [ u.f ]{style=\"background-color: light-gray\"}. This is similar to BFS, which we paint every vertex [ white ]{style=\"background-color: light-gray\"}. The [ time ]{style=\"background-color: light-gray\"} is set to [ 0 ]{style=\"background-color: light-gray\"}, which will use to compute the discovery time [ u.d ]{style=\"background-color: light-gray\"} and finishing time [ u.f ]{style=\"background-color: light-gray\"}. ::: algorithm [DFS]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: For this example, we'll use a directed graph as it's \\\"easier\\\" to pick an edge outwards and demonstrate. ::: center ::: For demonstration purposes, the discovery time and finishing time is denoted below the vertex, shown on the right. ::: algorithm [DFS-Visit]{.smallcaps} \\((G,u)\\) ::: algorithmic ::: ::: We'll choose vertex [ 8 ]{style=\"background-color: light-gray\"} as our starting vertex. It'll recursively call [ DFS-Visit ]{style=\"background-color: light-gray\"} until it reaches a dead end then it colors the vertex black. ::: center ::: Once the recursion finishes, it goes to the next [ v ]{style=\"background-color: light-gray\"} that is in [ Adj[u] ]{style=\"background-color: light-gray\"} and repeats the same thing. ::: center ::: Since there's no more [ v ]{style=\"background-color: light-gray\"} (or vertex to explore) in [ Adj[u] ]{style=\"background-color: light-gray\"}. We pick a new [ u ]{style=\"background-color: light-gray\"}. ::: center ::: The total running time of the [DFS]{.smallcaps} procedure is \\(\\text{O}(V + E)\\) , similar to [BFS]{.smallcaps} procedure. It provides valuable information about the structure of a graph and is more suitable for decision treelike a maze. We can use depth-first search to perform a topological sort of a directed acyclic graph (dag)a directed graph with no cycles. ::: center ::: By performing topological sort, we can find the topological orderingordering of its vertices along a horizontal line so that all directed edges go from left to right. ::: center ::: Note how the order they are listed in regarding their finishing times. The [Topological-Sort]{.smallcaps} sorts a dag by: ::: algorithm [Topological-Sort]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: We can perform [Topological-Sort]{.smallcaps} in \\(\\text{O}(V + E)\\) time, since DFS takes \\(\\text{O}(V + E)\\) . Elementary Graph Algorithms \u00b6 Minimum Spanning Tree \u00b6 The minimum spanning tree is the spanning tree of a weighted graph with minimum total edge weight. What this means, is suppose we have a connected graph with weighted edges. ::: center ::: We are interested in the subset of the edges which connects all vertices together, while minimizing the total edge cost. We can form three variations of the graph's spanning tree: ::: center ::: As you can see, tree 1 in particular is our minimum spanning tree because it posses the minimum total edge weight of \\(71\\) . Obviously when there's more vertices and edges, it will be much harder to figure out the MST. And so we'll go over two types of algorithms which does the following: Kruskal's algorithm Prim's algorithm For both algorithm, we will try and find the minimum spanning tree of the following completed graph below and compare them after. ::: center ::: Kruskal's Algorithm \u00b6 This algorithm creates a forest of trees. It works by picking the smallest edge then checking if it forms a cycle with the spanning tree formed so far. If not, the edge is added, otherwise, discard it. We start off by selecting the smallest edge in the graph, which is \\((6,1)\\) , as it has an edge weight of \\(10\\) . ::: center ::: Then, we select the next smallest edge, which is \\((4,3)\\) , with an edge weight of \\(12\\) . ::: center ::: The next minimum edge is \\((7,2)\\) , with an edge weight of \\(14\\) . ::: center ::: The next minimum edge is \\((2,3)\\) , with an edge weight of \\(16\\) . ::: center ::: The next minimum edge is \\((7,4)\\) , with an edge weight of \\(18\\) . However, notice that it forms a cycle, so we discard it. ::: center ::: Instead, we go to the next minimum edge, \\((5,4)\\) , with an edge weight of \\(22\\) . ::: center ::: The next minimum edge is \\((5,6)\\) , with an edge weight of \\(25\\) . ::: center ::: Once we have all our vertices or have \\(|V| - 1\\) edges, we are done with the algorithm. Our MST is complete, which has the weight of \\(99\\) . Prim's Algorithm \u00b6 This algorithm starts with one node. It then adds a node one by one that is unconnected to the new graph. For this example, let's start at vertex \\(6\\) . Since we started with \\(6\\) , we have to select the smallest edge that is connected to \\(6\\) , which are either: \\(25\\) or \\(10\\) . ::: center ::: We'll select the edge weight of \\(10\\) , since that's the smallest. Likewise, we now have to select the smallest edge that is connected to either \\(6\\) or \\(1\\) , which will pick edge \\((6,5)\\) , with an edge weight of \\(25\\) . ::: center ::: We are basically repeating the same step, but now with more and more edges to pick from. The dashed lines indicate the possible edges to select. ::: center ::: While it was not demonstrated in this example, but if an edge is selected and it forms a cycle, we will discard it and choose the next minimum edge, similar to Kruskal's algorithm. Once we have all our vertices or have \\(|V| - 1\\) edges, we are done with the algorithm, which also have a weight of \\(99\\) . As you can see, we have obtained the same MST for both Kruskal and Prim's algorithm, so which one to choose? Kruskal's algorithm runs faster in sparse graph, with the time complexity of \\(\\text{O}(E\\log{V})\\) . Prim's algorithm runs faster in dense graph, with the time complexity of \\(\\text{O}(E\\log{V})\\) , but it can be improved up to \\(\\text{O}(E + V\\log{V})\\) using Fibonacci heaps. Shortest Path \u00b6 We define the shortest path as the minimum length path from a vertex to another vertex in \\(G\\) , if such a path exists. Previously, we have done something similar with breadth-first search with an unweighted graph, in which each edge has a weight of \\(1\\) . ::: center ::: We are particularly interested in the single-source shortest path's problem, that is given a graph \\(G = (V,E)\\) , what is the shortest path from a given source vertex \\(s \\in V\\) to each vertex \\(v \\in V\\) . ::: center ::: Suppose we chose \\(A\\) as our source vertex, then the shortest-paths tree are: ::: center ::: If you calculate the weight from the source vertex \\(A\\) to every other vertex, you will get the minimum edge weight possible. From \\(A\\) to \\(B\\) , the total edge weight is \\(3\\) . From \\(A\\) to \\(C\\) , the total edge weight is \\(5\\) . From \\(A\\) to \\(D\\) , the total edge weight is \\(9\\) . From \\(A\\) to \\(E\\) , the total edge weight is \\(11\\) . You also may have notice that the shortest-paths tree are not unique, meaning there can also be more than one shortest-path tree, nonetheless should still provide the same answers. In this section, we'll go over two algorithms which does the following: Dijkstra's algorithm Bellman-Ford's algorithm As will demonstrate, each algorithm have their advantages and disadvantages when choosing one over the other. Both algorithm uses the [Relax]{.smallcaps} procedure, but implement them in varying ways. ::: algorithm [Relax]{.smallcaps} \\((u,v,w)\\) ::: algorithmic ::: ::: The process of \\\"relaxing\\\" an edge is to check if its worth going through the edge \\(\\langle u,v \\rangle\\) which would improve the shortest path from source vertex to some vertex [ v ]{style=\"background-color: light-gray\"}. Dijkstra's Algorithm relax each edge exactly once. Bellman-Ford's Algorithm relaxes each edge \\(|V| - 1\\) times. For example, assume we have the obtained the following weighted, directed graph and we wanna find the shortest path to vertex \\(D\\) . Suppose we start by relaxing the edge \\(\\langle C,D \\rangle\\) . ::: center ::: Every vertex initially start with a distance of \\(\\infty\\) and since \\(\\infty > 7 + 6 = 13\\) , we used the edge \\(\\langle C,D \\rangle\\) . But now let's relax the other edge, \\(\\langle B,D \\rangle\\) . ::: center ::: Since \\(13 > 6 + 4 = 10\\) , we now use the edge \\(\\langle B,D \\rangle\\) instead of \\(\\langle C,D \\rangle\\) . Dijkstra's Algorithm \u00b6 As we shall see, Dijkstra's algorithm is pretty similar to Prim's algorithm, which we covered in Minimum Spanning Tree. One limitation of this algorithm is that all edge weights must be non-negative. Let's pick \\(A\\) as our source vertex. There's a table in the right-side, which will use to keep track of distances from the source vertex to each vertex. ::: center ::: We put infinity for the other vertices as we haven't visited them yet. A change in the distance will be indicated by the grey boxes in each step. Next, we examine the edges leaving \\(A\\) . As denoted in the table, we can reach \\(B\\) and \\(C\\) with an edge weight of \\(10\\) and \\(3\\) respectively. ::: center ::: We always pick the smallest edge weight of which the vertex hasn't been discovered. In this case, it's \\(\\langle A,C \\rangle\\) . ::: center ::: Notice how we don't include the edge \\(\\langle A,B \\rangle\\) in consideration as \\(B\\) is now reachable from \\(\\langle A,C \\rangle\\) and \\(\\langle C,B \\rangle\\) with a smaller total edge weight of \\(7\\) . Now \\(D\\) and \\(E\\) are now reachable. As usual, we'll pick the next smallest edge, which is \\(\\langle C, E \\rangle\\) . ::: center ::: Similar to what was described before, we don't consider the edge \\(\\langle E, D \\rangle\\) , since \\(D\\) has a shortest path using the edge \\(\\langle C, D \\rangle\\) instead of \\(\\langle E, D \\rangle\\) . Our only options left are \\(\\langle C,B \\rangle\\) and \\(\\langle C,D \\rangle\\) . We pick \\(\\langle C, B \\rangle\\) , as it has the smallest edge weight of \\(7\\) . ::: center ::: From \\(A\\) to \\(D\\) , the edge \\(\\langle C, D \\rangle\\) will results in a distance of \\(11\\) , while \\(\\langle B,D \\rangle\\) will result in a distance of \\(9\\) . So all that's left is to pick the last remaining edge, which is \\(\\langle B, D \\rangle\\) . ::: center ::: All the edges have been discovered, so we are done with the algorithm. Bellman-Ford's Algorithm \u00b6 Bellman-Ford's algorithm is more general than Dijkstra's algorithm, such that it can deal with negative edge weights. However, it is a bit more time consuming in comparison. As usual, let start with vertex \\(A\\) as our source vertex and the distances to each vertex listed in the right. ::: center ::: With Bellman-Ford's algorithm, we want to relax all the edges. In other words, we should test out all the possible edges that will result in the shortest path. So this will be our first iteration. Starting at \\(A\\) , we can reach \\(B\\) and \\(C\\) with a weight of \\(10\\) and \\(3\\) , using the edges, \\(\\langle A,B \\rangle\\) and \\(\\langle A,C \\rangle\\) respectively. ::: center ::: From \\(B\\) , we can reach \\(D\\) at a total weight of \\(12\\) using the edge \\(\\langle B,D \\rangle\\) . Note that we won't use the edge \\(\\langle B,C \\rangle\\) , as it result in a longer path from \\(A\\) to \\(C\\) . ::: center ::: From \\(C\\) , we can reach \\(E\\) at a total weight of \\(5\\) using the edge \\(\\langle C,E \\rangle\\) . ::: center ::: Also, notice if we use the edge \\(\\langle C,B \\rangle\\) , we will get a shorter path to \\(B\\) with a total weight of \\(7\\) . Consequently, it also lowers the total weight of \\(D\\) to \\(9\\) . ::: center ::: Since we found a better path, we'll remove the edge \\(\\langle A,B \\rangle\\) . From \\(D\\) , we only have one edge to work with, which is \\(\\langle D,E \\rangle\\) , however, notice how this increases the total weight of \\(E\\) from \\(5\\) to \\(16\\) , so, we don't use it. ::: center ::: Similarly for \\(E\\) , we have the edge \\(\\langle E,D \\rangle\\) , however, this also increase the weight of \\(D\\) by \\(9\\) to \\(12\\) , so we don't use it. ::: center ::: Since we have checked all vertex. we're done with our first iteration. The algorithm at most takes \\(|V| - 1\\) iterations to fully obtain the shortest-tree path. However, it can sometimes be less if there's no changes occurring after the next iteration. In our second iteration, we proceed to the do the following, but using the graph we got from our first iteration. ::: center ::: Our only option is \\(\\langle A,B \\rangle\\) , however, this doesn't improve the distances of \\(B\\) , so we don't update the graph. Same thing can be said for \\(B\\) , \\(C\\) , \\(D\\) and \\(E\\) . So after our second iteration, we are finished. Remark In the actual exam, they might provide you the edges in which you should relax them by order. The process will still be the same as described from above, but the order may be different, as they might ask you to relax the edges of \\(E\\) before \\(D\\) or etc. One useful property of Bellman-Ford algorithm is that we can also use it to check for the existence of negative cycleone in which the overall sum of the cycle becomes negative. ::: center ::: If you add the weights of its edges, it's negative ( \\(-6 + 3 + 2 = -1\\) ). The concept of a shortest path is meaningless if there is a negative cycle, as we'll have a continuous loop. Refer to the example below. To demonstrate this, let's use the following weighted, directed graph and set \\(A\\) as the source vertex. As you can see, we will loop through the cycle \\(B\\) , \\(C\\) , and \\(D\\) , such that total weight keeps decreasing. ::: center ::: As mentioned before, Bell-man Ford's algorithm runs for \\(|V| - 1\\) iterations and it guarantees that at the end, the distances are guaranteed to be correct or [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is false. ::: center ::: However, as you can see that is not the case, such that [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is still true, thus, it will detect a negative cycle. As a summary, comparing the two algorithms we've covered in this section: The Dijkstra's algorithm is less time consuming, as it has a time complexity of \\(\\text{O}(E\\log{V})\\) , compared to Bellman-Ford's algorithm which is \\(\\text{O}(VE)\\) . Bellman-Ford works when there is negative weight edge, it also detects the negative weight cycle. Dynamic programming approach is taken to implement the algorithm. While Dijkstra's algorithm doesn't work when there is a negative weight edge. Greedy approach is taken to implement the algorithm.","title":"COE428"},{"location":"W2022/COE428/COE428/#introduction-to-algorithms","text":"","title":"Introduction to Algorithms"},{"location":"W2022/COE428/COE428/#algorithm-analysis","text":"An algorithm is a sequence of computational step that transform the input into the output. You will typically see them displayed as pseudocode shown below: It is very convenient to classify algorithms based on the relative amount of time or relative amount of space they require and specify as a function of the input size. Thus, we have the notions of: Time Complexity: Running time of the program as a function of the size of input. Space Complexity: Amount of computer memory required during the program execution, as a function of the input size.","title":"Algorithm Analysis"},{"location":"W2022/COE428/COE428/#running-time","text":"The time complexity of an algorithm can be measured by characterizing running time as a function, \\(T(n)\\) , of the input size, \\(n\\) . We count the number of primitive operations that are executed: Assigning a value to a variable Calling a method Performing an arithmetic operation Comparing two values Indexing into an array Returning from a method ::: algorithm ::: algorithmic \\(temp = a\\) ; \\(a = b\\) ; \\(b = temp\\) ; ::: ::: For this example, we say the running time is \\(T(n) = 3\\) , since there's three primitive operations executed by an algorithm. When it comes to loops, it becomes a bit more complex. ::: algorithm ::: algorithmic \\(x = s[0]\\) \\(x = s[i]\\) ::: ::: We define the size of the input array to be \\(n\\) : In line 1, its indexing an array, \\(s[0]\\) , then assigning it to \\(x\\) In line 2, inside the for loop: It first assigns \\(i = 1\\) On each iteration, it makes a comparison, \\(<\\) , for \\(n\\) times In line 3, the if statement repeats for : Its indexing an array, \\(s[i]\\) , then makes a comparison, \\(>\\) In line 4, there's two possibilities that could occur: If true, then its indexing an array, \\(s[i]\\) , then assigning it to \\(x\\) If false, then it won't execute At the end, \\(i++\\) , is incremented, then assigned back to \\(i\\) The last line returns \\(x\\) The number of primitive operations executed by algorithm can be characterized in two ways: Best-Case Analysis: \\(T(n) = 2 + 1 + n + (n-1)(2 + 0 + 2) + 1 = 5n\\) Worst-Case Analysis: \\(T(n) = 2 + 1 + n + (n-1)(2 + 2 + 2) + 1 = 7n - 2\\) ... to which it is bounded by two linear functions, \\(5n \\leq T(n) \\leq 7n - 2\\)","title":"Running Time"},{"location":"W2022/COE428/COE428/#classes-of-functions","text":"Let's first go over some common functions that characterize the running time of an algorithm: Constant function: \\(f(n) = c\\) Linear function: \\(f(n) = n\\) n-log-n function: \\(f(n) = n\\log{n}\\) Quadratic function: \\(f(n) = n^2\\) Cubic function: \\(f(n) = n^3\\) Exponents: \\(f(n) = b^n\\) Logarithms: \\(f(n) = \\log_2{n}\\) A review from MTH is the summation formula, which will be useful when analyzing the time complexity of algorithms: Arithmetic series: \\(\\displaystyle\\sum_{k = 0}^{n} k = 1 + 2 + \\cdots + n = \\frac{n(n+1)}{2}\\) Geometric series: \\(\\displaystyle\\sum_{k = 0}^{n} x^k = 1 + x + x^2 + \\cdots + x^n = \\frac{1-x^{n+1}}{1-x}\\) ... and the following terms which you might recall from coding: \\(\\lfloor x \\rfloor\\) or floor(x) is largest integer less than to equal to \\(x\\) . \\(\\lceil x \\rceil\\) or ceiling(x) is least integer greater than to equal to \\(x\\) .","title":"Classes of Functions"},{"location":"W2022/COE428/COE428/#growth-rate-of-running-time","text":"When choosing between algorithms, we care most about asymptotic performance; how the algorithm increases with the input size. ::: center ::: If you notice, as the input size increases, certain class of functions grows much more rapidly than others. ::: center ::: If we let the value of the growth-rate function represent the units of time, an algorithm with the function \\(f(n) = \\log_2{n}\\) would be much more efficient than an algorithm with the function \\(f(n) = 2^n\\) . In general, the order-of-growth can be classified to be: \\( \\(1 < \\log{n} < \\sqrt{n} < n < n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n\\) \\)","title":"Growth Rate of Running Time"},{"location":"W2022/COE428/COE428/#analyzing-and-designing-algorithms","text":"In this lecture, we will go over three different types of sorting algorithm: insertion sort, merge sort, and selection sort. The purpose of sorting algorithms is to solve the following problem: Input: A sequence of \\(n\\) numbers \\(\\langle a_1, a_2, \\dots, a_n \\rangle\\) Output: A permutation (reordering) \\(\\langle a'_1, a'_2, \\dots, a'_n \\rangle\\) of the input sequence such that \\(a'_1 \\leq a'_2 \\leq \\dots \\leq a'_n\\) . ::: dBox ::: definitionT Definition 2.1 (Key). The sequence are typically stored in array. We also refer to the number as keys. ::: :::","title":"Analyzing and Designing Algorithms"},{"location":"W2022/COE428/COE428/#insertion-sort","text":"Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. ::: algorithm [Insertion-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: Iterate from [ A[1] ]{style=\"background-color: light-gray\"} to [ A[n] ]{style=\"background-color: light-gray\"} over the array. Compare the current element [ key = A[j] ]{style=\"background-color: light-gray\"} to its predecessor. If the element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element. ::: list Note that iterations starts at [ A[1] ]{style=\"background-color: light-gray\"}, not [ A[0] ]{style=\"background-color: light-gray\"}. For the sake of convenience, we assume a fictitious record [ A[0] ]{style=\"background-color: light-gray\"} as the sentinel value with key of \\(-\\infty\\) . ::: It maybe easier to visualize this using images to better understand the psuedocode written. Suppose we start out with the following array with 6 elements. ::: center ::: If we apply the insertion sort algorithm, the following array would be sorted as shown below. Let's denote the [ key ]{style=\"background-color: light-gray\"} in green. ::: center ::: Since we care most about the asymptotic performance, we are interested on finding the running time \\(T(n)\\) . ::: algorithm ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: The lectures note and textbook goes in-depth deriving the following running time of insertion sort: \\( \\(T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^n t_j + c_6\\sum_{j = 2}^n (t_j - 1) + c_7\\sum_{j = 2}^n (t_j - 1) + c_8(n - 1)\\) \\) where \\(t_j\\) is the number of times the while loop is executed for that value of \\(j\\) . The main takeaway is knowing how it sorts and the function for best and worst-case running time of the following algorithm is. In the next lecture, we will go more in-depth on analyzing the time complexity using asymptotic notations, which simplifies all of this stuff.","title":"Insertion Sort"},{"location":"W2022/COE428/COE428/#best-case-complexity","text":"The best-case scenario is when the array is already sorted. ::: center ::: In this example, the while loop does the comparison but never enters the loop, since it always find that [ A[i] ]{style=\"background-color: light-gray\"} is always less than or equal to [ key ]{style=\"background-color: light-gray\"}. ::: center ::: Thus, \\(t_j = 1\\) , we can derive the number of comparisons for every outer loop iteration: \\( \\(\\sum_{j=2}^n t_j \\to \\sum_{j=2}^n 1 = (n - 2) + 1 = n - 1\\) \\) Substituting this in the equation to the running time simplifies \\(T(n)\\) to \\( \\({ \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5(n - 1) + c_8(n - 1) \\\\ &= (c_1 + c_2 + c_4 + c_5 + c_8)n - (c_2 + c_4 + 5 + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(c_n\\) simplify to some constants \\(a\\) and \\(b\\) then \\( \\(T(n) = an - b\\) \\)","title":"Best-Case Complexity"},{"location":"W2022/COE428/COE428/#worst-case-complexity","text":"The worst-case scenario is when the array is sorted in reversefrom largest to smallest. ::: center ::: In this example, \\(t_j\\) has to compare with all elements to the left \\(j\\) -th positioncompare with \\(j - 1\\) elements. Thus, \\(t_j = j\\) , we can derive the number of comparisons for every outer loop iteration \\( \\(\\sum_{j = 2}^{n} t_j \\to \\sum_{j = 2}^{n} j = 2 + 3 + 4 + \\dots + n = \\bigg[\\sum_{j=1}^n j\\bigg] - 1 = \\frac{n(n+1)}{2} - 1\\) \\) and as well the number of moves inside the while loop: \\( \\(\\sum_{j = 2}^{n} (t_j - 1) \\to \\sum_{j = 2}^{n} (j - 1) = 1 + 2 + 3 + \\dots + n - 1 = \\frac{n(n-1)}{2}\\) \\) Substituting this in the equation to the running time simplifies \\(T(n)\\) to \\( \\({ \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + (c_6 + c_7)\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_8(n - 1) \\\\ &= \\bigg[\\frac{c_5}{2} + \\frac{c_6}{2} + \\frac{c_7}{2}\\bigg]n^2 + (c_1 + \\dots + c_8)n - (c_2 + \\dots + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(c_n\\) simplify to some constants \\(a\\) , \\(b\\) and \\(c\\) then \\( \\(T(n) = an^2 + bn - c\\) \\)","title":"Worst-Case Complexity"},{"location":"W2022/COE428/COE428/#merge-sort","text":"Merge sort closely follows the divide-and-conquer paradigm. Intuitively, it operates as follows. Divide: Divide the \\(n\\) -element sequence to be sorted into two subsequences of \\(n = 2\\) elements each. Conquer: Sort the two subsequences recursively using merge sort. Combine: Merge the two sorted subsequences to produce the sorted answer. ::: algorithm [Merge-Sort]{.smallcaps} \\((A,p,r) \\to A[p \\dots r]\\) ::: algorithmic \\(q = \\lfloor (p + r)/2 \\rfloor\\) [Merge-Sort( \\(A,p,q\\) )]{.smallcaps} [Merge-Sort( \\(A,q + 1,r\\) )]{.smallcaps} [Merge( \\(A,p,q,r\\) )]{.smallcaps} ::: ::: Split the deck into two piles, until these become simple enoughan array of size \\(1\\) . Sort the left pile and sort the right pile using [ Merge-Sort() ]{style=\"background-color: light-gray\"}. Merge both piles into the final pile. ::: list In the [ Merge-Sort(A,p,r) ]{style=\"background-color: light-gray\"}, the floor function is used to determine [ q ]{style=\"background-color: light-gray\"}, so in the case there's a decimalit will result in an integer, ex. \\(\\lfloor 7.5\\rfloor = 7\\) . ::: It may also be helpful to use a diagram like before to fully understand what's happening. The number in red denotes the order in which steps are processed. ::: center ::: If you prefer are more concrete example, look at the code I wrote on the left-side, which demonstrate how recursion works in this sorting algorithm. Each indent indicates the recursion depth. Step 1 to 3: Calls [ Merge-Sort(A,p,q) ]{style=\"background-color: light-gray\"} to split the left children with different values of \\(q\\) and \\(r\\) (passing parameter by value). Step 4: Since left child can no longer split, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on right child. Step 5: If both left and right child are already split, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Step 6: Trace back to tree structure and find the node that does not complete the splitting, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on the right children. Step 7 to 8: The same process is done as for Step 3 and 4. Step 9: Like in Step 5, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Most of the steps are just repeated for the other half of the array, until all children have complete the splitting, then they are merged together.","title":"Merge Sort"},{"location":"W2022/COE428/COE428/#merge-algorithm","text":"The key operation of the merge sort algorithm is the merging of two sorted sequences, after divide and conquer. ::: algorithm [Merge]{.smallcaps} \\((A,p,q,r) \\to A[p \\dots q]\\) and \\(A[q + 1 \\dots r]\\) where \\(p \\leq q \\leq r\\) ::: algorithmic \\(n_1 = q - p + 1\\) \\(n_2 = r - q\\) Let L[1 ... \\(n_1 + 1\\) ] and L[1 ... \\(n_2 + 1\\) ] be new arrays \\(L[i] = A[p + i - 1]\\) \\(R[j] = A[q + j]\\) \\(L[n_1 + 1] = \\infty\\) \\(R[n_2 + 1] = \\infty\\) \\(i = 1\\) \\(j = 1\\) \\(A[k] = L[i]\\) \\(i = i + 1\\) \\(j = j + 1\\) ::: ::: It may look like a lot, but it's pretty simple. Most of the code are explained in the comments listed in the right. The main focus here is the [ for ]{style=\"background-color: light-gray\"} loop in Line 12 to 17. ::: center ::: The heavily shaded elements in [ A ]{style=\"background-color: light-gray\"} contain values that will be copied over, and heavily shaded elements in [ L[] ]{style=\"background-color: light-gray\"} and [ R[] ]{style=\"background-color: light-gray\"} contain values that have already been copied back into [ A[] ]{style=\"background-color: light-gray\"}. The lightly shaded elements in [ A[] ]{style=\"background-color: light-gray\"} indicate their final value.","title":"Merge Algorithm"},{"location":"W2022/COE428/COE428/#time-complexity","text":"Let's discuss the time complexity of the following algorithm, which we can break down to the divide-and-conquer paradigm. The time to split deck takes can be denoted by \\(c_1\\) , as it takes constant timedoes not depend on any input. The time to sort left pile and sort right pile can be denoted by \\(2T(n/2)\\) , due to recursion, where the size is now divided by two, \\(n/2\\) . The time to merge piles can be denoted by \\(c_2n + c_3\\) , as it takes linear timeonly the [ for ]{style=\"background-color: light-gray\"} loop depends on input size, while the rest take constant time, thus simplified to that. The time complexity results to \\( \\(T(n) = c_1 + T(n/2) + T(n/2) + c_2n + c_3\\) \\) Our goal is to determine the most rapidly growing term in \\(T(n)\\) and so we can set a few rules. We set constants \\(c_n\\) to either: \\(0\\) , if they will not be significant in the most rapidly growing term or ... \\(1\\) , if they will be For \\(T(n)\\) , when \\(n > 1\\) , we can set \\(c_1\\) and \\(c_3\\) to \\(0\\) and \\(c_2\\) to \\(1\\) , which simplifies to: \\( \\(T(n) = \\begin{cases}c_1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) ::: list Note that when \\(n = 1\\) , the code inside [ if ... else ]{style=\"background-color: light-gray\"} wont run, as there's only one element \\(p \\nless r\\) or \\(1 \\nless 1\\) , so we write it as a constant. ::: In order to solve the recurrence, when \\(n > 1\\) , we need a base case, so for simplicity, \\(T(1) = 0\\) and we can make a deduction from it. ::: tabu c | c c | c \\(n\\) & \\(T(n/2)\\) & \\(2T(n/2) + n\\) & \\(n\\log_2{n}\\) \\ \\(2\\) & \\(0\\) & \\(2(0) + 2 = 2\\) & \\(2\\) \\ \\(4\\) & \\(2\\) & \\(2(2) + 4 = 8\\) & \\(8\\) \\ \\(8\\) & \\(8\\) & \\(2(8) + 8 = 24\\) & \\(24\\) \\ \\(16\\) & \\(24\\) & \\(2(24) + 16 = 64\\) & \\(64\\) \\ \\(32\\) & \\(64\\) & \\(2(64) + 32 = 160\\) & \\(160\\) \\ ::: Examining the numbers allows us to form an educated guess it is growing by a function of \\(n\\log_2{n}\\) , which can also be deducted by drawing a recursion tree. We start by representing \\(T(n) = 2T(n/2) + n\\) as a graph where we put the non-recursive part ( \\(n\\) in this case) on the top row and put each recursive part on a row below. ::: center ::: Then expand downwards for the next level. ::: center ::: Repeat the same process. Eventually, it will reach a certain height which it reaches the base case and stop. ::: center ::: ::: {#height_avl} If you notice the sum of the non-recursive elements for each level is \\(n\\) . Let's denote the depth or height of the tree as \\(h\\) and so we can say the time complexity is ::: \\( \\(T(n) = n \\times h\\) \\) It will eventually reach the base case which we set to some constant when \\(n=1\\) , where \\(T(1) = 1\\) . We can rewrite the fraction in terms of the depth, \\(h\\) , where \\( \\(\\frac{n}{2^h} = 1 \\to n = 2^h \\to h = \\log_2{n}\\) \\) Thus, the time complexity is \\( \\(T(n) = n\\log_2{n}\\) \\) If you recall the order-of-growth from Lecture 2, we know that \\(n\\log{n} < n^2\\) , and so merge sorting beats insertion sort in the worst-case scenario, as it grows much more slowly.","title":"Time Complexity"},{"location":"W2022/COE428/COE428/#selection-sort","text":"The final sorting algorithm will cover is selection sort. ::: algorithm [Selection-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Select the first element as [ min ]{style=\"background-color: light-gray\"} Compare [ min ]{style=\"background-color: light-gray\"} with the second element. If the second element is smaller than minimum, assign the second element as [ min ]{style=\"background-color: light-gray\"}. Repeat until last element. After each iteration, [ min ]{style=\"background-color: light-gray\"} is placed in the front of the unsorted list. For each iteration, indexing starts from the first unsorted element. The steps are repeated until sorted. Let's use the example as we did for insertion sort, which is the following array with 6 elements. ::: center ::: The first iteration would like something like this. Let's denote the [ min ]{style=\"background-color: light-gray\"} in green and the line to the element it's being compared to. The arrow indicates a swap to be made. ::: center ::: In the next iteration, the first unsorted element is [ A[2] ]{style=\"background-color: light-gray\"}, so it starts at \\(3\\) . ::: center ::: Then it is repeated until all the elements are placed at their correct positions.","title":"Selection Sort"},{"location":"W2022/COE428/COE428/#time-complexity_1","text":"As we have covered for the other algorithm, let's analyze the time complexity of selection sort. ::: algorithm ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Combining each one of them, we get the following running time of selection sort: \\( \\(T(n) = c_1n + c_2(n-1) + c_3\\sum_{j=2}^n j + c_4\\sum_{j = 2}^n (j - 1) + c_5(n-1)\\) \\) As we have previously done with insertion sort, we can simplify the summation using the arithmetic series \\( \\({ \\begin{split} T(n) &= c_1n + c_2(n-1) + c_3\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + c_4\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_5(n-1) \\\\ &= \\bigg[\\frac{c_3}{2}+\\frac{c_4}{2}\\bigg]n^2 + (c_1 + \\dots + c_5)n - (c_2 + \\dots + c_5) \\end{split}}\\) \\) or equivalently, if we let \\(c_n\\) simplify to some constants \\(a\\) , \\(b\\) and \\(c\\) then \\( \\(T(n) = an^2 + bn - c\\) \\) Comparing it to the other two algorithms discussed, selection sort is on par with insertion sort in the worst-case scenario and so merge sorting is better than selection sort as well.","title":"Time Complexity"},{"location":"W2022/COE428/COE428/#complexity-analysis","text":"","title":"Complexity Analysis"},{"location":"W2022/COE428/COE428/#asymptotic-notations","text":"As covered briefly in the growth rate of running time, it's hard to determine which algorithm is better with no prior knowledge of the input size, so we consider the asymptotic behavior of the two functions for very large input size \\(n\\) . We use specific notations called asymptotic notations to express mathematical properties of asymptotic efficiency. ::: dBox ::: definitionT Definition 3.1 (Asymptotic efficiency). The study of how the running time of an algorithm increases as the size of the input increases without bound. ::: ::: There are three asymptotic notations, which will go over in this lecture: Big-Oh notation, \\(\\text{O}()\\) , for the upper bound or worst-case complexity Big-Omega notation, \\(\\Omega()\\) , for the lower bound or best-case complexity Theta notation, \\(\\Theta()\\) , for the average bound or average-case complexity We can apply these to the previous lecture, which we covered three different sorting algorithms with varying time complexity: Algorithm Time Complexity Best Worst Insertion Sort \\(\\Omega(n)\\) \\(\\text{O}(n^2)\\) Merge Sort \\(\\Omega(n\\log{n})\\) \\(\\text{O}(n\\log{n})\\) Selection Sort \\(\\Omega(n^2)\\) \\(\\text{O}(n^2)\\) -3ex -0.1ex -.4ex 0.5ex .2ex Big-Oh Notation (O-notation) The notation represents the upper bound of the running time of an algorithm. Thus, it gives the worst-case complexity of an algorithm. \\( \\({ \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Some tips for determining \\(\\text{O}()\\) complexity: Ignore the constants: \\( \\(5n \\to n\\) \\) Certain terms dominate other, which we ignore lower order terms: \\( \\(\\text{O}(1) < \\text{O}(\\log{n}) < \\text{O}(n) < \\text{O}(n\\log{n}) < \\text{O}(n^2) < \\cdots < \\text{O}(2^n) < \\cdots < \\text{O}(n!) < \\text{O}(n^n)\\) \\) It might be easier to understand if we have examples to determine the big-Oh notation. ::: exampleT Example 3.1 . Determine the upper bound \\(\\text{O}()\\) for \\(f(n)\\) : \\(f_A(n) = an^2 + bn + c\\) is \\(\\text{O}(n^2)\\) \\(f_B(n) = 2n + 3\\) is \\(\\text{O}(n)\\) \\(f_C(n) = 5 + (15 \\cdot 20)\\) is \\(\\text{O}(1)\\) \\(f_D(n) = n^2\\log{n} + n\\) is \\(\\text{O}(n^2\\log{n})\\) ::: ::: list When writing the big-Oh notation, try to write the closest function to the running time. While the function \\(\\text{O}(n^2)\\) is true for \\(f_B(n)\\) , the function \\(\\text{O}(n)\\) is the closest to \\(f_B(n)\\) . ::: The rules for determining the \\(\\text{O}()\\) complexity are as listed: If \\(g(n) = \\text{O}(G(n))\\) and \\(f(n) = \\text{O}(F(n))\\) , then: \\( \\(f(n) + g(n) = \\text{O}(F(n)) + \\text{O}(G(n)) = \\text{O}(\\text{max}[F(n), G(n)])\\) \\) \\( \\(f(n) \\cdot g(n) = \\text{O}(F(n)) \\cdot \\text{O}(G(n)) = \\text{O}(F(n) \\cdot G(n))\\) \\) If \\(g(n) = \\text{O}(kG(n))\\) , where \\(k\\) is a constant, then \\(g(n) = \\text{O}(G(n))\\) . If \\(f(n)\\) is a polynomial of degree \\(d\\ (P(n) = \\sum_{i=0}^d a_in^i\\) where \\(a_d \\neq 0)\\) , then \\(f(n)\\) is \\(\\text{O}(n^d)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Big-Omega Notation ( \\(\\Omega\\) -notation) The notation represents the lower bound of the running time of an algorithm. Thus, it provides the best-case complexity of an algorithm. \\( \\({ \\begin{aligned} \\Omega(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq cg(n) \\leq f(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: The rules for determining \\(\\text{O}()\\) complexity is also true for determining the \\(\\Omega()\\) complexity. Let's use an example from before. ::: exampleT Example 3.2 . Determine the lower bound \\(\\Omega()\\) for \\(f(n) = 2n + 3\\) : If we look at the order-of-growth for functions, \\(T(n)\\) belongs to the linear function, \\(n\\) and if we define our lower and upper bounds as such ( \\({\\overunderbraces{&\\br{2}{\\text{Lower bound}}}% {&1 < \\log{n} < \\sqrt{n} <& n &< n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n&} {& &\\br{2}{\\text{Upper bound}}}}\\) \\) So the lower bound can be defined by any of the following: ( \\(\\Omega(1) < \\Omega(\\log{n}) < \\Omega(n)\\) \\) Similar to the upper bound, we want the function closest to \\(f(n)\\) and so \\(f(n) = 2n + 3\\) is \\(\\Omega(n)\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Theta Notation ( \\(\\Theta\\) -notation) The next notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm, it is used for analyzing the average-case complexity of an algorithm. \\( \\({ \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c_1,\\ c_2, \\text{ and } n_0 \\text{ such that } \\\\ &\\ 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Equivalently, \\(f(n)\\) is \\(\\Theta(g(n))\\) if and only if \\(f(n)\\) is both \\(\\text{O}(g(n))\\) and \\(\\Omega(g(n))\\) . One notable example which we used previously is the function \\(f(n) = 2n + 3\\) , which as we demonstrated in previous notations are \\(\\text{O}(n)\\) and \\(\\Omega(n)\\) , thus \\(f(n)\\) is \\(\\Theta(n)\\) .","title":"Asymptotic Notations"},{"location":"W2022/COE428/COE428/#complexity-of-code-structures","text":"Loops are considered as dynamic if they depend on input size, otherwise they are static statements, everything within a loop is considered as static statementtakes a constant amount of time, \\(\\text{O}(1)\\) . The complexity is determined by: number of iterations in the loops \\(\\times\\) number of static statement -3ex -0.1ex -.4ex 0.5ex .2ex For Loop The following example is a simple for loop: for (int i = 0; i < n; i++) { // statement } The for loop is a dynamic statement, as it depends on the size of \\(n\\) . We are interested in the amount of times [ statement ]{style=\"background-color: light-gray\"} runs, which determines the time complexity of the following loop. Suppose \\(n = 3\\) then let's determine how many iterations: ::: tabu c c c Iteration & \\(i\\) &\\ & i = 0 &\\ 2 & i = 1 &\\ 3 & i = 2 &\\ 4 & i = 3 &\\ ::: You can see that the loop executes \\(3\\) times or in general, we can say \\(n\\) times. Thus, the time complexity is: \\( \\(n \\cdot 1 = \\text{O}(n)\\) \\) Note that there might be few variations of the for loop. Suppose there are also consecutive statements: for (int i = 0; i < n; i++) { // statement } for (int i = 1; i <= n; i++) { // statement } In both examples, the loop executes for \\(n\\) times. When we have consecutive statements, we would just add them together. If you recall, we ignore any constants of lower order terms. Thus, the time complexity is: \\( \\(\\underbrace{n \\cdot 1}_{\\substack{\\text{The first} \\\\ \\text{for loop}}} + \\underbrace{n \\cdot 1}_{\\substack{\\text{The second} \\\\ \\text{for loop}}} = 2n = \\text{O}(n)\\) \\) Note that this is not always the case for every for loop, as it depends on the initialization, condition test, and update statement. Suppose we have the following for loop to analyze: for (int i = 1; i <= n; i = i * 2) { // statement } Let's list out each iterations of the loop, till \\(k\\) iterations, since we do not know how many times this loop will execute. ::: tabu c c c Iteration & \\(i\\) \\ & i = 1 & \\(2^0\\) \\ 2 & i = 2 & \\(2^1\\) \\ 3 & i = 4 & \\(2^2\\) \\ \u22ee& \u22ee\\ \\(k\\) & i = \\(2^{k - 1}\\) &\\ ::: From the condition, we know that the loop will terminate once [ i > n ]{style=\"background-color: light-gray\"}. So we assume \\(i = n\\) , when it has reach \\(k\\) iterations; our very last iteration. Then we will solve for \\(k\\) : \\( \\({ \\begin{split} 2^{k - 1} &= n \\\\ k - 1 &= \\log_2{n} \\\\ k &= \\log_2{n} + 1 \\end{split}}\\) \\) If you recall from earlier, we ignore lower order terms. Thus, the time complexity is: \\( \\((\\log_2{n} + 1) \\cdot 1 = \\text{O}(\\log_2{n})\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Nested Loop The following example is a nested for loop: for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement } } As we covered earlier, the following for loop executes \\(n\\) times. Suppose now we have an inner loop, which also executes \\(n\\) times, then the statement is run \\(n \\times n\\) times. Thus, the time complexity is: \\( \\((n \\cdot n) \\cdot 1 = \\text{O}(n^2)\\) \\) The general formula for a nested loop is the time complexity of the outer loop times the inner loops. This also applies if we have a outer while loop with an inner for loop. -3ex -0.1ex -.4ex 0.5ex .2ex If Else Statement The following example is an if else statement: if (n == 0) { // statement 1 } else { for (int i = 0; i < n; i++) { // statement 2 } } If you notice, there's two possibilities that could occur: the if part, where[ statement 1 ]{style=\"background-color: light-gray\"} will run once, \\(\\text{O}(1)\\) or the else part, where [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(n\\) times, \\(\\text{O}(n)\\) . In general, the time complexity of an if else statement is: \\( \\(\\text{O}(if-else) = \\text{O}\\Big(\\text{max}\\Big[\\text{O}(\\text{condition1}), \\text{O}(\\text{condition2}), \\dots, \\text{O}(\\text{branch}1), \\text{O}(\\text{branch2}), \\dots\\Big]\\Big)\\) \\) As we are typically interested in the worst-cases, we only consider the branch with the largest running time. The condition runs once and then we add whichever is larger, which is the else part, thus, the time complexity is: \\( \\(1 + n = \\text{O}(n)\\) \\) or equivalently \\( \\(\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(1), \\text{O}(n)\\Big]\\Big) = \\text{O}(n)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Switch Statement The following example is a switch statement: switch (key) { case 'a': for (int i = 0; i < n; i++) { // statement 1 } case 'b': for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement 2 } } default: // statement 3 break; } Similar to the if else statement, we only consider the case with the largest running time, including the default case. In this example, for [ case \u2019b\u2019 ]{style=\"background-color: light-gray\"}, [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(n^2\\) times, \\(\\text{O}(n^2)\\) . Thus, the time complexity is: \\( \\(1 + n^2 = \\text{O}(n^2)\\) \\) or equivalently \\( \\(\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(n), \\text{O}(n^2), \\text{O}(1)\\Big]\\Big) = \\text{O}(n^2)\\) \\)","title":"Complexity of Code Structures"},{"location":"W2022/COE428/COE428/#recurrence-equations","text":"-4ex -1ex -.4ex 1ex .2ex Introduction In Lecture 2, we described the worst-case running time \\(T(n)\\) of merge-sort procedure by the recurrence: \\( \\(T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) whose solution we claimed to be \\(T(n) = \\Theta(n\\log n)\\) . Previously, we didn't really have a general method for finding the form of recurrences. Our goal for this lecture is to go in-depth in ways we can analyze recursive algorithms and form a general formula. ::: dBox ::: definitionT Definition 4.1 (Recursive algorithm). An algorithm which calls itself to solve smaller problems. ::: ::: Recurrence can be polymorphic, meaning it can take many forms: A recursive algorithm which divides to two problem with equal sizes. \\( \\(T(n) = 2T(n/2) + \\Theta(n)\\) \\) A recursive algorithm might divide subproblems into unequal sizes. \\( \\(T(n) = T(2n/3) + T(n/3) + \\Theta(n)\\) \\) They are not necessarily constrained to being a constant fraction of the original problem size. \\( \\(T(n) = T(n-1) + \\Theta(1)\\) \\) -4ex -1ex -.4ex 1ex .2ex Finding the Asymptotic Bounds There are three methods for solving recurrencesthat is, for obtaining asymptotic \" \\(\\Theta\\) \" or \" \\(\\text{O}\\) \" bounds on the solution: Substitution Method Recursion-Tree Method Master Method -3ex -0.1ex -.4ex 0.5ex .2ex Substitution Method This method is powerful, but we must be able to guess the form of the answer in order to apply it. It comprises of the following steps: Step 1: Try a few substitutions to find a pattern. Step 2: Guess the recurrence formula after \\(k\\) iterations (in terms of \\(k\\) and \\(n\\) ). Step 3: Set \\(k\\) so we get the base case. Step 4: Put \\(k\\) back into the formula to find a potential closed form. Step 5: Prove the potential closed form using induction. Using the merge-sort algorithm as an example, which has the following recurrence: \\( \\(T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) Let's go step-by-step, as described. The easiest way to find a pattern, is by simply writing out the first few iterations. Let's denote \\(k\\) as our number of iterations starting from \\(1\\) . \\( \\({\\begin{split} k &= 1 & T(n) &= 2T(n/2) + n \\\\ k &= 2 & T(n) &= 2\\Big[2T(n/4) + n/2\\Big] + n = 4 \\cdot T(n/4) + 2n \\\\ k &= 3 & T(n) &= 2\\bigg[2\\Big[2T(n/8) + n/4\\Big] + n/2\\bigg] + n = 8 \\cdot T(n/8) + 3n \\\\ \\end{split}}\\) \\) Our goal is to generalize this for \\(k\\) iterations. In other words, relating each of the constants to \\(k\\) . We can rewrite it as such \\( \\({\\begin{split} k &= 1 & T(n) &= 2^1 \\cdot T(n/2^1) + 1n \\\\ k &= 2 & T(n) &= 2^2 \\cdot T(n/2^2) + 2n \\\\ k &= 3 & T(n) &= 2^3 \\cdot T(n/2^3) + 3n \\\\ \\end{split}}\\) \\) Thus, we can form a general formula, using in terms of \\(k\\) and \\(n\\) \\( \\(T(n) = 2^k \\cdot T(n/2^k) + kn\\) \\) We know the base case is set to \\(T(1) = 1\\) . From our general formula, we can determine how many iterations there are in terms of \\(n\\) to reach the base case, by solving for \\(k\\) . \\( \\({\\begin{split} \\frac{n}{2^k} = 1 \\ \\to\\ n &= 2^k \\\\ k &= \\log_2{n} \\end{split}}\\) \\) Substituting \\(k = \\log_2{n}\\) back into the general formula, we get a potential closed form, as \\(T()\\) is no longer inside our formula. \\( \\({\\begin{split} T(n) &= 2^{\\log_2{n}} \\cdot T(n/2^{\\log_2{n}}) + n\\log_2{n} \\\\ &= n + n\\log_2{n} \\end{split}}\\) \\) We can guess that the solution is \\(T(n) = \\text{O}(n\\log{n})\\) . However, we need a definite proof that this is true, by using mathematical induction for the following statement. \\( \\(0 \\leq T(n) \\leq cn\\log{n} \\hspace{1cm} \\exists c > 0,\\ \\forall n \\geq n_0\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Mathematical Induction In order to prove something is true, we use mathematical induction. We must show that we can choose the constant \\(c\\) large enough so that \\(T(n) \\leq cn\\log{n}\\) is true. Remember, the base case is \\(T(1) = 1\\) . Then for \\(n = 1\\) , it yields \\(T(1) \\nleq c(1)\\log{1} = 0\\) . Consequently, the base case fails to hold, so what now? For asymptotic notation we can specify a specific bound, \\(\\forall n \\geq n_0\\) , where \\(n_0\\) is something we can choose. Thus \\(n_0 = 2\\) , removing it from consideration in the induction proof. The induction proof consists of three parts: the base case, inductive hypothesis and inductive step. Let's assume \\(n\\) is some power of \\(2\\) or \\(n = 2^k\\) , for sake of convenience. Base Case: Let \\(k = 1\\) or \\(n = 2\\) then: \\( \\(T(2) = 2T(1) + 2 = 2 + 2 = 4 \\leq c(2)\\log{2}\\) \\) We can see that the inequality holds true for the base case, such that there \\(c \\geq 2\\) . Inductive hypothesis: We will now assume that our proposition, \\(T(n) = \\text{O}(n\\log{n})\\) , holds true for \\(k -1\\) , which equivalently is \\(n/2\\) , therefore: \\( \\(T(n/2) \\leq c(n/2)\\log{(n/2)}\\) \\) To prove the inductive step, one assumes the induction hypothesis for \\(k-1\\) and then uses this assumption to prove that the statement holds for \\(k\\) . If instead, we assume our hypothesis to hold for \\(k\\) , then we must prove it holds for \\(k+1\\) . Inductive step: From our hypothesis, prove the guess of correct for \\(k\\) . Using the following: \\( \\(T(n) = 2T(n/2) + n\\) \\) Since we know \\(T(n/2) \\leq c(n/2)\\log{(n/2)}\\) , then we can rewrite it as such: \\( \\({\\begin{split} T(n) &\\leq 2\\Big[c(n/2)\\log{(n/2)}\\Big] + n \\\\ &\\leq cn\\log{(n/2)} + n = cn\\log{n} - cn\\log{2} + n \\\\ &\\leq cn\\log{n} + (1 - c)n \\\\ &\\leq cn\\log{n}\\qquad (\\forall c \\geq 1) \\end{split}}\\) \\) From the inductive step, we proved that proposition is true as we found that there exists some value of \\(c\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Recursion-Tree Method In a recursion tree, we sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to determine the total cost of all levels of the recursion. Step 1: Start by substituting the parent with non-recursive part of the formula and adding child nodes for each recursive part. Step 2: Expand each node repeating the step above, until you reach the base case. We already covered how to do this using merge-sort algorithm, so let's start off simple, by using the following recurrence: \\( \\(T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ T(n-1) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) As usual, let's go step-by-step. The non-recursive part, \\(n\\) , will be the parent node and the recursive part, \\(T(n-1)\\) , will be the child node. The costs within each level is displayed in the right-hand side. ::: center ::: Expand on \\(T(n-1)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(T(1) = 1\\) . The fully expanded tree has height \\(n\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the arithmetic series, thus: \\( \\(T(n) = 1 + \\cdots + (n-2) + (n-1) + n = \\frac{n(n+2)}{2} = \\Theta(n^2)\\) \\) Suppose you consider something a bit more complex, which divides the subproblems into unequal sizes, for the following recurrence: \\( \\(T(n) = T(n/4) + T(n/2) + n^2\\) \\) The non-recursive part, \\(n^2\\) , will be the parent node and the recursive part, \\(T(n/4)\\) and \\(T(n/2)\\) , will be the child nodes. ::: center ::: Expand on \\(T(n/4)\\) and \\(T(n/2)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(T(1)\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the geometric series, thus: \\( \\(T(n) = n^2\\bigg[1 + \\Big[\\frac{5}{16}\\Big] + \\Big[\\frac{5}{16}\\Big]^2 + \\cdots\\bigg] = \\Theta(n^2)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Master Method The master method provides a \"cookbook\" method for solving recurrences of the form: \\( \\(T(n) = aT(n/b) + f(n)\\) \\) where \\(a \\geq 1\\) , \\(b > 1\\) and \\(f(n)\\) be a function of \\( \\(f(n) = n^k\\log^p{n}\\) \\) Note that there are various variations of the master theorem, but this is definition is what I found the easiest to understand. It consists of memorizing these three cases: Case One: If \\(\\log_b{a} > k\\) , then \\(T(n) = \\Theta(n^{\\log_b{a}})\\) . Case Two: If \\(\\log_b{a} = k\\) and ... (a) \\(p > -1\\) , then \\(T(n) = \\Theta(n^k\\log^{p+1}{n})\\) . (b) \\(p = -1\\) , then \\(T(n) = \\Theta(n^k\\log({\\log{n}}))\\) . (c) \\(p < -1\\) , then \\(T(n) = \\Theta(n^k)\\) . Case Three: If \\(\\log_ba < k\\) and ... (a) \\(p \\geq 0\\) , then \\(T(n) = \\Theta(n^k\\log^{p}{n})\\) . (b) \\(p < 0\\) , then \\(T(n) =\\Theta(n^k)\\) . It looks a bit complicated at first glance, but once we get to the examples, it becomes quite easy. ::: exampleT Example 4.1 . Suppose we have the following recurrence: ( \\(T(n) = 2T(n/2) + 1\\) \\) We know \\(a = 2\\) and \\(b = 2\\) , but how do we get \\(k\\) and \\(p\\) ? We can rewrite it in the form of \\(n^k\\log^p{n}\\) : ( \\(f(n) = 1 = n^0\\log^0{n}\\) \\) You can confirm that both equations are identical, thus \\(k = 0\\) and \\(p = 0\\) . Since \\(\\log_2{2} > k\\) , then \\(T(n) = \\Theta(n^{\\log_b{a}})\\) . Substituting in the values for \\(a\\) and \\(b\\) , we get: ( \\(T(n) = \\Theta(n^{\\log_2{2}}) = \\Theta(n)\\) \\) ::: You can refer to this [video]{.underline} for more examples covering the three cases.","title":"Recurrence Equations"},{"location":"W2022/COE428/COE428/#elementary-data-structures","text":"-4ex -1ex -.4ex 1ex .2ex Stacks Stacks are dynamic sets in which the element removed from the set by the delete operation is prespecified. What defines a stack is that it implements a last-in, first-out (LIFO) principle, so only the top element is accessible. ::: center ::: There are three main methods on a stack: [ push(S,x) ]{style=\"background-color: light-gray\"} - Inserts an object \\(\\colorbox{light-gray}{\\texttt{x}}\\) onto top of Stack [ S ]{style=\"background-color: light-gray\"}. [ pop(S) ]{style=\"background-color: light-gray\"} - Removes the top object of stack [ S ]{style=\"background-color: light-gray\"}; if the stack is empty, an error occurs. [ top(S) ]{style=\"background-color: light-gray\"} - Returns the top object of the stack [ S ]{style=\"background-color: light-gray\"}, without removing it; if the stack is empty, an error occurs. ::: center ::: The following support methods should also be defined: [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in stack [ S ]{style=\"background-color: light-gray\"}. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if stack [ S ]{style=\"background-color: light-gray\"} is empty. We can implement each of the stack operations with just a few lines of code: ::: algorithm [Stack-Empty]{.smallcaps} \\((S)\\) ::: algorithmic \\(\\textsc{True}\\) \\(\\textsc{False}\\) ::: ::: ::: algorithm [Push]{.smallcaps} \\((S,x)\\) ::: algorithmic \\(S\\,.\\,top = S\\,.\\,top + 1\\) \\(S[S\\,.\\,top ] = x\\) ::: ::: ::: algorithm [Pop]{.smallcaps} \\((S)\\) ::: algorithmic \\\"underflow\\\" \\(S\\,.\\,top = S\\,.\\,top - 1\\) \\(S[S\\,.\\,top + 1]\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a stack [ S ]{style=\"background-color: light-gray\"} with \\(7\\) elements. Let \\(S\\,.\\,top\\) be a pointer to keep track of the last element (or top). When \\(S\\,.\\,top = 0\\) , there is no elements and is empty, so stack [ S ]{style=\"background-color: light-gray\"} has \\(0\\) elements. ::: center ::: When we call [ push(S,15) ]{style=\"background-color: light-gray\"}, \\(S\\,.\\,top\\) moves up by \\(1\\) and inserts element \\(15\\) to the stack. ::: center ::: Suppose we call the following: [ push(S,6) ]{style=\"background-color: light-gray\"}, [ push(S,2) ]{style=\"background-color: light-gray\"} and [ push(S,3) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ pop(S) ]{style=\"background-color: light-gray\"}, \\(S\\,.\\,top\\) moves down by \\(1\\) and returns the element that was removed, which is element \\(3\\) . ::: center ::: Although element \\(3\\) still appears in the array, it is no longer in the stack. When we call [ push(S,9) ]{style=\"background-color: light-gray\"}, \\(S\\,.\\,top\\) moves up by \\(1\\) and overwrites element \\(3\\) with \\(9\\) . ::: center ::: If you notice, when pushing an element or popping an element off the stack, it takes a constant amount of time. Let \\(n\\) be the numbers of elements in the stack. Each operation runs in time \\(\\text{O}(1)\\) . The space used is \\(\\text{O}(n)\\) . There are a few limitations we must consider: The maximum size of the stack must be defined priority and cannot be changed. When pushing a new element into a full stack, it causes an implementation error. -4ex -1ex -.4ex 1ex .2ex Queue Queue are another type of dynamic sets, which implements first-in, first-out (FIFO) principle, so queue items are removed in exactly the same order as they were added to the queue. ::: center ::: There are exist the following operations on a queue: [ enqueue(Q,x) ]{style=\"background-color: light-gray\"} - Inserts an element \\(\\colorbox{light-gray}{\\texttt{x}}\\) at the rear of the queue [ Q ]{style=\"background-color: light-gray\"}. [ dequeue(Q) ]{style=\"background-color: light-gray\"} - Removes the element at the front of queue [ Q ]{style=\"background-color: light-gray\"}. [ front() ]{style=\"background-color: light-gray\"} - Returns the front element of the queue without removing it. [ new() ]{style=\"background-color: light-gray\"} - Creates an empty queue. [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in queue. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if queue is empty. ::: center ::: Assume \\(n = Q\\,.\\,length\\) . The pseudocode for enqueue and dequeue is shown below: ::: algorithm [Enqueue]{.smallcaps} \\((Q,x)\\) ::: algorithmic \\(Q[Q\\,.\\,tail] = x\\) \\(Q\\,.\\,tail = 1\\) ::: ::: ::: algorithm [Dequeue]{.smallcaps} \\((Q)\\) ::: algorithmic \\(x = Q[Q\\,.\\,head]\\) \\(Q\\,.\\,head = 1\\) ::: ::: Note that we didn't account for the error when underflow and overflow occurs. -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a queue [ Q ]{style=\"background-color: light-gray\"} with \\(7\\) elements. Let \\(Q\\,.\\,head\\) be a pointer for the front of the queue and \\(Q\\,.\\,tail\\) be the back of the queue. When \\(Q\\,.\\,head = Q\\,.\\,tail\\) , there is no elements, so queue [ Q ]{style=\"background-color: light-gray\"} has \\(0\\) elements. ::: center ::: When we call [ enqueue(Q,15) ]{style=\"background-color: light-gray\"}, element \\(15\\) is added to the queue then \\(Q\\,.\\,tail\\) moves up by \\(1\\) . ::: center ::: Suppose we call the following: [ enqueue(Q,6) ]{style=\"background-color: light-gray\"}, [ enqueue(Q,2) ]{style=\"background-color: light-gray\"} and [ enqueue(Q,9) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ dequeue(Q) ]{style=\"background-color: light-gray\"}, element \\(15\\) located at the front of queue indicated by \\(Q\\,.\\,head\\) , is removed then \\(Q\\,.\\,head\\) moves up by \\(1\\) to element \\(6\\) . ::: center ::: As the final scenario, suppose we filled the array from \\(Q[2 .. 7]\\) , as shown below. ::: center ::: When we call \\(\\colorbox{light-gray}{\\texttt{enqueue(Q,x)}}\\) or add one more element, \\(Q\\,.\\,tail\\) will have to move up by one where \\(Q\\,.\\,head = Q\\,.\\,tail\\) . ::: center ::: But, if you recall, this means the queue is empty, which is not the case and so the queue overflows. Similar to a stack, when enqueueing or dequeueing an element, it takes a constant amount of time. Let \\(n\\) be the numbers of elements in the queue. Each operation runs in time \\(\\text{O}(1)\\) . There are also a few limitations we must consider which carries over for queue: The maximum size of the stack must be defined priority and cannot be changed. If we attempt to dequeue an element from an empty queue, the queue underflows. If we attempt to enqueue an element from a full queue, the queue overflows and so we can only store \\(n - 1\\) elements. -4ex -1ex -.4ex 1ex .2ex Linked Lists A collection of nodes that together form a linear ordering. Unlike an array, however, in which the linear order is determined by the array indices, the order in a linked list is determined by a pointer in each object. It consists of: A sequence of nodes Each node contains a value and link reference to some other node The last node contains a null link -3ex -0.1ex -.4ex 0.5ex .2ex Singly Linked Lists The most basic of all linked data structures, which are used to implement stacks and queues. Each node has data and a pointer to the next node. ::: center ::: Searching a singly linked list. ::: algorithm [List-Search]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,head\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: To search a list of \\(n\\) elements, the [List-Search]{.smallcaps} procedure takes \\(\\Theta(n)\\) time in the worst-case, since it may have to search the entire listsimilar to insertion sort. Inserting into a singly linked list. The [List-Insert]{.smallcaps} procedure splices the inserted element, [ x ]{style=\"background-color: light-gray\"}, onto the front of the linked list. ::: center ::: The running time for [List-Insert]{.smallcaps} on a list of \\(n\\) elements is \\(\\text{O}(1)\\) . Deleting from a singly linked list. The [List-Delete]{.smallcaps} procedure removes an element, [ x ]{style=\"background-color: light-gray\"}, from a linked list by getting a pointer to \\(\\colorbox{light-gray}{\\texttt{x}}\\) and it splices [ x ]{style=\"background-color: light-gray\"} out of the list by updating pointers. ::: center ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\text{O}(n)\\) time is required in the worst case. Some applications of singly linked lists are: Implement stacks and queues, as shown below. Dynamic memory allocation, which will cover in the very end. -3ex -0.1ex -.4ex 0.5ex .2ex Doubly Linked Lists We add a pointer to the previous node. Thus, we can go in either direction: forward or backward. ::: center ::: Searching a doubly linked list. A singly and linked list uses the same algorithm for searching. Thus, both take \\(\\Theta(n)\\) times in the worst-case to search through a list of \\(n\\) elements. <!-- --> Inserting into a doubly linked list. The [List-Insert]{.smallcaps} procedure is also similar to the singly, but now we also have to account for the previous pointer. ::: center ::: ::: algorithm [List-Insert]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,head\\) \\(L\\,.\\,head\\,.\\,prev = x\\) \\(L\\,.\\,head = x\\) \\(x\\,.\\,prev =\\) [nil]{.smallcaps} ::: ::: The running time for [List-Insert]{.smallcaps} on a list of \\(n\\) elements is \\(\\text{O}(1)\\) . Deleting from a doubly linked list. Likewise, same thing can be said for the [List-Delete]{.smallcaps} procedure, in which we now have to also assign the previous pointer ::: center ::: ::: algorithm [List-Delete]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(L\\,.\\,head = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\text{O}(n)\\) time is required in the worst case. Some applications of doubly linked lists are: Browsers to implement backward and forward navigation of visited web pagesthe back and forward button. Various application to implement Undo and Redo functionality. -3ex -0.1ex -.4ex 0.5ex .2ex Circularly Linked Lists A circularly singly linked list is a variation of a linked list in which the last element is linked to the first element. This forms a circular loop. ::: center ::: A circularly doubly linked list, in which in addition to the one above, the first element is linked to the last element. ::: center ::: In a circularly linked list, we used a sentinelrepresented by the dark grey node [ L.nil ]{style=\"background-color: light-gray\"}. ::: center ::: It represents [nil]{.smallcaps} which lies between the head and tail. It functions like any other object in a doubly linked list, which it has a pointer from the previous and next node. Below are the procedures used for circularly doubly linked list with sentinel. ::: algorithm [List-Search/]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,nil\\,.\\,next\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: ::: algorithm [List-Insert']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,nil\\,.\\,next\\) \\(L\\,.\\,nil\\,.\\,next\\,.\\,prev = x\\) \\(L\\,.\\,nil\\,.\\,next = x\\) \\(x\\,.\\,prev = L\\,.\\,nil\\) ::: ::: ::: algorithm [List-Delete']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: Some applications of circularly linked lists are: Useful for implementation of queue. Circular lists are useful in applications to repeatedly go around the list. Circular doubly linked lists are used for implementation of advanced data structures like Fibonacci Heap. -3ex -0.1ex -.4ex 0.5ex .2ex Implementing Pointers and Objects We can implement pointers and objects in languages that do not provide them by synthesizing them from arrays and array indices. For this example, let's use the following doubly linked list: ::: center ::: Single-array representation of objects. Analogous to storing an object in the memory. ::: center ::: Each object is represented by a contiguous sub-array of length \\(3\\) . The three attributes [ key ]{style=\"background-color: light-gray\"}, [ next ]{style=\"background-color: light-gray\"}, and [ prev ]{style=\"background-color: light-gray\"} correspond to the offsets: \\(0\\) , \\(1\\) , and \\(2\\) of the sub-array. Multiple-array representation of objects. We can represent a collection of objects that have the same attributes by using an array for each attribute. ::: center ::: You can think of each column (or vertical slice) as a single object. The pointers resides in the [ next ]{style=\"background-color: light-gray\"} and [ prev ]{style=\"background-color: light-gray\"} array, which point to the index where the next object resides. Allocating and freeing objects. To insert a key into a dynamic set represented by a doubly linked list, we must allocate a pointer to a currently unused object in the linked-list representation. ::: center ::: We keep the free objects in a singly linked list (only [ next ]{style=\"background-color: light-gray\"} pointer), which we call the free list. The free list acts like a stackthe next object allocated is the last one freed. -4ex -1ex -.4ex 1ex .2ex Heaps The (binary) heap data structure is an array of object that we can view as a nearly complete binary tree. Each node of the tree corresponds to an element of the array. ::: center ::: There are two kinds of binary heap. In both kinds, the values in the nodes satisfy a heap property, the specifics of which depend on the kind of heap. Max-heap. The max-heap property is that for every node \\(i\\) other than the root: \\( \\(A[\\textsc{Parent}(i)] \\geq A[i]\\) \\) which means that a child node can't have a greater value than its parent. Min-heap. The min-heap property is the opposite, which for every node \\(i\\) other than the root: \\( \\(A[\\textsc{Parent}(i)] \\leq A[i]\\) \\) which means that a parent node can't have a greater value than its child nodes. If all the nodes satisfy the heap property, then a binary tree is a heap. However, if a node does not have the heap property, the node is swapped with the parent. This operation is called sifting up. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Constructing a Heap A heap can be stored as an array \\(A\\) , where the: Root of tree is \\(A[1]\\) . The cell at index \\(0\\) is not used, thus we start at index \\(1\\) . Parent of \\(A[i]\\) is \\(A[\\lfloor i/2 \\rfloor]\\) . Left child of \\(A[i]\\) is \\(A[2i]\\) . Right child of \\(A[i]\\) is \\(A[2i + 1]\\) . To construct a heap: Start with a single node. Add a node to the right of the rightmost node in the deepest level. If the deepest level is full, start a new level. Each time we add a node, we may destroy heap property of its parent node. To fix this, sift up until either: We reach nodes whose values don't need to be swappedthe parent node is larger than both children. We reach the root. Suppose we have an array \\(A = [8, 10, 5, 12, 14]\\) , we would construct the heap as such: ::: center ::: Our final heap should look like this: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Maintaining Heap Property To implement this: Represent an arbitrary array as a binary tree. Devise a [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm that maintains the heap property of any given node \\(i\\) in the heap with sub-trees \\(l\\) and \\(r\\) rooted at \\(i\\) th children, given to be heaps. Devise a [ Build-Max-Heap() ]{style=\"background-color: light-gray\"} algorithm that uses [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm to construct a heap. ::: algorithm [Max-Heapify]{.smallcaps} \\((A,n)\\) ::: algorithmic ::: ::: ::: algorithm [Build-Max-Heap]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The worst-case time complexity of: [Max-Heapify]{.smallcaps} is \\(\\text{O}(\\log{n})\\) [Build-Max-Heap]{.smallcaps} is \\(\\text{O}(n)\\) The heapsort algorithm is based on the heap data structure, which uses these two main parts: building a max-heap and sorting it, to sort ::: algorithm [Heapsort]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: Thus, heapsort has a worst-case time complexity of \\(\\text{O}(n\\log{n})\\) like merge sort, but heapsort has a space complexity of \\(\\text{O}(1)\\) , since it sorts in-place, taking a constant amount of memory. -3ex -0.1ex -.4ex 0.5ex .2ex Priority Queue One of the most popular implementations of a heap, a priority queue is a data structure for maintaining a set \\(S\\) of elements, each with an associated value called a key. As with heaps, there are two kinds of priority queues: max-priority queue and min-priority queue. ::: center ::: We will focus here on how to implement max-priority queues, which are in turn based on max-heaps. A max-priority queue supports dynamic-set operations: [ Insert(S,x) ]{style=\"background-color: light-gray\"} - Inserts element [ x ]{style=\"background-color: light-gray\"} into set [ S ]{style=\"background-color: light-gray\"}. [ Maximum(S) ]{style=\"background-color: light-gray\"} - Returns an element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Extract-Max(S) ]{style=\"background-color: light-gray\"} - Removes and returns element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Increase-Key(S,x,k) ]{style=\"background-color: light-gray\"} - Increases value of element [ x ]{style=\"background-color: light-gray\"}'s key to [ k ]{style=\"background-color: light-gray\"}. Assume [ k \\geq x ]{style=\"background-color: light-gray\"}'s current key value. The procedure [Heap-Maximum]{.smallcaps} has a running time of \\(\\Theta(1)\\) . ::: algorithm [Heap-Maximum]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Extract-Max]{.smallcaps} has a running time of \\(\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Extract-Max]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Increase-Key]{.smallcaps} has a running time of \\(\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Increase-Key]{.smallcaps} \\((A,i,key)\\) ::: algorithmic ::: ::: The procedure [Max-Heap-Insert]{.smallcaps} has a running time of \\(\\text{O}(\\log{n})\\) . ::: algorithm [Max-Heap-Insert]{.smallcaps} \\((A,key)\\) ::: algorithmic ::: ::: In summary, a heap can support any priority-queue operation on a set of size \\(n\\) in \\(\\text{O}(\\log{n})\\) time.","title":"Elementary Data Structures"},{"location":"W2022/COE428/COE428/#hash-tables","text":"-4ex -1ex -.4ex 1ex .2ex Introduction Many applications require a dynamic set that supports only the dictionary operations. ::: dBox ::: definitionT Definition 6.1 (Dictionary). A data structure that stores (key, value) pairs and supports the operations [Insert]{.smallcaps}, [Search]{.smallcaps}, and [Delete]{.smallcaps}. ::: ::: So far we have seen a couple ways to implement dictionaries, such as linked lists. Now we will learn how to use a hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex How It Works? A hash tables takes a key (typically a string of characters or numbers) and passes it through a hash function to convert it into an index of the array to store the associated value. ::: center ::: Suppose you need to find a value of the key, you do not need to iterate through all items in the collection, because you can just use the hash function to easily find the index. Using a hash table offers a very fast lookup for a value based on the key, which should be the \\(\\text{O}(1)\\) operation. It is a generalization of an ordinary array. -3ex -0.1ex -.4ex 0.5ex .2ex Sample Problem As an example, you can think of a phone book. In the phone book, a person's name can be considered as a key, by which we can find a phone number. Case One: The simple and straightforward way to lookup number is to check all names in the phone book until we find a matching name. The worst-case search time is \\(\\text{O}(n)\\) . Case Two: Use a hash function that helps us to lookup entries much faster. Suppose we have a person's name \\\"James Davis\\\" with the phone number \\\"416-999-1234\\\". A hash function takes the key and maps it to an integer that is within the size of the array: \\( \\(\\text{String}\\ \\Rightarrow\\ \\boxed{\\text{Hash Function}}\\ \\Rightarrow\\ \\text{Index}\\) \\) Then it stores the value of the phone number to an index of the array. If we continue to add more people, it would map each one to an index of the array. ::: center ::: If we wanna lookup a person's phone number, all we need is the person's name and we can easily find the index it is stored in the array, by passing it through a hash function. Obviously, this is a watered-down explanation and doesn't go in-depthlike the possibility when two or more keys hash to the same slot. But before moving further, let's understand how direct-address table works to see the benefits of using hash tables instead. -3ex -0.1ex -.4ex 0.5ex .2ex Direct Address Table With an ordinary array, we store element whose key is \\(k\\) in position \\(k\\) of the array. ::: center ::: ::: dBox ::: definitionT Definition 6.2 (Direct addressing). Given a key \\(k\\) , we find the element whose key is \\(k\\) by just looking in the \\(k\\) th position of the array. ::: ::: A direct-address table (DAT) uses the keys as indices of the array and stores the values at those bucket locations. ::: center ::: It does facilitate fast searching, fast inserting and fast deletion operations: Inserting or deleting an element in the table, is the same as you would do for an array, hence we can do that in \\(\\text{O}(1)\\) time as we already know the index (via key). Searching an element takes \\(\\text{O}(1)\\) times, as we can easily access an element in an array in linear time if we already know the index of that element. Direct addressing is applicable when we can afford to allocate an array with one position for every possible key, and so it comes at a cost: It cannot handle collisionstwo keys are equal and contain different values. It is not recommended using the direct address table if the key values are very large. It has serious disadvantages, making it not suitable for the practical usage of current world scenarios, which is why we make use of hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Table As a recap, from the introduction, instead of storing an element with key \\(k\\) in index \\(k\\) , we use a hash function \\(h\\) and store the element in index \\(h(k)\\) . ::: center ::: ::: dBox ::: definitionT Definition 6.3 (Hash function). A hash function \\(h\\) maps all possible keys to the slots of an array \\(T[0 \\dots n - 1]\\) . ::: ::: While hash table offer the same time complexity of \\(\\text{O}(1)\\) when we talk about insertion, deletion, or searching an element, the main focus is in its ability to maintains the size constraint. The problem with DAT is if the universe \\(U\\) of keys is large, storing a table of size of \\(|U|\\) may be impractical or impossible. Often, the set of keys \\(K\\) actually stored is small, compared to \\(U\\) . ::: problem Problem 6.1 . Suppose we have a key of \\(7898\\) , which in turn is a large number. ::: Case One: Using a DAT table, we would need a huge array, for the key in index \\(7898\\) to store the value at \\(T[7898]\\) . In turn, we are wasting too much space, as most of the allocated space for the array is wasted. Case Two: But, in the case of a hash table, we can process this key via a hash function. The hash function \\(h(7898)\\) maps it to an index within the hash table \\(T[0 \\dots n - 1]\\) . Regarding the size of the hash table \\(n\\) it typically varies, as it depends in part on choice of the hash function and collision resolution, where a situation might arise when two or more keys hash to the same slot. -4ex -1ex -.4ex 1ex .2ex Hash Function A good hash function should minimizes collision as mush as possible. It is usually specified as the composition of two functions \\(h(k) = h_2\\big(h_1(k)\\big)\\) : Hash code. \\(h_1: \\text{keys}\\ \\to\\ \\text{integers}\\) Compression function. \\(h_2: \\text{integers}\\ \\to\\ [0 \\dots n - 1]\\) ::: center ::: The goal of the hash function is to disperse the keys in an apparently random way. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Code As mentioned previously, keys can be a string of characters. Thus, if the keys are not natural numbers, we find a way to interpret them as natural numbers. Some popular hash code maps are: Summing hash code. By adding up the [ASCII values]{.underline} of each letters in a string, we get an integer in return. For example, if the key is \\\"stop\\\": \\( \\(h_1(\"stop\") = 115 + 116 + 111 + 112\\) \\) However, this is not suitable for strings cause two different strings can have the same set of letters, but have different meaning\\\"post\\\", \\\"tops\\\", and \\\"pots\\\" will have the same hash code. Polynomial hash code. A better hash code takes into account the position of each character. Using the example from before: \\( \\(h_1(\"stop\") = (115 \\times a^0) + (116 \\times a^1) + (111 \\times a^2) + (112 \\times a^3)\\) \\) where \\(a\\) is a non-zero constantcompared to \\\"post\\\", \\\"tops\\\", and \\\"pots\\\", all have unique hash codes, which is ideal. -3ex -0.1ex -.4ex 0.5ex .2ex Compression Function The hash code typically returns a large range of integers and so the compression functions maps it in the range \\([0 \\dots n - 1]\\) , the indices of the hash table. There's two methods: Division Method. A simple-modulo based compression rule: \\( \\(h_2(k) = k\\ \\text{mod}\\ n\\) \\) The size \\(n\\) of the hash table is usually chosen to be a prime number, to help spread out the distribution of hash values. MAD Method. The Multiply-Add-Divide method still use \\(\\text{mod}\\ n\\) to get the numbers in the range, but a little fancier by spreading the numbers out first: \\( \\(h_2(k) = [(ak + b)\\ \\text{mod}\\ p]\\ \\text{mod}\\ n\\) \\) The values \\(a\\) and \\(b\\) are chosen at random as positive integers and \\(p\\) is a prime number, where \\(p > n\\) . With the addition of \\((ak + b)\\ \\text{mod}\\ p\\) , it eliminates patterns provided by \\(k\\ \\text{mod}\\ n\\) . Both incorporate the modulo operator, as it guarantees the output to be within the size of the hash table. Suppose we have a key of \\(7898\\) from the previous example and a hash table with \\(23\\) slots: \\( \\(h_2(7898) = 7898\\ \\text{mod}\\ 23 = 9\\) \\) Then the key will be mapped to index \\(9\\) of the hash table. -4ex -1ex -.4ex 1ex .2ex Collision Handling Collision occurs when different elements are mapped to the same index of the arraywhen \\(h(k_1) = h(k_2)\\) , but \\(k_1 \\neq k_2\\) . ::: center ::: Avoiding collision is ideal, nonetheless, it is impossible, so we use closed or open addressing to overcome this problem. Each of them have their pros and cons. -3ex -0.1ex -.4ex 0.5ex .2ex Closed Addressing Closed addressing (or open hashing) is also known as separate chaining. When collision occurs, the index keeps a reference to a linked list or dynamic array that stores all items with the same index. Let \\(e_1\\) and \\(e_2\\) represent the values attached to \\(k_1\\) and \\(k_2\\) respectively. ::: center ::: Separate chaining is fairly simple to implement and faster than open addressing in general. However, it is memory inefficient as it requires a secondary data structure and longs chains can result in \\(\\text{O}(n)\\) times. -3ex -0.1ex -.4ex 0.5ex .2ex Open Addressing Instead of referencing to a list or an array, open addressing (or closed hashing) resolves collision by searching for another empty bucket. ::: center ::: There's three types of open addressing: Linear Probing. When collision occurs, we linearly probe for the next bucket by increasing the index linearly until it finds an empty bucket: \\( \\(\\text{Index} = \\big[h(k) + i\\big]\\ \\text{mod}\\ n\\) \\) where \\(i\\) increases by one each iteration, until it finds an empty bucket. Quadratic Probing. Similar to the previous one, but instead we increase the index quadratically until it finds an empty bucket: \\( \\(\\text{Index} = \\big[h(k) + i^2\\big]\\ \\text{mod}\\ n\\) \\) where \\(i\\) increases by one each iteration, until it finds an empty bucket. Double Hashing. Using a secondary hash function \\(h'(k)\\) , it places the colliding item in the first available cell by: \\( \\(\\text{Index} = \\big[h(k) + jh'(k)\\big]\\ \\text{mod}\\ n\\) \\) where \\(j\\) increases by one each iteration, until it finds an empty bucket. The secondary hash function cannot have zero values and is typically written as such: \\( \\(h'(k) = q -(k\\ \\text{mod}\\ q)\\) \\) where \\(q\\) is a prime number, such that \\(q > n\\) . Unlike separate chaining, open addressing is more memory efficient, as it stores element in empty indices. However, it can create cluster: Linear probing can result in primary clustering. Quadratic probing can result in secondary clustering. Compared to the two, double hashing distributes the keys more evenly and produces a uniform distribution of records throughout the hash table.","title":"Hash Tables"},{"location":"W2022/COE428/COE428/#trees","text":"-4ex -1ex -.4ex 1ex .2ex Introduction A tree is a dynamic set of nodes storing elements in a parent-child relationship (edge) with the following properties: It has a special node called root. Each node different from the root has a parent node. There is a single unique path along the edges from the root to any particular nodedoesn't have any cycles. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Tree Terminology In a tree, we often refer to certain parts of tree, which are listed below. For reference: ::: center ::: Root: The top element with no parent ( \\(A\\) ). Siblings: Children of the same parent ( \\(G, H\\) both have the parent \\(C\\) ). External node: Also referred to as leave, ndoes with no children ( \\(E, I, J, K, G, H\\) ). Internal node: nodes with one or more children ( \\(A, B, C, F\\) ). Ancestors: A node that is connected to all lower-level node ( \\(A, B, F\\) are ancestors of \\(I, J, K\\) ). Descendants: The connected lower-level nodes ( \\(I\\) is a descendant of \\(A, B, F\\) ). Depth of a node: Number of ancestors ( \\(I\\) has a depth of \\(3\\) ). Height of a tree: The max node depth (The height of tree is \\(3\\) ). Sub-tree: A tree consisting of a node and all its descendants (Refer to the red triangle above). -3ex -0.1ex -.4ex 0.5ex .2ex Tree Traversals A traversal is defined as a systematic way of accessing or visiting all nodes of a tree. Let's use the following tree as an example: ::: center ::: There's three ways a tree can be traverse, but we'll only go over two of them. The last one will be covered in the next section. Preorder traversal. Root is visited first and then sub-trees rooted at its children are visited recursively ( \\(A \\to B \\to D \\to E \\to C \\to F \\to G\\) ). Postorder traversal. Recursively traverse the sub-trees rooted at children and then visit the root itself ( \\(D \\to E \\to F \\to G \\to B \\to C \\to A\\) ). -4ex -1ex -.4ex 1ex .2ex Binary Search Tree Search trees are designed to support efficient search operations, including [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps}. A binary tree is a tree with the following: Each internal node has at most two children. The children of a node are an ordered pairleft child, right child and left sub-tree, right sub-tree. The keys satisfy the binary-search tree property: \\(u.key \\leq v.key \\leq w.key\\) Node \\(u\\) is a node (any node) in the left sub-tree of node \\(v\\) . Node \\(w\\) is a node (any node) in the right sub-tree of node \\(v\\) . ::: center ::: In other words, the value of the key of the parent should be between the value of the key of the left child and right child. A binary search tree (BST) is organized, as the name suggests, in a binary tree, where [ root[T] ]{style=\"background-color: light-gray\"} points to the root of tree [ T ]{style=\"background-color: light-gray\"} and each node contains the fields: [ key ]{style=\"background-color: light-gray\"} (and possibly other satellite data) [ left ]{style=\"background-color: light-gray\"} which points to left child. [ right ]{style=\"background-color: light-gray\"} which points to right child. [ p ]{style=\"background-color: light-gray\"} which points to parent, where [ p[root[T]] = nil ]{style=\"background-color: light-gray\"} -3ex -0.1ex -.4ex 0.5ex .2ex Inorder Traversal The binary-search tree property allows us to print out all the keys in sorted tree by a simple recursive algorithm, called an inorder tree walk, which can be visualized as such: ::: center ::: \\( \\(D \\to B \\to E \\to A \\to F \\to C \\to G\\) \\) How [Inorder-Tree-Walk]{.smallcaps} works: Check to make sure that [ x ]{style=\"background-color: light-gray\"} is not [ nil ]{style=\"background-color: light-gray\"}. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s left sub-tree. Print [ x ]{style=\"background-color: light-gray\"}'s key. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. ::: algorithm [Inorder-Tree-Walk]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Querying Binary search tree can support such queries as [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} operations. The [Tree-Search]{.smallcaps} procedure starts at the root and traces a simple path downward in the tree. The running time is \\(\\text{O}(h)\\) , where \\(h\\) is the height of the tree. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: The [Iterative-Tree-Search]{.smallcaps} is more efficient in which works by \\\"unrolling\\\" the recursion into a while loop. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: The binary search-tree property guarantees that: the leftmost node is the minimum key of the binary search tree the rightmost node is the maximum key of the binary search tree Thus, the [Tree-Minimum]{.smallcaps} and [Tree-Maximum]{.smallcaps} procedure traverse the appropriate points until [nil]{.smallcaps} is reached. The running time for both is \\(\\text{O}(h)\\) . ::: algorithm [Tree-Minimum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: ::: algorithm [Tree-Maximum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: Before going over the procedure for successor and predecessor, let's define what it means. Assuming all keys are unique, if \\(x\\) has two children: The successor is the minimum value in its right sub-tree. The predecessor is the maximum value in its left sub-tree. Refer to this example using the key value of \\(25\\) : ::: center ::: If you recall from earlier, when we performed inorder traversal, we can find the successor and predecessor based entirely on the tree structure. ::: algorithm [Tree-Successor]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: We can break the code for [Tree-Successor]{.smallcaps} into two cases: If [ x.right ]{style=\"background-color: light-gray\"} is non-empty, then [ x ]{style=\"background-color: light-gray\"}'s successor is the minimum in [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. If [ x.right ]{style=\"background-color: light-gray\"} is empty, then go up the tree until the current node is a left child. If you cannot go up further and you reached root, then [ x ]{style=\"background-color: light-gray\"} is the largest element. For example, if we want to find the successor of the key value of \\(20\\) : The right sub-tree is empty, so we go up the tree to the key value of \\(19\\) . Since \\(20\\) is not a left child or located in the left sub-tree of \\(19\\) , go up the tree to the key value of \\(15\\) . Likewise, it is not a left child of \\(15\\) , so go up the tree to the key value of \\(25\\) . The key value of \\(25\\) has \\(20\\) as a left child, therefore, the successor of \\(20\\) is \\(25\\) . Refer to the diagram below: ::: center ::: The [Tree-Predecessor]{.smallcaps} procedure is symmetric to [Tree-Predecessor]{.smallcaps} procedure, which instead uses [ x\u2006.\u2006left ]{style=\"background-color: light-gray\"}. The running time for both is \\(\\text{O}(h)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion The operations of insertion and deletion cause the dynamic set represented by a binary search tree to change. Thus, the binary-search tree property must hold after this change. The [Tree-Insert]{.smallcaps} procedure works quite similar to [Tree-Search]{.smallcaps} and [Iterative-Tree-Search]{.smallcaps}, which begins at the root of the tree. ::: algorithm [Tree-Insert]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: In the code, we are trying to insert [ z ]{style=\"background-color: light-gray\"} to the tree [ T ]{style=\"background-color: light-gray\"}: The pointer [ x ]{style=\"background-color: light-gray\"} traces a simple path downward looking for a [nil]{.smallcaps} to replace with the input [ z ]{style=\"background-color: light-gray\"}. The trailing pointer [ y ]{style=\"background-color: light-gray\"} maintains the parent of [ x ]{style=\"background-color: light-gray\"}. Suppose we want to insert an item with key \\(9\\) . The while loop in lines 3-8 can be expressed as: ::: center ::: The [nil]{.smallcaps} occupies the position where we wish to place the input item [ z ]{style=\"background-color: light-gray\"}. The lines 10-15 set the pointers that cause [ z ]{style=\"background-color: light-gray\"} to be inserted. Deletion is somewhat more tricky than insertion. The process for deleting node [ z ]{style=\"background-color: light-gray\"} can be broken into three cases: Case One: If [ z ]{style=\"background-color: light-gray\"} has no children, then we simply remove it by modifying it's parent to replace [ z ]{style=\"background-color: light-gray\"} with [nil]{.smallcaps}. ::: center ::: Case Two: If [ z ]{style=\"background-color: light-gray\"} has one child, then delete [ z ]{style=\"background-color: light-gray\"} by making the parent of [ z ]{style=\"background-color: light-gray\"} point to [ z ]{style=\"background-color: light-gray\"}'s child, instead of [ z ]{style=\"background-color: light-gray\"}. ::: center ::: Case Three: If [ z ]{style=\"background-color: light-gray\"} has two children, then delete [ z ]{style=\"background-color: light-gray\"}'s successor, [ y ]{style=\"background-color: light-gray\"}, from the tree (via Case One or Case Two) and replace [ z ]{style=\"background-color: light-gray\"}'s key and satellite data with [ y ]{style=\"background-color: light-gray\"}. ::: center ::: The [Tree-Delete]{.smallcaps} procedure executes the three cases as follows. The running time is \\(\\text{O}(h)\\) . ::: algorithm [Tree-Delete]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: As you may have notice, the running time for these operations: [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps} all take \\(\\text{O}(h)\\) , where \\(h\\) is the height of the tree. These operations are fast if the height of the tree is small. For binary search trees given we have \\(n\\) items, the minimum height of a binary tree can be \\(\\log{n}\\) and the maximum be \\(n\\) . It can be depicted as such: ::: center ::: Ideally we want make sure the height of the binary tree is always \\(\\log{n}\\) , as it provides the worst-case running time of \\(\\text{O}(\\log{n})\\) , thus comes the motivation for the next topic. -3ex -0.1ex -.4ex 0.5ex .2ex Balanced Search Tree One way we can ensure our tree is always balanced is by implementing a self-balancing binary search tree. A search-tree data structure for which a height of \\(\\log{n}\\) is guaranteed when implementing dynamic set of \\(n\\) items. AVL Tree Red-Black Tree It ensures the \\(\\text{O}(\\log{n})\\) time complexity at all times, by maintaining the binary-search tree property and height-balance property of the tree, whenever insertion or deletion is performed. -4ex -1ex -.4ex 1ex .2ex Red-Black Trees Red-black trees are one of many search-tree schemes that are \"balanced\" in order to guarantee that basic dynamic-set operations take \\(\\text{O}(\\log{n})\\) time in the worst case. ::: center ::: It is a binary tree that satisfies the following red-black properties: Every node is either red or black. The root and leaves ([nil]{.smallcaps}) are black. If a node is red, then both of its children are black. For each node, all simple paths from the node to ([nil]{.smallcaps}) descendant leaves contain the same number of black nodes. To expand more on property 4, let's find the black-height of the key value of \\(7\\) . These are all the simple paths that can be taken indicated by the grey dashed arrow above: 7, 3, [nil]{.smallcaps} 7, 18, 10, 8, [nil]{.smallcaps} 7, 18, 10, 11, [nil]{.smallcaps} 7, 18, 22, 26, [nil]{.smallcaps} If we don't include the root node, notice how all simple paths consists of the (same number of) \\(2\\) black nodes. Likewise, the same can be said for every node in the tree. The red-black tree is a BST, so we can implement the dynamic-set operations [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} in \\(\\text{O}(\\log{n})\\) time. -3ex -0.1ex -.4ex 0.5ex .2ex Recoloring and Rotation However, it does not directly support the dynamic-set operations [Insert]{.smallcaps} and [Delete]{.smallcaps}. Because they modify the tree, the result may violate the red-black properties. We must change color of some nodes via recoloring Restructure the links of the tree via rotation For starters, let's go over the relationship in a binary tree: ::: center ::: There's two types of procedures called [Left-Rotate]{.smallcaps} and [Right-Rotate]{.smallcaps}. ::: center ::: The letters \\(\\alpha\\) , \\(\\beta\\) , and \\(\\gamma\\) represent an arbitrary sub-treeall of them have the same black-height. First, determine if recoloring needs to be done. Case One: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is red. Proceed with recoloring. Push \\(C\\) 's black onto \\(A\\) and \\(D\\) . Recurse and check for \\(C\\) 's uncle if it exists. ::: center ::: Case Two: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LR or RL imbalance. ::: center ::: There are four restructuring configurations depending on whether the double red nodes ( \\(A\\) and \\(B\\) ) are left or right children. ::: center ::: If there's a LR imbalance, perform [Left-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: If there's a RL imbalance, perform [Right-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: Case Three: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LL or RR imbalance. Preserve the color If there's a LL imbalance, perform [Right-Rotate]{.smallcaps} on top node. ::: center ::: If there's a RR imbalance, perform [Left-Rotate]{.smallcaps} on top node. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion To perform insertion, we insert [ x ]{style=\"background-color: light-gray\"} and color it red. The motivation for using the color red is that only property 2 and 3 might be brokenthese violation are fairly easy to fix. The [RB-Insert]{.smallcaps} procedure performs the following three cases describe in the previous section. The running time is \\(\\text{O}(\\log{n})\\) . Suppose we want to insert \\(15\\) using the red-black tree shown in the beginning, then we would insert as we normally would in a BST and color it red. Refer to the diagram below: ::: center ::: Just like deleting a node in a BST, it's just as complicated to delete a node in a red-black tree. The process for deleting node can be broken into three cases: Case One: If the deleted node is red, perform the deletion as you would in BST. No color changes should occur. ::: center ::: Case Two: If the deleted node is black and has one red child. Reattach the red child in place of the black node we removed, then recolor the red node as black to fix black-height of the tree. ::: center ::: Case Three: If the deleted node is black. Reattach a black child in place of the black node we removed, then recolor as a double black. ::: center ::: The double black is to keep track of where we violated the black depth property. Denoted as [ r ]{style=\"background-color: light-gray\"} and the sibling of [ r ]{style=\"background-color: light-gray\"} as [ y ]{style=\"background-color: light-gray\"}, we'll divide this into three sub-cases based on [ y ]{style=\"background-color: light-gray\"}: ::: list The color of [ x ]{style=\"background-color: light-gray\"}, parent of [ z ]{style=\"background-color: light-gray\"}, displayed can be black or red. These three sub-cases differ only on the color of [ y ]{style=\"background-color: light-gray\"}, sibling of [ r ]{style=\"background-color: light-gray\"}. ::: Case Three (a): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and has a red child [ z ]{style=\"background-color: light-gray\"}. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a left child, perform [Right-Rotate]{.smallcaps} on [ y ]{style=\"background-color: light-gray\"}, then proceed below. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a right child, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ x ]{style=\"background-color: light-gray\"} and [ z ]{style=\"background-color: light-gray\"} black, give [ y ]{style=\"background-color: light-gray\"} the former color of [ x ]{style=\"background-color: light-gray\"}, and color [ r ]{style=\"background-color: light-gray\"} black. ::: center ::: As you can see, we managed to achieve the same configuration as the original tree prior to the deletion of node. We basically converted the red node to be a black node, thus maintaining the red-black tree property. Case Three (b): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and both children of [ y ]{style=\"background-color: light-gray\"} are black. ::: center ::: If [ x ]{style=\"background-color: light-gray\"} is red, we color it black, then we color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: Otherwise, we only color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: In this case, we are essentially removing one black-height from the other sub-tree, to deal with the double black. Case Three (c): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is red. Perform an adjustment operation. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the right child of [ x ]{style=\"background-color: light-gray\"}, perform [Left-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the left child of [ x ]{style=\"background-color: light-gray\"}, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: The sibling of [ r ]{style=\"background-color: light-gray\"} should be black now, thus solve using Case Three (a) or Case Three (b). ::: list In Case Three (a) and Case Three (b), if [ r ]{style=\"background-color: light-gray\"} is in the right-side instead of the left-side, the direction of rotation changese.g. [Right-Rotate]{.smallcaps} instead of [Left-Rotate]{.smallcaps} and vice-versa, as we have done in Case Three (c). ::: -3ex -0.1ex -.4ex 0.5ex .2ex Comparing AVL and Red-Black Trees Since both provide dynamic-set operations in \\(\\text{O}(\\log{n})\\) time, which one to choose? AVL trees provide faster lookups than Red Black Trees because they are more strictly balanced. Red-Black Trees provide faster insertion and removal operations than AVL trees as fewer rotations are done due to relatively relaxed balancing. AVL trees store balance factors or heights with each node, thus requires storage for an integer per node whereas Red Black Tree requires only 1 bit of information per node. Red-Black Trees are used in most of the language libraries like map, multi-map, multi-set in C++ whereas AVL trees are used in databases where faster retrievals are required.","title":"Trees"},{"location":"W2022/COE428/COE428/#graph","text":"-4ex -1ex -.4ex 1ex .2ex Properties of a Graph A graph should consists of the following: Vertices (nodes), which specify some entities we are interested in. Edges (lines), which specify the relationship between entities. Weights (number in lines), which specify the weight the edge represent. The formal definition of a graph is a pair \\((V,E)\\) where: \\(V\\) is a collection of nodes, called vertices. \\(E\\) is a collection of pairs of vertices, called edges. ::: exampleT Example 8.1 . We can represent the following graph using the given vertices and edges: \\(V = \\{a,b,c,d,e,f\\}\\) \\(E = \\{(a,c),(b,c),(c,f),(b,d),(d,f),(c,d)\\}\\) ::: center ::: ::: A graph can be categorized into one of two types, depending on the edge type: Undirected Graph. Edges do not have a directionundirected edge are unordered pair of vertices, such that \\((u,v)\\) and \\((v,u)\\) are the same edge. ::: center ::: Directed Graph. Edges with directiondirected edges are ordered pair of vertices, such that \\(\\langle u,v \\rangle\\) and \\(\\langle v,u \\rangle\\) are two different edges. ::: center ::: To distinguish between the two edge types, we use round brackets \\(( )\\) for unordered pairs and angle brackets \\(\\langle \\rangle\\) for ordered pairs. -3ex -0.1ex -.4ex 0.5ex .2ex Graph Terminology We will go over a few graph terminologies, some of which you should be familiar with. The degree of a vertex is the number of incident edges of this vertex. Below are some examples. Pay close attention to the degree of vertex \\(z\\) . ::: center ::: Let \\(m\\) be the number of edges and \\(\\deg(a)\\) be the degree of vertex \\(a\\) , then \\( \\(\\sum_{a \\in V}\\deg(a) = 2m\\) \\) For undirected graphs, parallel edges are edges that have the same endpoints, whereas for directed graph, they are edges that have the same origin and destination. ::: center ::: Self-loop is an edge whose endpoints coincide, such as the edge \\((z,z)\\) ::: center ::: In this course, we will deal almost exclusively with simple graphs, which are graphs that do not have a parallel edge or self-loop. Let \\(n\\) be the number of vertices and \\(m\\) the number of edges, then \\( \\(m \\leq \\frac{n(n - 1)}{2}\\) \\) There are various definitions used to describe the movement in a graph: A path is a sequence of vertices, such that consecutive vertices are adjacent. A simple path is path such that all its vertices are distinct. ::: center ::: A cycle is a path on which the first vertex is equal to the last vertex. A simple cycle is a cycle such that all its vertices are distinct, except the first and last one. ::: center ::: Lastly, we'll cover the characteristics of a connected graph and the definition of a subgraph. A connected graph is a graph in which there is a path from any vertex to any other vertex in the graph. ::: center ::: We can say a tree is a connected graph without a cycleany two vertices are connected by exactly one path. ::: center ::: A subgraph of a graph \\((V,E)\\) is a pair \\((V', E')\\) where \\(V' \\subseteq V\\) and \\(E' \\subseteq E\\) . Both endpoints of edges in \\(E'\\) are in \\(V'\\) . ::: center ::: Then a spanning tree is a subgraph of a connected graph, which includes all vertices of the connected graph. ::: center ::: -4ex -1ex -.4ex 1ex .2ex Representations of Graphs We can choose between two standard ways to represent a graph \\(G = (V,E)\\) , as a collection of: Adjacency list Adjacency matrix Either way applies to both directed and undirected graph. They are useful in representing dense and sparse graphs: Sparse graphs. A graph with only a few edge. Dense graphs. The number of edges is close to the maximal number of edges. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency List The adjacency-list representation of a graph consists of an array \\(\\textit{Adj}\\) of \\(|V|\\) list, one for each vertex in \\(V\\) . For an undirected graph, the adjacency list \\(\\textit{Adj}[u]\\) contains all the vertices \\(v\\) such that there is an edge \\((u,v), (v,u) \\in E\\) . ::: center ::: Alternatively, you can think of it as a list of all the vertices adjacent to \\(u\\) . ::: center ::: <!-- --> - For a directed graph, the adjacency list \\(\\textit{Adj}[u]\\) contains all the vertices \\(v\\) such that there is an edge \\(\\langle u,v \\rangle \\in E\\) . ::: center ![image](Figure/8/Adjacency/adjacency_list2.png){height=\"3cm\"} ::: Alternatively, you can think of it as a list of destinations given the origin $u$. ::: center ![image](Figure/8/Adjacency/adjacency_list2a.png){height=\"2.5cm\"} ::: ::: list Note that in an adjacency list, the order doesn't matter, meaning we could have listed the vertex in any order. ::: A useful thing we could do with adjacency list is to represent weighted graphsedges with an associated weight to them. It can easily be done by storing it with vertex \\(v\\) in \\(u\\) 's adjacency list. ::: center ::: A potential disadvantage of the adjacency-list representation is that there is no quicker way to determine if a given edge \\((u,v)\\) is present in the graph. We would need to search for \\(v\\) in the adjacency list \\(\\textit{Adj}[u]\\) . If we want to check the edge \\((2,4)\\) , then we would need search through \\(\\textit{Adj}[2]\\) . ::: center ::: The worst-case running time would be the number of adjacent vertices, which is not ideal. A solution would be to use an adjacency-matrix representation, which requires a constant time \\(\\text{O}(1)\\) , but at the cost of using asymptotically more memory. -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency Matrix The adjacency-matrix representation of a graph consists of a \\(|V| \\times |V|\\) matrixassuming the vertices are numbered from \\(1\\) , \\(2\\) , ..., to \\(|V|\\) . We can represent the elements inside the matrix \\(A\\) as \\(a_{ij}\\) , where \\(i\\) and \\(j\\) indicate the row and column. For an undirected graph, if there is an edge \\((i,j), (j,i) \\in E\\) , then set \\(a_{ij} = 1\\) , otherwise, \\(a_{ij} = 0\\) . ::: center ::: Notice how \\(a_{22}\\) is \\(0\\) , since the edge \\((2,2)\\) does not exist. ::: center ::: For a directed graph, if there is an edge \\(\\langle i, j \\rangle \\in E\\) , then set \\(a_{ij} = 1\\) , otherwise, \\(a_{ij} = 0\\) . ::: center ::: Like the adjacency-list representation of a graph, an adjacency matrix can represent a weighted graph. Instead of storing \\(0\\) 's and \\(1\\) 's, we store the weight of the given edge. ::: center ::: If an edge does not exist, we can store a [nil]{.smallcaps} value, depicted as empty in the diagram above. -3ex -0.1ex -.4ex 0.5ex .2ex Comparison As we have demonstrated both are applicable to undirected and directed graphs, each with their own advantages and disadvantages. **Adjacency List** **Adjacency Matrix** **Space:** $\\Theta(|V + E|)$ $\\Theta(|V|^2)$ Time: List all vertices adjacent to \\(u\\) \\(\\Theta(\\deg(u))\\) \\(\\Theta(|V|)\\) Time: Determine if \\((u,v) \\in E\\) \\(\\Theta(\\deg(u))\\) \\(\\Theta(1)\\) The choice of which one to use comes down to the following criteria: The adjacency-list representation provides a compact way to represent sparse graphsthose for which \\(|E|\\) is much less than \\(|V|^2\\) . However, if \\(|E|\\) is close to \\(|V|^2\\) , then we may choose an adjacency-matrix representation since it almost have the same space complexity as the adjacency-list. Alternatively, if we need to be able to tell quickly if there is an edge connecting two given vertices, an adjacency-matrix representation is used. -4ex -1ex -.4ex 1ex .2ex Graph Traversals A traversal (or graph searching) is a systematic procedure for exploring a connected graph by examining all its vertices and/or edges. There's two types of traversal algorithms: Breadth-First Search (BFS) Depth-First Search (DFS) -3ex -0.1ex -.4ex 0.5ex .2ex Breadth-First Search Breadth-first search (BFS) is one of the simplest algorithms for searching a graph, which uses a queue data structure. For simplicity, we will use a tree to describe breadth-first search: Let's start at the root of tree. Let's add \\(A\\) to the queue. ::: center ::: We want to explore all the vertices that are adjacent to \\(A\\) , which are \\(B\\) and \\(C\\) . We will add them to the queue. ::: center ::: Note that the order they are placed in queue does not matter. We could have stored \\(C\\) first. Since we finished \\\"exploring\\\" \\(A\\) , we will move on, then \\(B\\) is next in queue. ::: center ::: Likewise, we add all vertices adjacent to \\(B\\) , which are \\(D\\) and \\(E\\) , to the queue. ::: center ::: Since we finished \\\"exploring\\\" \\(B\\) , we will move to \\(C\\) . ::: center ::: We will add the vertices adjacent to \\(C\\) . which are \\(F\\) and \\(G\\) to the queue. ::: center ::: When we move to vertex \\(D\\) , you will see there are no adjacent vertices, thus we don't add anything to the queue and move on to \\(E\\) . ::: center ::: The same can be said for \\(E\\) through \\(G\\) , in which we finish our breadth-first search. ::: center ::: This is a simplified explanation, but should provide a general idea of how it works. We associate the vertex colors to guide the algorithm: White vertices have not been discovered. All vertices start out white. Grey vertices are discovered but not fully explored. Black vertices are discovered and fully explored. The algorithm attaches several attributes to each vertex, such as color [ u.color ]{style=\"background-color: light-gray\"}, parent [ u.\\pi ]{style=\"background-color: light-gray\"}, and distance [ u.d ]{style=\"background-color: light-gray\"} which is computed by the algorithm. To keep track of progress, breadth-first search colors each vertex white, grey, or black. Distance is used to represent the smallest number of edges that must be traverse from the starting vertex [ s ]{style=\"background-color: light-gray\"} to end vertex [ v ]{style=\"background-color: light-gray\"}. ::: algorithm [BFS]{.smallcaps} \\((G,s)\\) ::: algorithmic ::: ::: We start off by painting every vertex [ white ]{style=\"background-color: light-gray\"} except our starting vertex [ s ]{style=\"background-color: light-gray\"} and setting the distance to [ \\infty ]{style=\"background-color: light-gray\"}, as we not sure how far it is from the starting vertex or whether it is even reachable. ::: algorithm ::: algorithmic ::: ::: Then we paint our starting vertex [ s ]{style=\"background-color: light-gray\"} to [ grey ]{style=\"background-color: light-gray\"}, set the distance to [ 0 ]{style=\"background-color: light-gray\"}, since it's our starting point. The parent of [ s ]{style=\"background-color: light-gray\"} is [ nil ]{style=\"background-color: light-gray\"}doesn't exist. ::: center ::: Note that vertex [ s ]{style=\"background-color: light-gray\"} is something that we chose, which in this example is [ 5 ]{style=\"background-color: light-gray\"}. ::: algorithm ::: algorithmic ::: ::: Then we initialize [ Q ]{style=\"background-color: light-gray\"} to the queue containing just the vertex [ s ]{style=\"background-color: light-gray\"} which is [ 5 ]{style=\"background-color: light-gray\"}. ::: center ::: ::: algorithm ::: algorithmic ::: ::: The [ while ]{style=\"background-color: light-gray\"} loop functions similarly to what we have demonstrated in the first example. ::: center ::: The total running time of the [BFS]{.smallcaps} procedure is \\(\\text{O}(V + E)\\) . As you may have notice, it is particularly useful for finding the shortest path from the starting vertex [ s ]{style=\"background-color: light-gray\"} to some vertex [ v ]{style=\"background-color: light-gray\"} in the graph. -3ex -0.1ex -.4ex 0.5ex .2ex Depth-First Search Depth-first search (DFS) as the name implies, searches \\\"deeper\\\" first until it cannot go further at which point it backtracks and continues, which uses a stack data structure. Let's use the same tree as we have used for BFS, to compare the difference: As before, we will start at the root of three. Let's add \\(A\\) to the stack. ::: center ::: Then we arbitrarily pick an edge outwards of \\(A\\) , which will choose \\(B\\) and add to the stack. ::: center ::: Note there's multiple ways, so we could have also chosen to go with vertex \\(C\\) instead. Continue to pick an edge outwards, which there is only one, so will choose \\(D\\) . ::: center ::: Since there's no more vertices to explore, we backtrack to vertex \\(B\\) . ::: center ::: Continue to pick an edge outwards that has not been visited yet, which is vertex \\(E\\) . ::: center ::: Then we backtrack all the way to vertex \\(A\\) , since all of vertex \\(B\\) and \\(E\\) has been explored. ::: center ::: Then, we repeat the same steps for the right sub-tree, by picking some arbitrary edge outwards until we have fully discovered every vertex. DFS uses the same color scheme as we previously described in BFS. However, one unique thing about the algorithm is the it uses two timestamps for: when it first discovers the vertex [ u.d ]{style=\"background-color: light-gray\"} and ... when it finishes exploring the vertex [ u.f ]{style=\"background-color: light-gray\"}. This is similar to BFS, which we paint every vertex [ white ]{style=\"background-color: light-gray\"}. The [ time ]{style=\"background-color: light-gray\"} is set to [ 0 ]{style=\"background-color: light-gray\"}, which will use to compute the discovery time [ u.d ]{style=\"background-color: light-gray\"} and finishing time [ u.f ]{style=\"background-color: light-gray\"}. ::: algorithm [DFS]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: For this example, we'll use a directed graph as it's \\\"easier\\\" to pick an edge outwards and demonstrate. ::: center ::: For demonstration purposes, the discovery time and finishing time is denoted below the vertex, shown on the right. ::: algorithm [DFS-Visit]{.smallcaps} \\((G,u)\\) ::: algorithmic ::: ::: We'll choose vertex [ 8 ]{style=\"background-color: light-gray\"} as our starting vertex. It'll recursively call [ DFS-Visit ]{style=\"background-color: light-gray\"} until it reaches a dead end then it colors the vertex black. ::: center ::: Once the recursion finishes, it goes to the next [ v ]{style=\"background-color: light-gray\"} that is in [ Adj[u] ]{style=\"background-color: light-gray\"} and repeats the same thing. ::: center ::: Since there's no more [ v ]{style=\"background-color: light-gray\"} (or vertex to explore) in [ Adj[u] ]{style=\"background-color: light-gray\"}. We pick a new [ u ]{style=\"background-color: light-gray\"}. ::: center ::: The total running time of the [DFS]{.smallcaps} procedure is \\(\\text{O}(V + E)\\) , similar to [BFS]{.smallcaps} procedure. It provides valuable information about the structure of a graph and is more suitable for decision treelike a maze. We can use depth-first search to perform a topological sort of a directed acyclic graph (dag)a directed graph with no cycles. ::: center ::: By performing topological sort, we can find the topological orderingordering of its vertices along a horizontal line so that all directed edges go from left to right. ::: center ::: Note how the order they are listed in regarding their finishing times. The [Topological-Sort]{.smallcaps} sorts a dag by: ::: algorithm [Topological-Sort]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: We can perform [Topological-Sort]{.smallcaps} in \\(\\text{O}(V + E)\\) time, since DFS takes \\(\\text{O}(V + E)\\) .","title":"Graph"},{"location":"W2022/COE428/COE428/#elementary-graph-algorithms","text":"","title":"Elementary Graph Algorithms"},{"location":"W2022/COE428/COE428/#minimum-spanning-tree","text":"The minimum spanning tree is the spanning tree of a weighted graph with minimum total edge weight. What this means, is suppose we have a connected graph with weighted edges. ::: center ::: We are interested in the subset of the edges which connects all vertices together, while minimizing the total edge cost. We can form three variations of the graph's spanning tree: ::: center ::: As you can see, tree 1 in particular is our minimum spanning tree because it posses the minimum total edge weight of \\(71\\) . Obviously when there's more vertices and edges, it will be much harder to figure out the MST. And so we'll go over two types of algorithms which does the following: Kruskal's algorithm Prim's algorithm For both algorithm, we will try and find the minimum spanning tree of the following completed graph below and compare them after. ::: center :::","title":"Minimum Spanning Tree"},{"location":"W2022/COE428/COE428/#kruskals-algorithm","text":"This algorithm creates a forest of trees. It works by picking the smallest edge then checking if it forms a cycle with the spanning tree formed so far. If not, the edge is added, otherwise, discard it. We start off by selecting the smallest edge in the graph, which is \\((6,1)\\) , as it has an edge weight of \\(10\\) . ::: center ::: Then, we select the next smallest edge, which is \\((4,3)\\) , with an edge weight of \\(12\\) . ::: center ::: The next minimum edge is \\((7,2)\\) , with an edge weight of \\(14\\) . ::: center ::: The next minimum edge is \\((2,3)\\) , with an edge weight of \\(16\\) . ::: center ::: The next minimum edge is \\((7,4)\\) , with an edge weight of \\(18\\) . However, notice that it forms a cycle, so we discard it. ::: center ::: Instead, we go to the next minimum edge, \\((5,4)\\) , with an edge weight of \\(22\\) . ::: center ::: The next minimum edge is \\((5,6)\\) , with an edge weight of \\(25\\) . ::: center ::: Once we have all our vertices or have \\(|V| - 1\\) edges, we are done with the algorithm. Our MST is complete, which has the weight of \\(99\\) .","title":"Kruskal's Algorithm"},{"location":"W2022/COE428/COE428/#prims-algorithm","text":"This algorithm starts with one node. It then adds a node one by one that is unconnected to the new graph. For this example, let's start at vertex \\(6\\) . Since we started with \\(6\\) , we have to select the smallest edge that is connected to \\(6\\) , which are either: \\(25\\) or \\(10\\) . ::: center ::: We'll select the edge weight of \\(10\\) , since that's the smallest. Likewise, we now have to select the smallest edge that is connected to either \\(6\\) or \\(1\\) , which will pick edge \\((6,5)\\) , with an edge weight of \\(25\\) . ::: center ::: We are basically repeating the same step, but now with more and more edges to pick from. The dashed lines indicate the possible edges to select. ::: center ::: While it was not demonstrated in this example, but if an edge is selected and it forms a cycle, we will discard it and choose the next minimum edge, similar to Kruskal's algorithm. Once we have all our vertices or have \\(|V| - 1\\) edges, we are done with the algorithm, which also have a weight of \\(99\\) . As you can see, we have obtained the same MST for both Kruskal and Prim's algorithm, so which one to choose? Kruskal's algorithm runs faster in sparse graph, with the time complexity of \\(\\text{O}(E\\log{V})\\) . Prim's algorithm runs faster in dense graph, with the time complexity of \\(\\text{O}(E\\log{V})\\) , but it can be improved up to \\(\\text{O}(E + V\\log{V})\\) using Fibonacci heaps.","title":"Prim's Algorithm"},{"location":"W2022/COE428/COE428/#shortest-path","text":"We define the shortest path as the minimum length path from a vertex to another vertex in \\(G\\) , if such a path exists. Previously, we have done something similar with breadth-first search with an unweighted graph, in which each edge has a weight of \\(1\\) . ::: center ::: We are particularly interested in the single-source shortest path's problem, that is given a graph \\(G = (V,E)\\) , what is the shortest path from a given source vertex \\(s \\in V\\) to each vertex \\(v \\in V\\) . ::: center ::: Suppose we chose \\(A\\) as our source vertex, then the shortest-paths tree are: ::: center ::: If you calculate the weight from the source vertex \\(A\\) to every other vertex, you will get the minimum edge weight possible. From \\(A\\) to \\(B\\) , the total edge weight is \\(3\\) . From \\(A\\) to \\(C\\) , the total edge weight is \\(5\\) . From \\(A\\) to \\(D\\) , the total edge weight is \\(9\\) . From \\(A\\) to \\(E\\) , the total edge weight is \\(11\\) . You also may have notice that the shortest-paths tree are not unique, meaning there can also be more than one shortest-path tree, nonetheless should still provide the same answers. In this section, we'll go over two algorithms which does the following: Dijkstra's algorithm Bellman-Ford's algorithm As will demonstrate, each algorithm have their advantages and disadvantages when choosing one over the other. Both algorithm uses the [Relax]{.smallcaps} procedure, but implement them in varying ways. ::: algorithm [Relax]{.smallcaps} \\((u,v,w)\\) ::: algorithmic ::: ::: The process of \\\"relaxing\\\" an edge is to check if its worth going through the edge \\(\\langle u,v \\rangle\\) which would improve the shortest path from source vertex to some vertex [ v ]{style=\"background-color: light-gray\"}. Dijkstra's Algorithm relax each edge exactly once. Bellman-Ford's Algorithm relaxes each edge \\(|V| - 1\\) times. For example, assume we have the obtained the following weighted, directed graph and we wanna find the shortest path to vertex \\(D\\) . Suppose we start by relaxing the edge \\(\\langle C,D \\rangle\\) . ::: center ::: Every vertex initially start with a distance of \\(\\infty\\) and since \\(\\infty > 7 + 6 = 13\\) , we used the edge \\(\\langle C,D \\rangle\\) . But now let's relax the other edge, \\(\\langle B,D \\rangle\\) . ::: center ::: Since \\(13 > 6 + 4 = 10\\) , we now use the edge \\(\\langle B,D \\rangle\\) instead of \\(\\langle C,D \\rangle\\) .","title":"Shortest Path"},{"location":"W2022/COE428/COE428/#dijkstras-algorithm","text":"As we shall see, Dijkstra's algorithm is pretty similar to Prim's algorithm, which we covered in Minimum Spanning Tree. One limitation of this algorithm is that all edge weights must be non-negative. Let's pick \\(A\\) as our source vertex. There's a table in the right-side, which will use to keep track of distances from the source vertex to each vertex. ::: center ::: We put infinity for the other vertices as we haven't visited them yet. A change in the distance will be indicated by the grey boxes in each step. Next, we examine the edges leaving \\(A\\) . As denoted in the table, we can reach \\(B\\) and \\(C\\) with an edge weight of \\(10\\) and \\(3\\) respectively. ::: center ::: We always pick the smallest edge weight of which the vertex hasn't been discovered. In this case, it's \\(\\langle A,C \\rangle\\) . ::: center ::: Notice how we don't include the edge \\(\\langle A,B \\rangle\\) in consideration as \\(B\\) is now reachable from \\(\\langle A,C \\rangle\\) and \\(\\langle C,B \\rangle\\) with a smaller total edge weight of \\(7\\) . Now \\(D\\) and \\(E\\) are now reachable. As usual, we'll pick the next smallest edge, which is \\(\\langle C, E \\rangle\\) . ::: center ::: Similar to what was described before, we don't consider the edge \\(\\langle E, D \\rangle\\) , since \\(D\\) has a shortest path using the edge \\(\\langle C, D \\rangle\\) instead of \\(\\langle E, D \\rangle\\) . Our only options left are \\(\\langle C,B \\rangle\\) and \\(\\langle C,D \\rangle\\) . We pick \\(\\langle C, B \\rangle\\) , as it has the smallest edge weight of \\(7\\) . ::: center ::: From \\(A\\) to \\(D\\) , the edge \\(\\langle C, D \\rangle\\) will results in a distance of \\(11\\) , while \\(\\langle B,D \\rangle\\) will result in a distance of \\(9\\) . So all that's left is to pick the last remaining edge, which is \\(\\langle B, D \\rangle\\) . ::: center ::: All the edges have been discovered, so we are done with the algorithm.","title":"Dijkstra's Algorithm"},{"location":"W2022/COE428/COE428/#bellman-fords-algorithm","text":"Bellman-Ford's algorithm is more general than Dijkstra's algorithm, such that it can deal with negative edge weights. However, it is a bit more time consuming in comparison. As usual, let start with vertex \\(A\\) as our source vertex and the distances to each vertex listed in the right. ::: center ::: With Bellman-Ford's algorithm, we want to relax all the edges. In other words, we should test out all the possible edges that will result in the shortest path. So this will be our first iteration. Starting at \\(A\\) , we can reach \\(B\\) and \\(C\\) with a weight of \\(10\\) and \\(3\\) , using the edges, \\(\\langle A,B \\rangle\\) and \\(\\langle A,C \\rangle\\) respectively. ::: center ::: From \\(B\\) , we can reach \\(D\\) at a total weight of \\(12\\) using the edge \\(\\langle B,D \\rangle\\) . Note that we won't use the edge \\(\\langle B,C \\rangle\\) , as it result in a longer path from \\(A\\) to \\(C\\) . ::: center ::: From \\(C\\) , we can reach \\(E\\) at a total weight of \\(5\\) using the edge \\(\\langle C,E \\rangle\\) . ::: center ::: Also, notice if we use the edge \\(\\langle C,B \\rangle\\) , we will get a shorter path to \\(B\\) with a total weight of \\(7\\) . Consequently, it also lowers the total weight of \\(D\\) to \\(9\\) . ::: center ::: Since we found a better path, we'll remove the edge \\(\\langle A,B \\rangle\\) . From \\(D\\) , we only have one edge to work with, which is \\(\\langle D,E \\rangle\\) , however, notice how this increases the total weight of \\(E\\) from \\(5\\) to \\(16\\) , so, we don't use it. ::: center ::: Similarly for \\(E\\) , we have the edge \\(\\langle E,D \\rangle\\) , however, this also increase the weight of \\(D\\) by \\(9\\) to \\(12\\) , so we don't use it. ::: center ::: Since we have checked all vertex. we're done with our first iteration. The algorithm at most takes \\(|V| - 1\\) iterations to fully obtain the shortest-tree path. However, it can sometimes be less if there's no changes occurring after the next iteration. In our second iteration, we proceed to the do the following, but using the graph we got from our first iteration. ::: center ::: Our only option is \\(\\langle A,B \\rangle\\) , however, this doesn't improve the distances of \\(B\\) , so we don't update the graph. Same thing can be said for \\(B\\) , \\(C\\) , \\(D\\) and \\(E\\) . So after our second iteration, we are finished. Remark In the actual exam, they might provide you the edges in which you should relax them by order. The process will still be the same as described from above, but the order may be different, as they might ask you to relax the edges of \\(E\\) before \\(D\\) or etc. One useful property of Bellman-Ford algorithm is that we can also use it to check for the existence of negative cycleone in which the overall sum of the cycle becomes negative. ::: center ::: If you add the weights of its edges, it's negative ( \\(-6 + 3 + 2 = -1\\) ). The concept of a shortest path is meaningless if there is a negative cycle, as we'll have a continuous loop. Refer to the example below. To demonstrate this, let's use the following weighted, directed graph and set \\(A\\) as the source vertex. As you can see, we will loop through the cycle \\(B\\) , \\(C\\) , and \\(D\\) , such that total weight keeps decreasing. ::: center ::: As mentioned before, Bell-man Ford's algorithm runs for \\(|V| - 1\\) iterations and it guarantees that at the end, the distances are guaranteed to be correct or [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is false. ::: center ::: However, as you can see that is not the case, such that [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is still true, thus, it will detect a negative cycle. As a summary, comparing the two algorithms we've covered in this section: The Dijkstra's algorithm is less time consuming, as it has a time complexity of \\(\\text{O}(E\\log{V})\\) , compared to Bellman-Ford's algorithm which is \\(\\text{O}(VE)\\) . Bellman-Ford works when there is negative weight edge, it also detects the negative weight cycle. Dynamic programming approach is taken to implement the algorithm. While Dijkstra's algorithm doesn't work when there is a negative weight edge. Greedy approach is taken to implement the algorithm.","title":"Bellman-Ford's Algorithm"},{"location":"W2022/ELE404/ELE404/","text":"Introduction This will cover various topics for the course ELE404: Electronic Circuits I, using the textbook, Microelectronic Circuits , by A. Sedra, K. Smith, T. Carusone, and V. Gaunet, and lectures notes provided by the professor, Dr. Fei Yuan. Other resources used: The textbook, Electronic Devices (Electron Flow Version) , by T. Floyd Last Updated: 2022-04-17 Diodes and Their Application \u00b6 Semiconductors \u00b6 The first few topics covered in beginning of the module, might be reminiscent of topics covered in PCS224, so it won't go to in-depth. As a recap, a link to the notes from Fall 2021: Application of Diodes \u00b6 We will now go over the actual application of diodes and ways to analyze them. -3ex -0.1ex -.4ex 0.5ex .2ex Diode Rectifier and Filter One of the most important applications of diodes is in the design of rectifier circuitsconvert an AC voltage to a DC voltage. There's two types: half-wave and full-wave rectifier. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Half-Wave Rectifier The half-wave rectifier utilizes alternate half-cycles of the input sinusoid. The circuit is shown below. ::: center ::: The circuit functions as follows: Positive Cycle: When \\(v_{in} \\geq v_D\\) , the output voltage \\(v_O = v_{in} - v_D\\) , due to the constant-voltage-drop diode. Negative Cycle: When \\(v_{in} < v_D\\) , the output voltage \\(v_O = 0\\) , since the diode is reverse-biased. Note that for an ideal diode, the voltage drop would be \\(v_D = 0\\) , as shown in the left. However, since these do not actually exists, we use a more realistic model, as shown in the right. ::: center ::: As you can see in the graph, a half-wave rectifier has a low energy efficiency as half of the input is wasted. -2ex -0.1ex -.2ex .2ex .2ex Half-Wave Rectifier with a Filter Capacitor In order to filter out the ripples, a large capacitor \\(C\\) is neededreducing the substantially the variations in the rectifier output voltage \\(v_O\\) . The circuit is shown below ::: center ::: As a result, when \\(v_{in}\\) drops from its peak, the capacitor will discharge, instead of a dropping all the way to \\(\\SI{0}{\\volt}\\) . ::: center ::: A closer look at the output voltage, we can derive a few equations. The peak voltage \\(V_P\\) is the maximum value of the DC source, where \\( \\(V_P = v_{in,\\ max} - v_D\\) \\) The ripple voltage \\(V_r\\) is the residual periodic variation of the DC voltagethe difference between \\(V_P\\) and the voltage when discharging ends. \\( \\(V_r = \\frac{T}{\\tau}V_P = \\frac{V_P}{fR_LC} = \\frac{i_L}{fC}\\) \\) The output DC voltage \\(\\overline{v_O}\\) can be obtained by taking the average of the extreme values of \\(v_O\\) . \\( \\(\\overline{v_O} = V_P - \\frac{1}{2}V_r\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Full-Wave Rectifier The full-wave rectifier has improve efficiency, by utilizing both halves of the sinusoid. The circuit is shown below. ::: center ::: The circuit functions as follows: Positive Cycle: When \\(v_{in} \\geq v_D\\) , the upper diode \\(D_1\\) will be ON, while lower diode \\(D_2\\) will be OFF, and the output voltage \\(v_O = v_{in} - v_D\\) . Negative Cycle: When \\(v_{in} \\leq -v_D\\) , the upper diode \\(D_1\\) will be OFF, while lower diode \\(D_2\\) will be ON, and the output voltage \\(v_O = |v_{in}| - v_D\\) . ::: center ::: The output voltage is shown below. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Full-Wave Rectifier with a Filter Capacitor Likewise, a large shunt capacitor is used to filter out voltage ripples. The circuit is shown below. ::: center ::: The only notable difference that can be seen from the graph is the period \\(T\\) cut in half. ::: center ::: The peak voltage \\(V_P\\) is still the same as previously shown, where \\( \\(V_P = v_{in,\\ max} - v_D\\) \\) For the ripple voltage \\(V_r\\) , the period \\(T\\) is replaced by \\(T/2\\) or \\(2f\\) , resulting in \\( \\(V_r = \\frac{V_P}{2fR_LC} = \\frac{i_L}{2fC}\\) \\) As in the half-wave case, the output DC voltage \\(\\overline{v_O}\\) doesn't change. \\( \\(\\overline{v_O} = V_P - \\frac{1}{2}V_r\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Bridge Rectifier A cheaper implementation of the full-wave rectifier with no center-tapped transformer. However, it uses two diodes in conjunction, resulting in a bigger voltage loss. The circuit is shown below. ::: center ::: The circuit functions as follows: Positive Cycle: When \\(v_{in} \\geq 2v_D\\) , the diodes \\(D_1\\) , \\(D_2\\) will be ON, while diodes \\(D_3\\) , \\(D_4\\) will be OFF, and the output voltage \\(v_O = v_{in} - 2v_D\\) . Negative Cycle: When \\(v_{in} \\leq 2v_D\\) , the diodes \\(D_1\\) , \\(D_2\\) will be OFF, while diodes \\(D_3\\) , \\(D_4\\) will be ON, and the output voltage \\(v_O = |v_{in}| - 2v_D\\) . ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Bridge Rectifier with a Filter Capacitor Let's apply a capacitor again to filter out the ripples. The circuit is shown below. ::: center ::: Like the full-wave rectifier, the period is also half \\(T/2\\) of the input wave. ::: center ::: However, the peak voltage has a bigger voltage loss of \\(2v_D\\) due to the two diodes. \\( \\(V_P = v_{in,\\ max} - 2v_D\\) \\) The ripple voltage \\(V_r\\) is the same as stated in the full-wave rectifier \\( \\(V_r = \\frac{V_P}{2fR_LC} = \\frac{i_L}{2fC}\\) \\) The same case for the output DC voltage \\(\\overline{v_O}\\) which also doesn't change. \\( \\(\\overline{v_O} = V_P - \\frac{1}{2}V_r\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Voltage Regulator A voltage regulator is a circuit whose purpose is to provide a constant DC voltage between its output terminalsto further reduce the voltage ripple. -2ex -0.1ex -.2ex .2ex .2ex Resistor-based Voltage Regulator The resistor-based voltage regulator is not the most ideal as you'll see why. The circuit is shown below ::: center ::: The output voltage \\(v_O\\) is given by \\( \\(v_O = \\frac{R_L \\parallel R_2}{R_L \\parallel R_2 + R_1}V_{CC}\\) \\) For an ideal voltage regulator, we want \\(v_O\\) to be independent of \\(R_L\\) , as it's something we have no control over. Though one may consider the case if \\(R_L \\gg R_1, R_2\\) , then \\(R_L \\parallel R_2 \\approx R_2\\) \\( \\(v_O \\approx \\frac{R_2}{R_1 + R_2}V_{CC}\\) \\) While \\(v_O\\) is independent of \\(R_L\\) , there are still some flaws in the design: (1) It is still proportional to \\(V_{CC}\\) resulting in \\(v_O\\) fluctuating based on \\(V_{CC}\\) (2) Since \\(R_1\\) and \\(R_2\\) is very small, a very large current will flow through them; resulting in a rapid increase of temperature and smoke. -2ex -0.1ex -.2ex .2ex .2ex Diode-based Voltage Regulator Suppose we now replace \\(R_2\\) with a forward-biased diode, as shown below. ::: center ::: If you recall, the equivalent of a forward-biased diode is to a small voltage source \\(\\SI{0.7}{\\volt}\\) in series with small resistance \\(r_D\\) . The ideal output voltage \\(v_O\\) is given by \\( \\(v_O = 0.7N\\) \\) where \\(N\\) is the number of diodes cascaded and \\(r_D = \\SI{0}{\\ohm}\\) . Note that \\(V_{CC} > 0.7N\\) is required. However, the actual value of \\(v_O\\) is determined by including \\(r_D\\) . By performing KCL at node \\(v_O\\) , we get \\( \\(G_L(v_O) + G(v_O - V_{CC}) + g_D(v_O - 0.7) = 0\\) \\) Note that the resistance is converted to conductance to minimize the use of fraction, where \\( \\(G = \\frac{1}{R} \\to G_L = \\frac{1}{R_L} \\text{ and } g_D = \\frac{1}{r_D}\\) \\) Isolating for \\(v_O\\) , we get the actual output voltage which is given by \\( \\(v_O \\approx 0.7 \\bigg[1 + \\frac{GV_{CC}}{0.7g_D} - \\frac{G_L + G}{g_D}\\bigg]\\) \\) We can determine the sensitivity of \\(v_O\\) to \\(R_L\\) and \\(V_{CC}\\) to see how much it fluctuates by \\( \\(\\frac{dv_O}{dR_L} = \\frac{0.7}{g_D} = 0.7r_D \\hspace{4cm} \\frac{dv_O}{dV_{CC}} = \\frac{G}{g_D} = \\frac{r_D}{R}\\) \\) Since \\(r_D\\) is typically a small value, where \\(r_D \\ll R, R_L\\) or \\(g_D \\gg G, G_L\\) , the output voltage is less sensitive to both \\(V_{CC}\\) and \\(R_L\\) , thus making it a good voltage regulator. -2ex -0.1ex -.2ex .2ex .2ex Zener-diode-based Voltage Regulator Let's now consider a Zener diode, which is reverse-biased, as shown below. ::: center ::: Likewise, we can also draw the equivalent circuit of a Zener diode as such. Then by performing KCL at node \\(v_O\\) , we get \\( \\(G_Lv_O + G(v_O - V_{CC}) + g_Z(v_O - V_{Z0}) = 0\\) \\) and isolating for \\(v_O\\) , the actual output voltage is given by \\( \\(v_O \\approx V_{Z0}\\bigg[1 + \\frac{GV_{CC}}{V_{Z0}g_Z} - \\frac{G_L + G}{g_Z}\\bigg]\\) \\) The sensitivity of \\(v_O\\) to \\(R_L\\) and \\(V_{CC}\\) are \\( \\(\\frac{dv_O}{dG_L} = -\\frac{V_{Z0}}{g_Z} = -V_{Z0}r_Z \\hspace{4cm} \\frac{dv_O}{dV_{CC}} = -\\frac{G}{g_Z} = \\frac{r_Z}{R}\\) \\) If you notice, the sensitivity of \\(v_O\\) for forward-bias and Zener diode are almost identical, so how exactly do we determine which is better? ::: center ::: If we were to compare the slope side-by-side, you will notice that \\(g_Z \\gg g_D\\) , which means there's less fluctuations when using a Zener diode voltage regulator oppose to a forward-biased diode. -3ex -0.1ex -.4ex 0.5ex .2ex Limiting and Clamping Circuits In this section, we will present other circuit applications of diodes apart from rectifier circuits. -2ex -0.1ex -.2ex .2ex .2ex Voltage Clipper As the name suggest, it takes an input waveform and clips or cuts off its top half, bottom half or both halves depending on the orientation and number of diode. ::: center ::: The circuit works by alternating between the two diodes in forward and reverse-bias, resulting in both halves being clipped. The sinusoidal wave is displayed below. ::: center ::: A few other variety of basic limiting circuits with respect to its transfer characteristics: ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Peak Detector A peak detector is used to detect maximum value of the input voltage. The circuit looks similar to that of a half-wave rectifier, just without a load connected onto. ::: center ::: Since there's no load to discharge on, the output voltage \\(v_O\\) remains constant. It will only rise when in forward-biased and there exists a \\(v_{in} > v_O\\) . The sinusoidal wave is displayed below. ::: center ::: Keep in mind, we assumed the diode is an ideal diode. For an actual diode, there will be some voltage drop \\(v_D\\) across the peak. Similar to a real half-wave rectifier, as discussed previously. -2ex -0.1ex -.2ex .2ex .2ex Voltage Doubler The last application of diode which will cover is a voltage doubler. ::: center ::: The circuit can be broken down into two sections: the positive and negative phase. ::: center ::: During the negative phase, \\(D_1\\) will be ON and \\(D_2\\) will be OFF, which leaves the capacitor \\(C_1\\) to charge up. ::: center ::: Then when it enters the positive phase, \\(D_1\\) will be OFF and \\(D_2\\) will be ON, the output voltage \\(V_O\\) is the sum of the input voltage and the voltage of \\(C_1\\) obtained in the previous half cycle. Bipolar Junction Transistors \u00b6 A bipolar transistor is a semiconductor device commonly used for amplificationbipolar as in both holes and electrons serve as current carriers. -4ex -1ex -.4ex 1ex .2ex Structure of BJT They are of two types of bipolar junction transistors (BJTs) namely; NPN transistor ::: center ::: PNP transistor ::: center ::: ::: list The direction of the arrow in the transistor symbol (between base and emitter) tells you the direction of current. ::: The BJT consists of three differently doped semiconductor region: the emitter region (e), the base region (b), and the collector region (c). Emitter: \\(n^+\\) for NPN transistors and \\(p^+\\) for PNP transistors. Base: \\(p\\) for NPN transistors and \\(n\\) for PNP transistors. Collector: \\(n\\) for NPN transistor and \\(p\\) for PNP transistors. If you notice the width is not symmetrical due to the doping ratios, where the emitter is heavily doped, the collector is moderately doped and the base is lightly doped. -4ex -1ex -.4ex 1ex .2ex Operation of BJT If you notice from the diagram earlier, the transistor consists of two pn junctions, the emitter--base junction (EBJ) and the collector--base junction (CBJ). Depending on the bias condition of each of these junctions, different modes of operation of the BJT are obtained, as shown below. ::: tabu c c c Mode & Emitter-Base Junction & Collector-Base Junction \\ Cut-Off & Reverse & Reverse\\ Active & Forward & Reverse\\ Saturation & Forward & Forward\\ ::: Let's define the bias conditions for modes of operation in the EBJ. ::: center ::: The order of subscript matter, which implies the order in which they are calculated. For NPN, \\(V_{BE} = V_B - V_E\\) , whereas for PNP, \\(V_{EB} = V_E - V_B\\) . Likewise, the same implies for \\(V_{CB}\\) and \\(V_{BC}\\) . Forward Bias: When \\(V_{BE} > \\SI{0.7}{\\volt}\\) or \\(V_{EB} > \\SI{0.7}{\\volt}\\) , electrons or holes in the emitter will exit and move to the base or collector. Reverse Bias: When \\(V_{BE} < \\SI{0.7}{\\volt}\\) or \\(V_{EB} < \\SI{0.7}{\\volt}\\) , no flow of electrons or holes from the emitter to the base or collector. ::: list Technically, the diode is forward-biased when \\(V > 0\\) and the diode is negative biased when \\(V < 0\\) . However, we account for the voltage drop in diode, in this scenario, which is \\(\\SI{0.7}{\\volt}\\) . ::: Figuratively, we can describe it using the \\(i\\) - \\(v\\) relationship of the emitter-base junction: ::: center ::: Note that the small region in between cut-off and active region is not the saturation regionwill get to this in a bit. Just know that we do not operate in this region. -3ex -0.1ex -.4ex 0.5ex .2ex Cut-Off Mode In cut-off mode, the EBJ is in reverse bias ( \\(V_{BE} < \\SI{0.7}{\\volt}\\) for NPN and \\(V_{EB} < \\SI{0.7}{\\volt}\\) for PNP), which means no emitter current, \\(i_E = 0\\) . Since \\( \\(i_E = i_C + i_B\\) \\) where the emitter current \\(i_E\\) is equal to the sum of the collector current \\(i_C\\) and base current \\(i_B\\) . We have \\(i_C = 0\\) and \\(i_B = 0\\) . The transistor is essentially inactive, thus appears as an open-circuit. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Active Mode Of the three modes, the active mode is the most important. In active mode, the EBJ is in forward bias ( \\(V_{BE} > \\SI{0.7}{\\volt}\\) for NPN and \\(V_{EB} > \\SI{0.7}{\\volt}\\) for PNP) and the CBJ is in reverse bias. We'll focus on the NPN transistor, but note that PNP transistor operates in the similar manner. As a reminder, current and electron flow are backwards: Since the EBJ is forward-biased (the diode is ON), free electrons from the emitter region can easily cross the emitter-base junction. ::: center ::: If you remember, the base is lightly doped, so only a small percentage of the free electrons can recombine with the holes in the base region, which produces a small \\(i_B\\) . ::: center ::: The free electrons that entered the base region but didn't recombine with the holes move toward the CBJ which is reverse-biased (the diode is OFF), producing \\(i_C\\) , such that \\(i_C \\gg i_B\\) . ::: center ::: ::: list The collector region is connected to the \\(+\\) of \\(V_{CB}\\) , so free electrons in the base region are attracted to the \\(+\\) side and are swept across into the collector region, despite the diode being reverse-biased. Refer to the zoomed-in diagram in the right. ::: In order to use the BJT as an amplifier, it should be operated in the active region. The conceptual circuit for NPN is shown below to illustrate the operation of the transistor as an amplifier. ::: multicols 2 ::: flushleft ::: The total base-emitter voltage becomes \\( \\(v_{BE} = V_{bias} + v_{in}\\) \\) where \\(V_{bias}\\) and \\(v_{in}\\) are the DC and AC base-emitter voltage respectively. Each serve a different purpose. ::: The DC voltage, \\(V_{bias} > \\SI{0.7}{\\volt}\\) , allows us to operate where \\(i_C-v_{BE}\\) relation is linear. The input signal to be amplified is represented by the AC voltage \\(v_{in}\\) that is superimposed on \\(V_{bias}\\) . ::: center ::: We need to perform two types of analysis: DC analysis, to determine the operating point and ... AC analysis, to determine voltage gain. In order to do these analysis, we need to model the BJT using two transistor models: large-signal equivalent circuit and small-signal equivalent circuit. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Large-Signal Equivalent Circuit (without Base-Width Modulation) We'll begin by only considering the DC component, so the small-signal source is eliminated for DC analysis, such that \\(v_{BE} = V_{bias}\\) . ::: center ::: An important thing to make note of, since \\(i_B \\ll i_C\\) , we have the approximation \\( \\(i_E \\approx i_C\\) \\) So you may often see \\(i_C\\) and \\(i_E\\) interchange. The collector current can be expressed as \\( \\(\\label{eq:collectorcurrent} i_C \\approx I_Se^{v_{BE}/V_t}\\) \\) If you notice, this is equivalently the equation for diode current. The base current can be expressed as a fraction of the collector current \\( \\(i_B = \\frac{i_C}{\\beta} = \\bigg[\\frac{I_S}{\\beta}\\bigg]e^{v_{BE}/V_t}\\) \\) where \\(\\beta\\) is a transistor parameter. Note for PNP, replace \\(v_{BE}\\) with \\(v_{EB}\\) . We can form a set of relations between \\(i_C\\) , \\(i_B\\) , and \\(i_E\\) : \\( \\(i_C = \\alpha i_E \\hspace{2cm} i_C = \\beta i_B\\) \\) where \\(\\alpha\\) is related to \\(\\beta\\) by \\( \\(\\alpha = \\frac{\\beta}{\\beta + 1}\\) \\) We can express the emitter current as \\( \\(\\label{eq:emitter_nobase} i_E = i_B + i_C = \\frac{i_C}{\\alpha} = \\bigg[\\frac{I_S}{\\alpha}\\bigg]e^{v_{BE}/V_t}\\) \\) Thus, the large-signal equivalent circuit of the BJT in active mode would look like ::: center ::: ::: list This circuit does not account for the base-width modulation, which occurs when the connector-emitter voltage is increased. As noted in , the collector current is only affected by \\(v_{BE}\\) , but not by \\(v_{CE}\\) . ::: -2ex -0.1ex -.2ex .2ex .2ex Large-Signal Equivalent Circuit (with Base-Width Modulation) Previously, we assumed that if we increased the connector-emitter voltage \\(v_{CE}\\) , that the collector current \\(i_C\\) will remain constant. However, this is not the case as you'll see from the \\(i_C\\) - \\(v_{CE}\\) characteristics of a BJT. ::: center ::: In reality, \\(i_C\\) does depends on \\(v_{CE}\\) . It varies linearly with \\(v_{CE}\\) . This is due to the Early effect, which we can derive the equation of the slope, by ::: center ::: So our collector current can be expressed as \\( \\(\\label{eq:collectorcurrentbw} i_C \\approx I_Se^{v_{BE}/V_t} + I_Se^{v_{BE}/V_t}\\bigg[\\frac{v_{CE}}{V_A}\\bigg] = I_Se^{v_{BE}/V_t}\\bigg[1 + \\frac{v_{CE}}{V_A}\\bigg]\\) \\) Alternatively, the preferred way of writing is expressed as \\( \\(i_C \\approx I_Se^{v_{BE}/V_t} + g_ov_{CE}\\) \\) where \\(g_o\\) is the output conductance \\( \\(g_o = \\frac{1}{r_o} = \\frac{I_Se^{v_{BE}/V_t}}{V_A} \\approx \\frac{I_C}{V_A}\\) \\) One very important thing to make note of, \\(r_o\\) is not a physical resistorits intended to quantify the impact of base-width modulation onto the circuit; more-so an artificial parameter. Thus, the large-signal equivalent circuit of the BJT in active mode would now look like ::: center ::: We'll discuss the small-equivalent circuit in a separate section, as they're a lot to talk about. -3ex -0.1ex -.4ex 0.5ex .2ex Saturation Mode In saturation mode, the EBJ is in forward bias ( \\(V_{BE} > \\SI{0.7}{\\volt}\\) for NPN and \\(V_{EB} > \\SI{0.7}{\\volt}\\) for PNP) and the CBJ is in forward bias ( \\(V_{BC} > \\SI{0.7}{\\volt}\\) for NPN and \\(V_{CB} > \\SI{0.7}{\\volt}\\) for PNP). The BJT behaves as a resistor in saturation region. Looking back at the \\(i_C-v_{CE}\\) characteristics of a BJT, the active and saturation region is separated by the blue dashed line. ::: center ::: This is the boundary voltage, \\(v_{CE,\\, sat}\\) , typically around \\(\\SI{0.1}{\\volt}\\) to \\(\\SI{0.2}{\\volt}\\) . A closer look at the BJT in saturation: ::: center ::: In deep saturation, a better \\(i_C-v_{CE}\\) linear relation exists, where \\(v_{CE}\\) is smaller. As mentioned earlier, the BJT behaves likes a resistor which has a resistance of: \\( \\(r_{CE,\\, sat} = \\frac{1}{\\tfrac{di_C}{dv_{CE}}}\\) \\) A few things to keep note of: \\(r_{CE,\\, sat}\\) is a physical resistor, unlike \\(r_o\\) , with a small value of a few ohms to a few tens of ohms. In comparison to the output resistance in active mode: \\(r_{CE,\\, sat} \\ll r_o\\) . Increasing \\(v_{BE}\\) , increases the slope of \\(i_C-v_{CE}\\) relation and \\(r_{CE,\\, sat}\\) drops. The resistance, \\(r_{CE,\\, sat}\\) , is due to the resistance of the emitter \\(R_E\\) and that of the collector \\(R_c\\) , which can be modelled as: ::: center ::: -4ex -1ex -.4ex 1ex .2ex Design of Amplifiers Previously, we discussed the large-signal equivalent circuit, so now we will go over the small-signal equivalent. When passing a small-signal, some components function differently, as shown below: Linear resistor: The small-signal equivalent circuit of a linear resistor is the resistor itself. ::: center ::: Linear capacitor: The small-signal equivalent circuit of a linear capacitor is the capacitor itself. ::: center ::: Independent voltage source: The small-signal equivalent circuit of an independent voltage source is a short-circuit. ::: center ::: Independent current source: The small-signal equivalent circuit of an independent current source is an open-circuit. ::: center ::: Controlled sources: The small-signal equivalent circuit of a controlled source is the same controlled source. ::: center ::: Referring back to \\(i_C-v_{BE}\\) relation, the large slope gives rise to a large transconductance. ::: multicols 2 ::: flushleft ::: The transconductance \\(g_m\\) at DC operating point \\( \\(g_m = \\bigg[\\frac{i_C}{dv_{BE}}\\bigg]_{DC} \\approx \\frac{d}{dv_{BE}}\\bigg[I_Se^{v_{BE}/V_t}\\bigg] = \\frac{I_C}{V_t}\\) \\) where \\(I_C = I_Se^{v_{bias}/V_t}\\) denotes the DC current. ::: As a follow up, BJT needs to have a large \\(g_m\\) , which is one reason we do not operate in saturation region due to the small \\(g_m\\) . ::: center ::: If we compare \\(\\Delta i_C\\) in active and saturation mode, you can see that \\(\\Delta i_{C,\\, active} \\gg i_{C,\\, sat}\\) . Referring back to the equation for transconductance \\(g_m\\) , we can approximate it to be equivalently the formula for the slope: \\( \\(g_m = \\frac{di_C}{dv_{BE}} \\approx \\frac{\\Delta i_C}{\\Delta v_{BE}}\\) \\) Since in both cases, \\(\\Delta v_{BE} = v_{BE2} - v_{BE1}\\) is the same, with the only difference being \\(\\Delta i_C\\) , then \\(g_{m,\\, active} \\gg g_{m,\\, sat}\\) This all relates back to how voltage is amplified or rather how a large voltage gain is obtained. ::: center ::: In the first curve, the \\(i_C-v_{BE}\\) relation, a small \\(v_{in}\\) is mapped to a large \\(i_C\\) via \\(g_m\\) : \\(i_C = g_mv_{in}\\) Notice how our amplified output is current, so we need to map it back to voltage, which is the second curve, the \\(i_C-v_{CE}\\) relation. The resultant \\(i_C\\) is then mapped to a large \\(v_{ce}\\) via \\(r_o\\) where \\(g_o = 1/r_o\\) : \\(v_{ce} = r_oi_C\\) As a result, voltage amplification is achieved. -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Equivalent Circuit Refer to the lecture slides for the in-depth derivation. The collector current can be expressed as \\( \\(i_C = g_ov_{ce} + g_mv_{be}\\) \\) where \\(i_C\\) , \\(v_{ce}\\) , and \\(v_{be}\\) are small-signal (AC) quantities denoted by the lower-case. Another version is using the relationship \\(\\beta i_b = i_C\\) , the equation becomes \\( \\(i_C \\approx \\beta i_b + g_ov_{ce}\\) \\) Thus, the small-signal equivalent model of the BJT in active mode using a voltage-controlled current source ::: center ::: and the other using a current-controlled current source ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Parameters The base-emitter resistance \\(r_{be}\\) , equivalently \\(1/g_{be}\\) , can be expressed as \\( \\(r_{be} = \\bigg[\\frac{dv_{BE}}{di_B}\\bigg]_{DC} = \\beta\\frac{V_t}{I_C} = \\beta\\frac{1}{g_m} = \\frac{V_t}{I_B}\\) \\) A few things to note, \\(r_{be}\\) consist of both emitter \\(r_e\\) and base resistance \\(r_b\\) \\( \\(r_{be} = r_b + r_e\\) \\) But, since the emitter is heavily doped, \\(r_b \\gg r_e\\) , which means the base-emitter is dominated by the base resistance, \\(r_{be} \\approx r_b\\) . The emitter resistance \\(r_e\\) can be expressed as \\( \\(r_e = \\bigg[\\frac{dv_{BE}}{di_E}\\bigg] = \\frac{1}{\\beta + 1}\\bigg[\\frac{dv_{BE}}{di_B}\\bigg]_{DC} = \\frac{r_{be}}{\\beta + 1}\\) \\) Each terminal have different characteristics shown below: ::: center ::: We are basically expressing the resistance looking into the circuit. It maybe clearer to see what the purpose of determining these equations are, by using the following diagram instead ::: center ::: As a recap, there are four small-signal parameters associated with BJT: ::: center ::: Transconductance \\(\\displaystyle g_m = \\bigg[\\frac{di_C}{dv_{BE}}\\bigg]_{DC} = \\frac{I_C}{V_t}\\) Output conductance \\(\\displaystyle g_o = \\bigg[\\frac{di_C}{dv_{CE}}\\bigg]_{DC} = \\frac{I_C}{V_A}\\) Base-emitter resistance \\(\\displaystyle r_{be} = \\bigg[\\frac{dv_{BE}}{di_B}\\bigg]_{DC} = \\frac{V_t}{I_B}\\) Transistor parameter \\(\\displaystyle \\beta = \\frac{I_C}{I_B}\\) ::: list The transistor parameter \\(\\beta\\) is typically given, which usually has a value of \\(100\\) . The other three parameters are what varies in ways due to the DC operating point. ::: BJT Voltage Amplifiers \u00b6 -4ex -1ex -.4ex 1ex .2ex Load Line and Maximum Signal Swing Recall that a transistor is optimal as an amplifier when operating in the active region. The load line is a line drawn on the characteristics curve. ::: center ::: The load line can be expressed as \\( \\(i_C = \\frac{V_{CC} - v_{CE}}{R_c}\\) \\) The intersection of the load line and \\(i_C-v_{CE}\\) curve with \\(v_{BE} = V_{bias}\\) gives us the DC operating point (or the bias point). Once the operating point is established you essentially know how to bias the transistor optimallyeither by varying \\(V_{bias}\\) or \\(R_c\\) . ::: list Note that it is normally done by varying \\(V_{bias}\\) in the circuit, as varying \\(R_c\\) affects gain, which will get to in a bit. ::: Two important considerations in deciding the location of the DC operating point are the gain and allowable signal. In deciding the value for \\(V_{bias}\\) , it is useful to refer to the \\(i_C-v_{CE}\\) curve: ::: center ::: If DC operating point is too close to the boundary of active and saturation regions, the max signal swing will be smaller (shown on the left). If DC operating point is too close to \\(V_{CC}\\) , the max signal swing will be smaller (shown on the right). From the DC operating point, we can determine the lower and upper bounds of \\(v_o\\) signal swing coming from \\(v_{in}\\) in the circuit: The lower bound can be expressed as \\(v_{o,\\,total,\\,min} = V_{sat}\\) The upper bound can be expressed as \\(v_{o,\\,total,\\,max} = V_{CC}\\) ::: center ::: From the two equations, we can determine the max voltage swing, which is \\( \\(\\label{eq:maxvoltswing} v_{o,\\,AC,\\,max} = \\frac{V_{CC} - V_{sat}}{2}\\) \\) There are three basic configurations for connecting BJT as an amplifier: Common-Emitter (CE) Amplifier Common-Base (CB) Amplifier Common-Collector (CC) Amplifier Each has distinctly different attributes and hence areas of application, which will cover in each section. -4ex -1ex -.4ex 1ex .2ex Common-Emitter (CE) Amplifier The common-emitter amplifier is the most widely use of the three. The common-emitter amplifier circuit is shown below: ::: center ::: The purpose of each component are explained below: The signal source represents the Thevenin equivalent of the signal generator with an internal resistance \\(R_s\\) . The two isolating capacitors \\(C_1\\) and \\(C_2\\) are used to separate the AC signals from the DC biasing voltageby passing AC signals and blocking any DC component. It uses two resistors \\(R_1\\) and \\(R_2\\) to generate DC biasing voltage \\(V_B\\) at the basereferred to as voltage divider biasing. Suppose \\(R_c\\) is known, we can express the optimal \\(V_B\\) that yields the max output voltage swing shown in which is \\( \\(V_B = V_t\\ln{\\bigg[\\frac{V_{CC}-V_{sat}}{2I_SR_c}\\bigg]}\\) \\) Since \\(V_B\\) is generated by a DC biasing voltage, we'll rewrite \\(V_B\\) in terms of \\(R_1\\) and \\(R_2\\) . In order to derive this equation, we need to consider two possible cases: Case 1: If \\(R_1\\) and \\(R_2\\) small, \\(I_B\\) can ignored since \\(I_{R1},I_{R2} \\gg I_B\\) . Therefore, we can apply voltage division to determine \\(V_B\\) , such that \\( \\(V_B = \\frac{R_2}{R_1 + R_2}V_{CC}\\) \\) One downside to this is that it will consume more (DC) power. Case 2: If \\(R_1\\) and \\(R_2\\) are large, \\(I_B\\) can no longer be ignored since \\(I_{R1},I_{R2}\\) is now comparable to \\(I_B\\) , where the current splits from \\(I_B\\) and \\(I_{R2}\\) . Performing KCL at node B yields \\( \\(V_B = \\frac{G_1V_{CC} - (I_C/\\beta)}{G_1 + G_2}\\) \\) Note that \\(G_1 = 1/R_1\\) and \\(G_2 = 1/R_2\\) and \\(I_C = \\beta I_B\\) . Since we always prefer to reduce power consumption, we make \\(R_1\\) and \\(R_2\\) large. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CE Amplifier By replacing the BJT with an equivalent circuit shown below, we can use this circuit to determine the characteristics parameters of the amplifier \\(R_{in}\\) , \\(R_{out}\\) , and \\(A_{v}\\) as follows. In small-signal analysis, the capacitors \\(C_1\\) , \\(C_2\\) are replaced by effective shorts and the DC source \\(V_{CC}\\) is replaced by a ground. ::: center ::: ::: list The circuit above is shown without a voltage-divider bias, \\(R_1\\) and \\(R_2\\) , instead uses \\(V_b\\) . ::: The input resistance \\(R_{in}\\) is the resistance \\\"seen\\\" by the AC source connected to the input. ::: multicols 2 ::: flushleft ::: The input resistance is expressed by the following formula: \\( \\(R_{in} = r_{be}\\) \\) and with DC biasing, the equation can be expressed as: \\( \\(R_{in} = R_1 \\parallel R_2 \\parallel r_{be}\\) \\) ::: The output resistance \\(R_{out}\\) is the resistance looking in at the collector. ::: multicols 2 ::: flushleft ::: The output resistance is expressed by the following formula: \\( \\(R_{out} = R_c \\parallel r_o\\) \\) where \\(r_o\\) is intended to quantify the impact of base-width modulation. ::: The voltage gain \\(A_v\\) is the ratio of the output voltage \\(v_o\\) at the collector to the input voltage \\(v_{in}\\) . \\( \\(A_v = \\frac{v_o}{v_{in}}\\) \\) Since \\(v_{in} = v_{be}\\) and by performing KCL at output node (refer to the diagram used for \\(R_{out}\\) ) \\( \\(\\frac{v_o}{R_c} + \\frac{v_o}{r_o} + g_mv_{be} = \\bigg(\\frac{1}{R_c}+\\frac{1}{r_o}\\bigg)v_o + g_mv_{in} = 0 \\quad \\Longleftrightarrow \\quad \\frac{v_o}{v_{in}} = -g_m\\bigg(\\frac{R_cr_o}{R_c + r_o}\\bigg)\\) \\) The voltage gain is expressed by the following formula: \\( \\(\\label{eq:ce_voltgain} A_v = -g_mR_{out} = -g_m(R_c \\parallel r_o)\\) \\) ::: list Notice how \\(A_v\\) is negative, which indicates a phase inversion from input to output. In other words, CE amplifier is an inverting amplifier. ::: One issue with the design is when we try to fabricate this on-chip, resistors take up a ton of space and becomes quite expensive, which brings up the next point. So an alternative is by replacing the resistor \\(R_c\\) with an active load as shown below. ::: center ::: The purpose of \\(Q_2\\) is to behave like a resistor with no amplification: It can be achieved by passing through a constant voltage \\(V_{b2}\\) at the base. Since \\(V_{b2}\\) is constant, the current of \\(Q_2\\) is constant and therefore, it can be represented by a constant source \\(I_{CC2}\\) with an output resistance \\(r_{o2}\\) . If we can use \\(r_{o2}\\) to fabricate \\(R_c\\) , then a large \\(A_v\\) is obtained without using a large resistor. ::: list Keep in mind, \\(Q_1\\) is an NPN transistor, so \\(Q_2\\) is a PNP transistor for \\(R_{out}\\) to be large. ::: The characteristics parameters are the same as before, with the only difference changing \\(R_c\\) to \\(r_{o2}\\) in the output resistance: Input resistance: \\(R_{in} = r_{be}\\) Output resistance: \\(R_{out} = r_{o1} \\parallel r_{o2}\\) Voltage gain: \\(A_v = -g_mR_{out} = -g_m(r_{o1}\\parallel r_{o2})\\) -2ex -0.1ex -.2ex .2ex .2ex Summary The CE amplifier gives high input resistance (draws little current), moderately high output resistance (easier to match for maximum power transfer), and high voltage gain, \\(A_{v}\\) (a desirable feature of an amplifier). It is an inverting amplifier, by increasing \\(g_m\\) or \\(R_{out}\\) , increases the voltage gain \\(A_v\\) , but it comes at a cost: Increasing \\(g_m\\) results in more power consumption. It is costly to fabricate a large resistor on chip. By replacing the resistor with an active load, one can increase the voltage gain without using a large resistor, with an extra cost of providing an additional biasing voltage. -3ex -0.1ex -.4ex 0.5ex .2ex Emitter Degeneration When we include a resistance \\(R_E\\) in the emitter as shown below, it can lead to significant changes in the amplifier characteristics. ::: center ::: The emitter degeneration forms a negative feedback mechanism that stabilizes \\(I_C\\) . In other words, it stabilizes the DC operating point, accounting for the fluctuations which can change the value of \\(V_B\\) . Without \\(R_E\\) , let's say \\(V_{b1}\\) increases, then \\(V_{BE1}\\) increases as well, since \\( \\(V_{b1} = V_{BE1}\\) \\) As a result, \\(I_C\\) increases, changing the DC operating point and affecting \\(v_o\\) . \\( \\(I_C = I_Se^{V_{BE1}/V_t}\\) \\) When \\(R_E\\) is present, \\(V_{BE1}\\) will be smaller subsequently, since \\( \\(V_{b1} = V_{BE1} + R_EI_E \\qquad \\Longleftrightarrow \\qquad V_{BE1} = V_{b1} - R_EI_E\\) \\) Then \\(I_C\\) decreases, making it less susceptible to fluctuations. ::: center ::: With the addition of \\(R_E\\) , the characteristics parameters changed. ::: center ::: The input resistance is: \\( \\(R_{in} \\approx (1 + g_{m1}R_E)r_{be1}\\) \\) and with DC biasing: \\( \\(\\label{eq:ce_rindc} R_{in} \\approx R_1 \\parallel R_2 \\parallel (1 + g_{m1}R_E)r_{be1}\\) \\) The output resistance is: \\( \\(R_{out} = (r_{o1} \\parallel r_{o2})(1 + g_{m1}R_E)\\) \\) The voltage gain is: \\( \\(\\label{eq:ce_evoltgain} A_v = \\frac{-g_{m1}(r_{o1} \\parallel r_{o2})}{1 + g_{m1}R_E}\\) \\) The emitter degeneration reduces the power consumption, but comes at a cost, which lowers the voltage gain by \\(1 + g_{m1}R_E\\) in the denominator. A way to preserve the voltage gain is by adding a shunt capacitor \\(C_E\\) in parallel to \\(R_E\\) . ::: center ::: If you recall from 202, the impedance of a capacitor is \\( \\(z_{C_E} = \\frac{1}{j\\omega C_E}\\) \\) where \\(\\omega\\) is the input frequency: In DC, emitter degeneration is active because \\(C_E\\) behaves as an open-circuit ( \\(z_{C_E} = \\infty\\) ), since \\(\\omega = 0\\) . In AC, emitter degeneration is inactive at \\(\\omega\\) because \\(C_E\\) behaves as a short-circuit ( \\(z_{C_E} = 0\\) ), since \\(C_E\\) designed as such to be sufficiently large. As a result, the voltage gain is no longer affected by emitter degeneration, such that \\( \\(A_v \\approx g_m(R_c \\parallel r_o)\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Summary With emitter degeneration, it posses four unique characteristics: Increased input resistance by a factor of \\(1 + g_{m1}R_E\\) Reduced voltage gain by a factor of \\(1 + g_{m1}R_E\\) Increased output resistance Reduces power consumption To preserve voltage gain at the frequency of the input, shunt capacitor \\(C_E\\) that behaves as a short circuit at input frequency is added. ::: center ::: -4ex -1ex -.4ex 1ex .2ex Common-Base (CB) Amplifier The common-base amplifier circuit is shown below: ::: center ::: The input is now located at the emitter, while the output is still at the collector. The components still perform the same purpose as described in CE amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CB Amplifier In small-signal analysis, the following circuit is obtained. ::: center ::: The input resistance is: \\( \\(R_{in} \\approx \\frac{r_{be}}{\\beta + 1} = r_e\\) \\) and with DC biasing: \\( \\(R_{in} \\approx R_1 \\parallel R_2 \\parallel r_e\\) \\) The output resistance is the same as that of CE amplifier: \\( \\(R_{out} = R_c \\parallel r_o\\) \\) The voltage gain is: \\( \\(A_v \\approx g_{m1}(R_c \\parallel r_{o})\\) \\) ::: list Since \\(A_v\\) is positive, CB amplifier is a non-inverting amplifier, unlike CE amplifier which is an inverting amplifier. ::: One thing to add, we can also replace \\(R_c\\) with an active load, like we did with CE amplifier. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Summary The CB amplifier has a low input resistance. This is undesirable as it will draw a large current when driven by a voltage input. Refer to the next section. It is a non-inverting amplifier, with the same voltage gain as that of CE amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Characterizing Amplifiers In general, CB amplifiers should not be used as a voltage amplifiers due to its low input resistance, \\(R_{in} = r_e\\) . To understand why, refer to the characterization of an amplifier below. ::: center ::: For amplifiers, a large input resistance is desirable as it reduces the loading effect of the amplifier on the signal source. ::: center ::: The larger the input resistance \\(R_{in}\\) , the more the input signal \\(v_s\\) will appear at the input of the amplifier. \\( \\(\\label{eq:input_volt} v_{in} = \\frac{R_{in}}{R_s + R_{in}}v_s \\approx \\bigg(1 - \\frac{R_s}{R_{in}}\\bigg)v_s\\) \\) If you recall, CE amplifiers have a large input resistance, thus making it a desirable voltage amplifier. \\( \\(\\frac{R_s}{R_{in}} \\approx 0 \\qquad \\Longrightarrow \\qquad v_{in} \\approx v_s\\) \\) However, CB amplifiers have a small input resistance, since \\(r_e = r_{be}/(\\beta + 1)\\) , where \\(\\beta\\) is around \\(100\\) , resulting to a huge voltage loss from \\(v_s\\) to \\(v_{in}\\) . Suppose we use a Norton equivalent source, instead of a Thevenin equivalent source. ::: center ::: Now in this scenario, the signal of the source is current. \\(R_N\\) is designed to be very large, so all the current will be delivered to the load, oppose to \\(R_s\\) in Thevenin equivalent. \\( \\(i_{in} = \\frac{R_N}{R_{in} + R_N}i_N \\approx \\bigg(1 - \\frac{R_{in}}{R_N}\\bigg)i_N\\) \\) So if \\(R_N\\) is large enough or \\(R_{in}\\) is very small, then all the current will be delivered to the load. \\( \\(\\frac{R_{in}}{R_N} \\approx 0 \\qquad \\Longrightarrow \\qquad i_{in} \\approx i_N\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Additional Remarks When designing the circuit, we might not have a current generator, so how can we apply them? ::: center ::: By placing a common-emitter, the transistor \\(Q_1\\) maps \\(v_{in}\\) to \\(I_{C1}\\) and we know it has an output resistance \\(r_{o1}\\) due to base width modulationthe equivalent circuit is shown on the right. ::: list This isn't really covered in the scope of the course, just intended to demonstrate how it would look like. It is referred to as a cascode amplifier. ::: -4ex -1ex -.4ex 1ex .2ex Common-Collector (CC) Amplifier The common-collector amplifier is usually referred to as an emitter follower. The common-collector amplifier circuit is shown below: ::: center ::: Since the output voltage is at the emitter, it is in phase with the base voltage, so there is no inversion from input to output. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CC Amplifier In small-signal analysis, the following circuit is obtained. ::: center ::: The input resistance is the same as that of CE amplifier: \\( \\(R_{in} = r_{be}\\) \\) The output resistance is: \\( \\(R_{out} \\approx \\frac{1}{g_{m1}} = r_e\\) \\) The voltage gain is: \\( \\(A_v \\approx 1\\) \\) ::: list Because there is no inversion and \\(A_v \\approx 1\\) , the output voltage closely follows the input voltage in both phase and amplitudethus the term emitter-follower. ::: Suppose we replace \\(R_E\\) with an active load, as we have done with the previous two amplifiers. ::: center ::: An NPN-based source follower functions as a voltage-down shifter as \\(v_o = v_{in} - v_{BE1}\\) . It shifts the level down by \\(v_{BE1}\\) . An PNP-based source follower functions as a voltage-up shifter as \\(v_o = v_{in} + v_{EB1}\\) . It shifts the level up by \\(v_{EB1}\\) . ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Characterizing Amplifiers Due to its high input resistance and a low output resistance, it can be used a voltage buffer. To understand the use of this, let's look back at the diagram we used for CB amplifiers. ::: center ::: Previously, we covered how a high input resistance is more favorable. Now, we'll focus on the output resistance. ::: center ::: For amplifiers, a small output resistance is desirable, as it increases the output voltage \\(v_{out}\\) that will be delivered to the load resistor \\(R_L\\) . \\( \\(v_L = \\frac{R_L}{R_L + R_{out}}v_{out} \\approx \\bigg(1 - \\frac{R_{out}}{R_L}\\bigg)v_{out}\\) \\) Since CC amplifiers have a small output resistance, such that \\(R_{out} \\ll R_L\\) , then \\( \\(\\frac{R_{out}}{R_L} \\approx 0 \\qquad \\Longrightarrow \\qquad v_L = v_{out}\\) \\) Thus, comes down to the reason we use an emitter follower. As an example, we have a CE amplifier with a load resistor attached at the output. ::: center ::: When a load resistor is present, the voltage gain in is now: \\( \\(A_{v,\\ \\text{w/ load}} = -g_{m1}(R_{out} \\parallel R_L) = -g_{m1}(r_{o1} \\parallel r_{o2} \\parallel R_L)\\) \\) Depending on \\(R_L\\) : Case 1: If \\(R_L \\gg R_{out}\\) , then \\(A_{v,\\ \\text{w/ load}} = A_{v,\\ \\text{w/o load}}\\) , where \\(R_L\\) will have no impact on the voltage gain. Case 2: If \\(R_L \\ll R_{out}\\) , then \\(A_{v,\\ \\text{w/ load}} \\ll A_{v,\\ \\text{w/o load}}\\) , where \\(R_L\\) will have a severe loading impact on voltage gain. A solution to Case 2, if we add an emitter follower stage to lower the output resistance, then \\(R_L\\) will have no impact on the voltage gain. ::: center ::: If you recall an emitter follower has a voltage gain of \\(A_v \\approx 1\\) , which is good, since it does not alter or change the voltage gain (More on this in the next section). By applying an emitter follower, we can make \\(R_{out}\\) even smaller, such that \\(R_L \\gg R_{out}\\) . -4ex -1ex -.4ex 1ex .2ex Multi-stage Amplifiers [Two or more (CE or CB) amplifiers can be connected in a cascaded arrangement with the output of one amplifier driving the input of the next.]{#bjt_multistage} ::: center ::: Isolation capacitors exists at the input, output, and in between stages to: Isolate the amplifier from both input source and output load DC bias each stage individually The overall voltage gain of cascaded amplifiers is the product of the individual voltage gains. \\( \\(A_v = A_{v1}A_{v2}A_{v3} \\cdots A_{vn}\\) \\) where \\(n\\) is the number of stages. ::: center ::: Each triangular symbol represents a separate amplifier. As an example, we have a two-stage BJT amplifier below: ::: center ::: The input voltage \\(v_1\\) can be obtained using voltage division (refer to and ). \\( \\(v_1 = \\frac{R_{in1}}{R_{in1}+R_s}v_s\\) \\) where \\(R_{in1} = R_1 \\parallel R_2 \\parallel (1 + g_{m1}R_{E1})r_{be1}\\) The voltage gain of stage 1, \\(A_{v1}\\) , is equivalently the voltage gain of a CE amplifier with emitter degeneration (refer to ). The load resistance is \\(R_{c1} \\parallel R_{in2}\\) . \\( \\(A_{v1} = \\frac{-g_{m1}(R_{c1} \\parallel R_{in2})}{1 + g_{m1}R_{E1}}\\) \\) where \\(R_{in2} = R_3 \\parallel R_4 \\parallel (1 + g_{m2}R_{E2})r_{be2}\\) Likewise, the same goes for voltage gain of stage 2, \\(A_{v2}\\) . The load resistance is \\(R_{c2}\\) . \\( \\(A_{v2} = \\frac{-g_{m2}R_{c2}}{1 + g_{m2}R_{E2}}\\) \\) Combining all three, we can determine the overall voltage gain: \\( \\(A_v = \\bigg[\\frac{R_{in1}}{R_{in1}+R_s}v_s\\bigg] \\times \\bigg[\\frac{-g_{m1}(R_{c1} \\parallel R_{in2})}{1 + g_{m1}R_{E1}}\\bigg] \\times \\bigg[\\frac{-g_{m2}R_{c2}}{1 + g_{m2}R_{E2}}\\bigg]\\) \\) ::: list For load resistance, we didn't include \\(r_{o1}\\) or \\(r_{o2}\\) because a large resistor ( \\(r_{o1},r_{o2}\\) is \\(\\SI{10}{\\kilo\\ohm}\\) to \\(\\SI{100}{\\kilo\\ohm}\\) ) in parallel with a small resistor has the resistance of the smaller one roughly./ ::: -4ex -1ex -.4ex 1ex .2ex Current Mirrors A current mirror is a circuit designed to copy a current through one active device by controlling the current in another active device of a circuit, keeping the output current constant regardless of loading. -3ex -0.1ex -.4ex 0.5ex .2ex Diode-Connected BJT It uses a diode-connected BJT, where the base and collector are tied together. ::: center ::: The reason we call it a diode-connected BJT is because if you remember we have a diode \\(D_1\\) between base and emitter then a diode \\(D_2\\) between base and collector. ::: center ::: When a wire is connected between the base and collector, the diode \\(D_2\\) is short-circuited. As a result, the entire circuit functions like a diode \\(D_1\\) . ::: center ::: Looking into the circuit, the input resistance is simply the resistance of the diode \\(D_1\\) , which is: \\( \\(R_{in} \\approx \\frac{1}{g_m} \\approx r_{e}\\) \\) And so a diode-connected BJT synthesizes a resistor of low resistance \\(r_e\\) . Equivalently, we constructed a resistor without physically adding one to the circuit. -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror without Base-Width Modulation Let's now look at a BJT current mirror. Refer to the circuit below. ::: center ::: For the sake of simplicity, let's first ignore base-width modulation. In other words, we ignore \\(r_o\\) . If you recall from , the collector current \\(I_{C1}\\) and \\(I_{C2}\\) can be expressed as \\( \\(i_{C1} \\approx I_{s1}e^{v_{BE}/V_t}\\) \\) \\( \\(i_{C2} \\approx I_{s2}e^{v_{BE}/V_t}\\) \\) and by dividing both equations we have \\( \\(\\frac{i_{C2}}{i_{C1}} = \\frac{I_{s2}}{I_{s1}}\\) \\) When \\(Q_1\\) and \\(Q_2\\) are identical, we have that \\(I_{s2} = I_{s1}\\) , thus \\( \\(i_{C2} = i_{C1}\\) \\) which defines it to be a current mirror. If we further make further simplifications, which we also ignore \\(i_B\\) , since the base current is very small, such that \\(i_{B1}, i_{B2} \\ll i_{C_1}\\) , the current source \\( \\(J = i_{C1} + i_{B1} + i_{B2}\\) \\) can be expressed as \\(J \\approx i_{C1}\\) or \\( \\(i_{C2} \\approx \\frac{I_{s2}}{I_{s1}}J\\) \\) Though for a more accurate analysis where base currents \\(i_{B1}\\) and \\(i_{B2}\\) are accounted for while \\(r_o\\) is not accounted for, we have \\( \\(i_{C2} = \\frac{1}{1 + (2/\\beta)}J \\approx \\bigg(1 - \\frac{2}{\\beta}\\bigg)J\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror with Base-Width Modulation Let's now consider the impact of base-width modulation \\(r_o\\) . When accounted for, the circuit can be depicted as ::: center ::: From , the collector current \\(I_{C1}\\) and \\(I_{C2}\\) can be expressed as \\( \\(i_{C1} \\approx I_{s1}e^{v_{BE}/V_t}\\bigg(1 + \\frac{v_{CE1}}{V_A}\\bigg)\\) \\) \\( \\(i_{C2} \\approx I_{s2}e^{v_{BE}/V_t}\\bigg(1 + \\frac{v_{CE2}}{V_A}\\bigg)\\) \\) and by dividing both equations we have \\( \\(\\frac{i_{C2}}{i_{C1}} = \\frac{I_{s2}}{I_{s1}}\\frac{1 + (v_{CE2}/V_A)}{1 + (v_{CE1}/V_A)}\\) \\) However, \\(v_{CE1}\\) is not the same as \\(v_{CE2}\\) or in other words, \\(v_{CE1} \\neq v_{CE2}\\) because they are located in two different branches, so there's no way we can set them to the same voltage. If we let \\(v_{CE1} = v_{CE}\\) and \\(v_{CE2} = v_{CE} + \\Delta v_{CE}\\) , then we can express the difference in voltage between \\(v_{CE1}\\) and \\(v_{CE2}\\) by \\(\\Delta v_{CE}\\) , thus \\( \\(\\frac{i_{C2}}{i_{C1}} = \\frac{I_{s2}}{I_{s1}}\\underbrace{\\bigg(1 + \\frac{\\Delta v_{CE}/V_A}{1 + (v_{CE}/V_A)}\\bigg)}_{\\neq 1}\\) \\) Since \\(v_{CE1} \\neq v_{CE2}\\) in general, the impact of \\(v_{CE}\\) mismatch cannot be neglected for precision current mirrors. -3ex -0.1ex -.4ex 0.5ex .2ex Current Sinks and Current Sources The purpose of current mirrors is to construct current sources and current sinks from a master current source whose value is stableindependent of temperature and supply voltage fluctuations. ::: center ::: The current sources and current sinks allows us to very easily generate DC biasing current across a circuit. The currents are set by the ratio of transistor size, specifically the area of the emitter. For example, we have a two-stage amplifier consists consists of two CE amplifiers formed by \\(Q_1\\) and \\(Q_2\\) . The circuit is denoted in blue. ::: center ::: Ideally we're suppose to use a master current source, as the one we have does not provide a constant current source. ::: multicols 2 ::: flushleft ::: In reality it's not even actual current source, but two resistors using basic voltage division, such that \\( \\(V_A = \\frac{r_{e4}}{r_{e4} + r_{e3}}V_{CC}\\) \\) If \\(Q_3\\) and \\(Q_4\\) have the same emitter dimensions, then \\(r_{e4} = r_{e3}\\) \\( \\(V_A = \\frac{V_{CC}}{2}\\) \\) ::: Since \\(v_{BE4} = V_A\\) , we can express the collector current \\(i_{C4}\\) as \\( \\(i_{C4} = I_{s4}e^{V_{A}/V_t}\\) \\) Once \\(V_{A}\\) is known, we can determine collector currents of the current sources and sinks. For example, \\(i_{C5}\\) is \\( \\(i_{C5} = \\frac{I_{s5}}{I_{s4}}i_{C4}\\) \\) If \\(Q_4\\) and \\(Q_5\\) have the same emitter dimension, then \\(I_{s5} = I_{s4}\\) and \\(i_{C5} = i_{C4}\\) . We can perform the same analysis for \\(i_{C6}\\) to \\(i_{C8}\\) . MOSFETs \u00b6 Most of the stuff covered should be a recap from PC224, though the format of some formulas might looks different, but nonetheless are the same. -4ex -1ex -.4ex 1ex .2ex Structure of MOSFET Similar to BJTs, there are two types of Metal-Oxide Field-Effect transistors (MOSFETs) namely: NMOS transistor ::: center ::: PMOS transistor ::: center ::: It's a four-terminal symmetrical device: the source terminal (S), the drain terminal (D), the gate terminal (G), and the substrate or bulk terminal (B). Source: Heavily doped n-typed semiconductor for NMOS and heavily doped p-typed silicon for PMOS Drain: Heavily doped n-typed semiconductor for NMOS and heavily doped p-typed silicon for PMOS Gate: Polysilicon compound (highly conductive and yet more stable than metal) Substrate: Lightly doped p-type for NMOS and lightly doped p-type silicon for PMOS Note how the following references are denoted by their subscripts: \\(v_{GS}\\) is the gate to source voltage, otherwise known as \\(v_{GS} = v_G - v_S\\) . \\(v_{SG}\\) is the source to gate voltage, otherwise known as \\(v_{SG} = v_S - v_G\\) . \\(i_{DS}\\) is the current flowing from drain to source. \\(V_T\\) is the threshold voltagedefines the boundary of MOSFET being ON or OFF: Often the source terminal is connected to the ground, so they remove it from the subscript, such as \\(i_D\\) , but should still be the same thing as \\(i_{DS}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Dimensions of MOSFET Refer to the diagram below: ::: center ::: The dimensions of MOSFET are defined as such: \\(L_d\\) is the drawn length \\(L\\) or \\(L_{\\text{eff}}\\) is the effective length \\(W\\) is the width \\(t_{ox}\\) is the gate oxide thickness The dimensions of source and drain are dictated by the number of contacts A capacitor formed by the gate (top plate) and negative ions (bottom plate) is created \\( \\(C_g = C_{ox}(WL)\\) \\) and \\(C_{ox}\\) is the gate capacitance per area and is given by \\( \\(C_{ox} = \\frac{\\epsilon_{ox}}{t_{ox}}\\) \\) where \\(\\epsilon_{ox} = \\SI{3.45e-11}{\\farad\\per\\meter}\\) is the permittivity of oxide. -4ex -1ex -.4ex 1ex .2ex Operation of MOSFET There are three mode of MOSFET operation: cut-off mode, triode mode, and saturation modeeach mode have different applications. To get a better understanding of the different modes, we will be referencing the \\(i_{D}-v_{DS}\\) characteristics of the MOSFET, which looks similar to that of the \\(i_C-v_{CE}\\) characteristics of a BJT. ::: center ::: Note that the graph shown is for NMOS. For PMOS, you would have to reverse all the formulas, so instead of \\(v_{DS}\\) it would be \\(v_{SD}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Cut-Off Mode When \\(v_{GS} < V_T\\) , the MOSFET is OFF and no channel is induced. ::: center ::: The MOSFET will be in cut-off mode, where no current flows, such that \\( \\(i_{DS} = 0\\) \\) In cut-off, the MOSFET functions as an open switch with no current flow. -3ex -0.1ex -.4ex 0.5ex .2ex Triode Mode When \\(v_{GS} > V_T\\) , the MOSFET is ON and a channel will be induced where current starts flowing if \\(v_{DS} > 0\\) . ::: center ::: The triode mode and saturation mode is bounded by the pinch-off condition, \\(v_{GS} - V_T\\) . The MOSFET will operate in the triode mode as long as \\( \\(v_{DS} < v_{GS} - V_T\\) \\) For NMOS, the channel current is given by \\( \\(i_{DS,\\,\\text{triode}} = \\mu_nC_{ox}\\frac{W}{L}\\bigg[(v_{GS} - V_T)v_{DS} - \\frac{1}{2}v_{DS}^2 \\bigg]\\) \\) For PMOS, the channel current is given by \\( \\(i_{SD,\\,\\text{triode}} = \\mu_pC_{ox}\\frac{W}{L}\\bigg[(v_{SG} - |V_{Tp}|)v_{SD} - \\frac{1}{2}v_{SD}^2 \\bigg]\\) \\) If we look back at the \\(i_{D}-v_{DS}\\) characteristics of the MOSFET in the triode mode, you'll notice it behaves like a linear resistor when a small \\(v_{DS}\\) is applied ::: center ::: such that we can represent the channel conductance as \\( \\(g_{DS} = \\bigg[\\frac{di_{DS}}{dv_{DS}}\\bigg]_{DC} \\approx \\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)\\) \\) In triode mode, the MOSFET functions as a voltage-controlled resistor with conductance \\(g_{DS}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Saturation Mode When \\(v_{GS} > V_T\\) and \\(v_{DS} = v_{GS} - V_T\\) , the MOSFET is ON and a channel will pinch-off at the drain. ::: center ::: For NMOS, the channel current at pinch-off is given by \\( \\(i_{DS,\\,\\text{pinch-off}} = \\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)\\) \\) For PMOS, the channel current at pinch-off is given by \\( \\(i_{SD,\\,\\text{pinch-off}} = \\mu_pC_{ox}\\frac{W}{L}(v_{SG} - |V_{Tp}|)\\) \\) At pinch-off, the channel current \\(i_{DS}\\) becomes independent of \\(v_{DS}\\) , where \\( \\(g_o = \\frac{di_{DS}}{dv_{DS}} = 0\\) \\) It behaves as a voltage-controlled current source with transconductance ::: center ::: which can be defined as \\( \\(g_m = \\bigg[\\frac{di_{DS}}{dv_{GS}}\\bigg]_{DC} = \\frac{2i_{DS}}{v_{GS} - V_T}\\) \\) so to increase pinch-off, one can: increase the width \\(W\\) , size of the transistor, or Increase \\(v_{GS} - V_T\\) , otherwise referred to as over-drive voltage. When \\(v_{DS}\\) exceeds pinch-off, such that \\(v_{DS} > v_{GS} - V_T\\) , the MOSFET will operate in the saturation mode. ::: center ::: From the previous \\(i_{D}-v_{DS}\\) characteristics of the MOSFET, we assumed that as we increase \\(v_{DS}\\) , the channel current \\(i_{DS}\\) will remain constant. ::: center ::: If you recall for BJTs, we had something called the \\\"Early effect\\\", likewise the same thing can occur in MOSFET. ::: center ::: The output conductance \\(g_o\\) can redefined to be \\( \\(g_o = i_{DS,\\,\\text{pinch-off}}\\lambda\\) \\) For MOSFETs, we use \\(\\lambda = 1/V_A\\) , as it is most widely used, where \\(V_A\\) is Early voltage. For NMOS, the channel current at saturation is given by \\(i_{DS,\\,\\text{pinch-off}} + g_ov_{DS}\\) or \\( \\(i_{DS,\\,\\text{sat}} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)^2(1 + \\lambda v_{DS})\\) \\) For PMOS, the channel current at saturation is given by \\( \\(i_{SD,\\,\\text{sat}} = \\frac{1}{2}\\mu_pC_{ox}\\frac{W}{L}(v_{SG} - |V_{Tp}|)^2(1 + \\lambda v_{SD})\\) \\) In saturation mode, the MOSFET functions as a voltage-controlled current source with finite output resistance. -2ex -0.1ex -.2ex .2ex .2ex Summary For NMOS: \\( \\(i_{DS} = 0 \\begin{cases}v_{GS} < V_T\\end{cases}\\) \\) \\( \\(i_{DS,\\,\\text{triode}} = \\mu_nC_{ox}\\frac{W}{L}\\bigg[(v_{GS} - V_T)v_{DS} - \\frac{1}{2}v_{DS}^2 \\bigg] \\begin{cases}v_{GS} > V_T \\\\ v_{DS} < v_{GS} - V_T\\end{cases}\\) \\) \\( \\(\\label{eq:nmos_currentsat} i_{DS,\\,\\text{sat}} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)^2(1 + \\lambda v_{DS}) \\begin{cases}v_{GS} > V_T \\\\ v_{DS} > v_{GS} - V_T\\end{cases}\\) \\) For PMOS: \\( \\(i_{SD} = 0 \\begin{cases}v_{SG} < V_T\\end{cases}\\) \\) \\( \\(i_{SD,\\,\\text{triode}} = \\mu_nC_{ox}\\frac{W}{L}\\bigg[(v_{SG} - |V_{Tp}|)v_{SD} - \\frac{1}{2}v_{SD}^2 \\bigg] \\begin{cases}v_{SG} > V_T \\\\ v_{SD} < v_{SG} - |V_{Tp}|\\end{cases}\\) \\) \\( \\(i_{SD,\\,\\text{sat}} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(v_{SG} - |V_{Tp}|)^2(1 + \\lambda v_{DS}) \\begin{cases}v_{SG} > V_T \\\\ v_{SD} > v_{SG} - |V_{Tp}|\\end{cases}\\) \\) -4ex -1ex -.4ex 1ex .2ex Design of Amplifiers The same idea kinda be applied for MOSFETs when doing small-signal analysis, as we have previously covered with BJTs. ::: center ::: The total gate to source voltage becomes \\( \\(v_{GS} = V_{bias} + v_{in}\\) \\) where \\(V_{bias}\\) ensures that the MOSFET operates in saturation, which gives rise to a large \\(g_m\\) . ::: center ::: In the first curve, the \\(i_{DS}-v_{GS}\\) relation, a small \\(v_{in}\\) is mapped to a large \\(i_{DS}\\) via \\(g_m\\) . Notice how our amplified output is current, so we need to map it back to voltage, which is the second curve, the \\(i_{DS}-v_{DS}\\) relation. The resultant \\(i_{DS}\\) is then mapped to a large \\(v_{DS}\\) via \\(r_o\\) where \\(g_o = 1/r_o\\) . As a result, voltage amplification is achieved. -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Equivalent Circuit The small-signal equivalent circuit is shown below. ::: center ::: The drain current can be expressed as combination of the DC and AC signal of the circuit \\( \\(i_{DS} = I_{DS} + i_{ds}\\) \\) For NMOS, the small-signal drain current can be expressed as \\( \\(i_{ds} \\approx g_mv_{gs} + g_ov_{ds}\\) \\) where \\( \\(\\label{eq:nmos_transconductance} g_m = \\frac{2I_{DS}}{V_{GS} - V_T} \\qquad g_o = \\lambda I_{DS}\\) \\) For PMOS, the small-signal drain current can be expressed as \\( \\(i_{sd} \\approx g_mv_{sg} + g_ov_{sd}\\) \\) where \\( \\(\\label{eq:pmos_transconductance} g_m = \\frac{2I_{SD}}{V_{SG} - |V_{Tp}|} \\qquad g_o = \\lambda I_{SD}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Voltage Gain With the output resistor \\(r_o\\) added, the voltage gain becomes \\( \\(A_v = \\frac{v_o}{v_{in}} = -g_m(R_D \\parallel r_o)\\) \\) and by increasing \\(I_{DS}\\) or \\(R_D\\) , boost the voltage gain. In the next module, we will go over more in-depth, covering the three basic configurations for connecting the MOSFET as an amplifier, similar to what we covered for BJTs. MOSFET Voltage Amplifiers \u00b6 -4ex -1ex -.4ex 1ex .2ex Load Line and Maximum Signal Swing Similarly, in MOSFET voltage amplifiers, we need to pick the proper DC operating point given the \\(V_{bias}\\) in order to achieve maximum voltage swing allowed. ::: center ::: The load line can be expressed as: \\( \\(\\label{eq:mosfet_loadline} i_{DS} = \\frac{V_{DD} - v_{DS}}{R_D}\\) \\) The intersection of the load line and \\(i_{DS}-v_{DS}\\) curve with \\(v_{GS} = V_{bias}\\) gives us the DC operating point (or the bias point). Once the operating point is established you essentially know how to bias the transistor optimallyeither by varying \\(V_{bias}\\) or \\(R_D\\) . ::: list Note that it is normally done by varying \\(V_{bias}\\) in the circuit, as varying \\(R_D\\) affects gain, which will get to in a bit. ::: From the DC operating point, we can determine the lower and upper bounds of \\(v_o\\) signal swing coming from \\(v_{in}\\) in the circuit. ::: center ::: For a common-source amplifier with an NMOS: The lower bound can be expressed as \\(v_{o,\\,total,\\,min} = V_{sat}\\) The upper bound can be expressed as \\(v_{o,\\,total,\\,max} = V_{DD}\\) For a common-source amplifier with a PMOS: The lower bound can be expressed as \\(v_{o,\\,total,\\,min} = 0\\) The upper bound can be expressed as \\(v_{o,\\,total,\\,max} = V_{DD} - V_{sat}\\) From the upper and lower bound equations, we can derive a few equations. The max voltage swing \\( \\(v_{o,\\,AC,\\,max} = \\frac{V_{DD} - V_{sat}}{2}\\) \\) The optimal DC voltage at output \\( \\(v_{o,\\,DC} = \\frac{V_{DD} + V_{sat}}{2} = V_{DD} - R_DI_{DS}\\) \\) The optimal drain resistance by solving for \\(R_D\\) \\( \\(\\frac{V_{DD} + V_{sat}}{2} = V_{DD} + - R_D\\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(V_{bias} - V_T)^2\\) \\) There are three basic configurations for connecting MOSFET as an amplifier: Common-Source (CS) Amplifier Common-Gate (CG) Amplifier Common-Drain (CD) Amplifier Each has distinctly different attributes and hence areas of application, which will cover in each section. -4ex -1ex -.4ex 1ex .2ex Common-Source (CS) Amplifier Of the three basic MOS amplifier configurations, the common source is the most widely used. The common-source amplifier circuit is shown below: ::: center ::: The purpose of each component are explained below: \\(R_s\\) is the internal resistance of the signal source \\(v_s\\) . \\(R_1\\) and \\(R_2\\) generate a DC biasing voltage \\(V_{DC}\\) at the gatereferred to as voltage divider biasing. \\( \\(V_{DC} = \\frac{R_2}{R_1 + R_2}V_{DD}\\) \\) \\(C_1\\) and \\(C_2\\) are isolating capacitors used to separate the AC signals from the DC biasing voltageby passing AC signals and blocking any DC component. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CS Amplifier The small-signal equivalent of the CS amplifier with resistor load is shown below: ::: center ::: Because there is no gate current flow, the input resistance \\(R_{in}\\) is: \\( \\(R_{in} = \\infty\\) \\) The output resistance is: \\( \\(R_{out} = R_D \\parallel r_o\\) \\) The voltage gain is: \\( \\(A_v = - g_mR_{out} = -g_m(R_D \\parallel r_o)\\) \\) The negative voltage gain indicates that the it is an inverting amplifierphase difference between \\(v_o\\) and \\(v_{in}\\) is \\(180^\\circ\\) . ::: center ::: To increase the voltage amplification, we can modify one of two components: \\(g_m\\) or \\(R_{out}\\) . Increasing \\(g_m\\) is not ideal, as a large \\(g_m\\) will increase either \\(W/L\\) or \\(V_{GS} - V_T\\) , which result in a large drain current \\(I_{DS}\\) where \\( \\(g_m\\uparrow = k_n\\frac{W}{L}\\uparrow(V_{GS} - V_T)\\uparrow \\quad\\Longrightarrow\\quad I_{DS} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}\\uparrow(v_{GS} - V_T)^2\\uparrow\\) \\) thus, equivalently increase the power consumption \\( \\(P\\uparrow = V_{DD}I_{DS}\\uparrow\\) \\) From \\(R_{out}\\) , we can increase \\(R_D\\) , which is more ideal as it doesn't affect power consumption. However, increasing \\(R_D\\) affects the load line shown in . ::: center ::: The output can result in distortion if \\(R_D\\) is increases, as for an NMOS for example ::: center ::: thus, increasing \\(R_D\\) needs to be chosen carefully to maximize output voltage swing. In order to increase \\(R_D\\) in an effective manner, we can use a current-resistor load instead. The small-signal equivalent of the CS amplifier with current-source load is shown below: ::: center ::: The purpose of the transistor \\(M_2\\) is to behave like a resistor with no amplification: It can be achieve by passing through a constant voltage \\(V_{b2}\\) at the gate. Since \\(V_{b2}\\) is constant, the current of \\(M_2\\) is constant and therefore, it can be represented by a constant source \\(I_{SD2}\\) with an output resistance \\(r_{o2}\\) . If we can use \\(r_{o2}\\) to fabricate \\(R_D\\) , then a large \\(A_v\\) is obtained without using a large resistor. ::: list Keep in mind, \\(M_1\\) is an NMOS, so \\(M_2\\) must be a PMOS, otherwise it will result no voltage gain of \\(A_v \\approx -1\\) . Refer below for an in-depth explanation. ::: ::: {#corollary:mos_res} ::: cBox ::: corollaryT Corollary 5.1 . The purpose of using a PMOS is regarding the resistance looking into MOSFET. Refer to the diagram below: ::: center ::: If we use an NMOS for \\(M_2\\) , the output resistance would be: ( \\(R_{out} = r_{o1} \\parallel \\frac{1}{g_{m2}} \\approx \\frac{1}{g_{m2}}\\) \\) Since \\(r_{o1} \\gg (1/g_{m2})\\) , the voltage gain would be: ( \\(A_v = -g_{m1}\\frac{1}{g_{m2}} \\approx -1\\) \\) ::: ::: ::: Now, the output resistance can be expressed using \\(r_{o2}\\) instead of \\(R_D\\) , where: \\( \\(R_{out} = r_{o1} \\parallel r_{o2}\\) \\) Likewise, the voltage gain is: \\( \\(\\label{eq:cs_voltgain} A_v = -g_{m1}R_{out} = -g_{m1}(r_{o1} \\parallel r_{o2})\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Summary The CS amplifiers has infinite input impedance (draws no current at DC), and a moderately high output resistance (easier to match), and a high voltage gain (a desirable feature of an amplifier). It is an inverting amplifier. The voltage gain can increases, by increasing \\(g_m\\) or \\(R_D\\) : Increasing \\(g_m\\) will increases the power consumption. Increasing \\(R_D\\) is ideal, but needs to be chosen carefully to maximize output voltage swing. A current-source load is used in place of the resistor load, where a large load resistance \\(r_{o2}\\) of the amplifier is obtained without using an expensive resistor \\(R_D\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Source Degeneration A small resistor is added at the source to form source degeneration. ::: center ::: The purpose for adding source degeneration is similar to an emitter degeneration of CE amplifiers. The source degeneration forms a negative feedback. When the input voltage \\(V_{GS1}\\) attempts to increase, the voltage drop across \\(R_s\\) increases reducing \\(V_{GS1}\\) . \\( \\(V_{b1} = V_{GS1} + R_sI_{DS1}\\) \\) When \\(V_{DC}\\) (or \\(V_{b1}\\) ) is fixed, adding \\(R_s\\) reduce \\(I_{DS1}\\) subsequently DC power consumption. This is at a cost of reduced voltage gain by a factor of \\(1 + g_{m1}R_s\\) . \\( \\(\\label{eq:cs_voltgaindegen} A_v \\approx \\frac{-g_{m1}(r_{o1} \\parallel r_{o2})}{1 + g_{m1}R_s}\\) \\) Alternatively, we can mitigate the reduce voltage gain by introducing a shunt capacitor \\(C_s\\) in parallel with \\(R_s\\) , shown below. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Summary The voltage gain is reduced by a factor of \\(1 + g_mR_s\\) as seen in . To preserve voltage gain at the frequency of the input, a shunt capacitor \\(C_s\\) is added in parallel with \\(R_s\\) . -4ex -1ex -.4ex 1ex .2ex Common-Gate (CG) Amplifier The common-gate amplifier circuit is shown below: ::: center ::: The properties of a CG amplifier: The input signal enters the amplifier from the source. The output of the amplifier is taken at the drain. The gate is routed to a DC voltage \\(V_{b1}\\) and hence acts as an AC ground in small-signal equivalent circuit, as shown in the next section. The transistor operates in saturation mode, \\(V_{GS} = V_b - v_s > V_T\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CG Amplifier The small-signal equivalent of the CG amplifier with resistor load is shown below: ::: center ::: The input resistance is: \\( \\(R_{in} \\approx \\frac{1}{g_m}\\) \\) The output resistance is: \\( \\(R_{out} = R_D \\parallel r_o\\) \\) The voltage gain is: \\( \\(A_v \\approx g_m(R_{out}) = g_m(R_D \\parallel r_o)\\) \\) Notice that the gain expression is the same as for the CS amplifier without the negative sign, which makes it a non-inverting amplifier. However, CS has a larger overall voltage gain, since CG suffers a signal loss due to its small input impedance: The larger the input resistance \\(R_{in}\\) , the more the input signal \\(v_s\\) will appear at the input of the amplifier. \\( \\(v_{in} = \\frac{R_{in}}{R_s + R_{in}}v_s \\approx \\bigg(1 - \\frac{R_s}{R_{in}}\\bigg)v_s\\) \\) If you recall, CS amplifiers have a large input resistance, \\(R_{in} \\gg R_s\\) , thus making it a desirable voltage amplifier. \\( \\(\\frac{R_s}{R_{in}} \\approx 0 \\qquad \\Longrightarrow \\qquad v_{in} \\approx v_s\\) \\) While CG amplifiers have a small input resistance, \\(1/g_m\\) , resulting to a huge voltage loss from \\(v_s\\) to \\(v_{in}\\) . Despite having the same voltage gain, the CS amplifier have a lower overall voltage gain. The small-signal equivalent of the CG amplifier with current-source load is shown below: ::: center ::: Refer to . The purpose of \\(M_2\\) is identical to the one described for CS amplifier, such that the output resistance can be expressed using \\(r_{o2}\\) instead of \\(R_D\\) , where: \\( \\(R_{out} = r_{o1} \\parallel r_{o2}\\) \\) Then, the voltage gain is: \\( \\(A_v \\approx g_m(R_{out}) = g_m(r_{o1} \\parallel r_{o2})\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Summary Unlike CS amplifiers, the CG amplifiers has a low input impedance, which is undesirable, as it will draw large current when driven by a voltage input. It is an non-inverting amplifier. The voltage gain is made similar in magnitude to that of the CS amplifier, but suffers signal loss, thus posses a lower overall voltage gain. -4ex -1ex -.4ex 1ex .2ex Common-Drain (CD) Amplifier The common-drain amplifier, more commonly known as the source follower, is similar to the emitter follower for the BJT. The common-drain amplifier circuit is shown below: ::: center ::: The properties of a CD amplifier: The input signal enters the amplifier from the gate. The output of the amplifier is taken at the source. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CD Amplifier The small-signal equivalent of the CD amplifier with resistor load is shown below: ::: center ::: Likewise with CS amplifiers, because there is no gate current flow, the input resistance \\(R_{in}\\) is: \\( \\(R_{in} = \\infty\\) \\) The output resistance is: \\( \\(R_{out} = \\frac{1}{g_m}\\) \\) The voltage gain is: \\( \\(A_v \\approx 1\\) \\) As the name suggests, the output follows the input, thus the voltage gain of \\(1\\) . The small-signal equivalent of the CG amplifier with current-source load is shown below: ::: center ::: A NMOS-based source follower functions as a voltage down-shifter as \\(v_o = v_{in} - v_{GS1}\\) . It shifts the voltage level down by \\(v_{GS1}\\) . A PMOS-based source follower functions as a voltage up-shifter as \\(v_o = v_{in} + v_{SG1}\\) . It shifts the voltage level down by \\(v_{SG1}\\) . Since \\(v_{GS1,\\,min} = V_{Tn}\\) and \\(v_{GS1,\\,min} = |V_{Tp}|\\) , the minimum voltage shift is the threshold voltage. -2ex -0.1ex -.2ex .2ex .2ex Summary The CD amplifier has infinite input impedance (draws no current at DC), a relatively low output resistance, and a voltage gain that is near unity, \\(A_v \\approx 1\\) . The source follower is used as the output (or last) stage in a multistage amplifier, refer to the next section. Its function is to equip the overall amplifier with a low output resistanceenabling it to supply relatively large load currents without loss of gain. -4ex -1ex -.4ex 1ex .2ex Multi-Stage Amplifiers Two or more (CS or CG) amplifiers can be connected in a cascaded arrangement with the output of one amplifier driving the input of the next. ::: center ::: Isolation capacitors exists at the input, output, and in between stages to: Isolate the amplifier from both input source and output load DC bias each stage individually The overall voltage gain of cascaded amplifiers is the product of the individual voltage gains. \\( \\(A_v = A_{v1}A_{v2}A_{v3} \\cdots A_{vn}\\) \\) where \\(n\\) is the number of stages, as previously described in . The process of solving them is still the same as shown before, but the equations you use differ as you are now using MOSFET instead of BJT. -4ex -1ex -.4ex 1ex .2ex Current Mirrors As a recap, a current mirror is a circuit designed to copy a current through one active device by controlling the current in another active device of a circuit. -3ex -0.1ex -.4ex 0.5ex .2ex Diode-Connected MOSFET In MOSFET, we have something very similar to the diode-connected BJT, where the drain and gate are tied together. ::: center ::: When a wire is connected between the drain and gate, the diode \\(D_2\\) is short-circuited. As a result, the entire circuit functions like a diode \\(D_1\\) . ::: center ::: Looking into the circuit, the input resistance is simply the resistance of the diode \\(D_1\\) , which is: \\( \\(R_{in} \\approx \\frac{1}{g_m}\\) \\) Thus, a diode-connected MOSFET synthesizes a resistor of low resistance. -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror without Base-Width Modulation Let's now look at a MOSFET current mirror. Refer to the circuit below. ::: center ::: Since the drain and gate are connected, the MOSFET operates in saturation region, because \\( \\(v_D = v_G\\) \\) Therefore, we can say \\( \\(v_{DS} \\geq v_{GS} - V_T\\) \\) From , if we neglect \\(r_o\\) , then the drain current is given by: \\( \\(i_{DS1} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_1(v_{GS1} - V_T)^2\\) \\) \\( \\(i_{DS2} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_2(v_{GS2} - V_T)^2\\) \\) and by dividing both equations we have \\( \\(i_{DS2} = \\frac{(W/L)_2}{(W/L)_1}i_{DS1}\\) \\) If \\(M_1\\) and \\(M_2\\) are identical, such that \\((W/L)_1 = (W/L)_2\\) , then it functions as current mirror as \\(i_{DS1} = i_{DS2}\\) . By adjusting the \\(W/L\\) ratio, it becomes a current amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror with Base-Width Modulation Let's now consider the impact of base-width modulation \\(r_o\\) . When accounted for, the circuit can be depicted as: ::: center ::: Now the equation for drain current becomes \\( \\(i_{DS1} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_1(v_{GS1} - V_T)^2(1 + \\lambda v_{DS1})\\) \\) \\( \\(i_{DS2} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_2(v_{GS2} - V_T)^2(1 + \\lambda v_{DS2})\\) \\) and by dividing both equations we have \\( \\(\\frac{i_{DS2}}{i_{DS1}} = \\frac{(W/L)_2}{(W/L)_1}\\bigg(\\frac{1 + \\lambda v_{DS1}}{1 + \\lambda v_{DS2}}\\bigg)\\) \\) Note that we cannot determine the current ratio simply by \\(W/L\\) alone, as it introduces an uncertainty, since \\(v_{DS1} \\neq v_{DS2}\\) . If we let \\( \\(v_{DS1} = v_{DS} \\hspace{2cm} v_{DS2} = v_{DS} + \\Delta v_{DS}\\) \\) where \\(\\Delta v_{DS}\\) is the difference in voltage between \\(v_{DS1}\\) and \\(v_{DS2}\\) . As you can see, the current ratio is directly proportional to \\(\\Delta v_{DS}\\) \\( \\(\\Delta\\bigg(\\frac{i_{DS2}}{i_{DS1}}\\bigg) = \\frac{(W/L)_2}{(W/L)_1}\\bigg(\\frac{\\lambda\\Delta v_{DS}}{1 + \\lambda v_{DS}}\\bigg)\\) \\) So in order to produce a very accurate current mirror, you must reduce \\(\\Delta v_{DS}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Current Sinks and Current Sources The purpose of current mirrors is to construct current sources and current sinks from a master current source whose value is stable---independent of temperature and supply voltage fluctuations. ::: center ::: Solving them are fairly straightforward, as the currents are set by the ratio of transistor size, specifically the area of the emitter \\(W/L\\) . For example, we have a two-stage CS amplifier. The circuit is denoted in black and the current sources and sinks are in blue. ::: center ::: Given \\(J\\) , then \\(i_{DS1} = J\\) . the current sinks are: \\(\\displaystyle i_{DS2} = \\frac{(W/L)_2}{(W/L)_1}J\\) \\(\\displaystyle i_{DS7} = \\frac{(W/L)_7}{(W/L)_1}J\\) Given \\(i_{DS2} = i_{SD3}\\) , the current sources are: \\(\\displaystyle i_{SD4} = \\frac{(W/L)_4}{(W/L)_3}I_{SD3} = \\frac{(W/L)_4}{(W/L)_3}\\frac{(W/L)_2}{(W/L)_1}J\\) Finally, the current flowing through the circuit: \\(\\displaystyle i_{DS5} = i_{SD4}\\) \\(\\displaystyle i_{SD6} = i_{DS7}\\) Differential MOSFET Voltage Amplifiers \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction As the name suggests, the differential amplifier amplifies the difference between two input signals (or the differential input signal). They are useful for suppressing noise, as the supply voltage is generally not constant. ::: center ::: Its fluctuation is known as supply voltage noise, which affects the load line subsequently the DC operating point. Varying DC operating point causes \\(g_m\\) to vary, resulting in varying \\(v_{out}\\) not caused by \\(v_{in}\\) . ::: center ::: In other words, if there is noise in the supply voltage, then it will also get coupled with the output signal. Therefore, the output of the amplifier that you design should be insensitive to supply voltage noise, ground noise, and the fluctuation of the input of the amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Single-Ended Signaling The amplifiers we have encountered so far are single-ended amplifiers, which amplifies a single input signal. ::: center ::: Signals are represented by nodal voltages with reference to a constant voltage, typically the ground. And as we demonstrated earlier, single-ended signaling is much more susceptible to noise. -4ex -1ex -.4ex 1ex .2ex Differential Amplifiers Thus, we introduce differential signaling. ::: center ::: Compared to single-ended signaling, signals are represented by the difference between two single-ended nodal voltages. It's consists of a DC component \\(v_{in,\\, cm}\\) and AC components \\(v_{in,\\, AC}^+\\) and \\(v_{in,\\, AC}^-\\) , where \\( \\(v_{in}^+ = v_{in,\\, AC}^+ + v_{in,\\, cm} \\hspace{2cm} v_{in}^- = v_{in,\\, AC}^- + v_{in,\\, cm}\\) \\) You can think of \\(v_{in,\\, cm}\\) as our DC biasing voltage of the input transistors of the differential amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex NMOS Differential Pair It consists of a pair of identical common-source amplifiers with the source of the input transistors \\(M_2\\) and \\(M_3\\) tied together. ::: center ::: As you can see \\(V_{b2}\\) acts as our DC biasing voltage (or \\(v_{in,\\,cm}\\) ) and the AC component \\(v_{in}^+\\) , \\(v_{in}^-\\) . ::: center ::: The transistors \\(M_4\\) and \\(M_5\\) function as constant current-source load, which if you recall from the previous module are used in place of a resistor load. Though, one component you may not be particular familiar with is the transistor \\(M_1\\) , located at the source of the CS amplifier. \\(M_1\\) forms the tail current source of constant current \\(I_{SS}\\) , where \\( \\(\\label{eq:nmos_tailcurrent} I_{DS2} = I_{DS3} = \\frac{I_{SS}}{2}\\) \\) The purpose of the tail current source is to split that difference between the two sides of the amplifier. As shown from the equation above, \\(I_{SS}\\) is split evenly between \\(M_2\\) and \\(M_3\\) . -3ex -0.1ex -.4ex 0.5ex .2ex PMOS Differential Pair Suppose we use PMOS as the input transistor. ::: center ::: Similar to NMOS, \\(V_{b2}\\) makes up the DC component and \\(v_{in}^+\\) , \\(v_{in}^-\\) which make up the AC components. ::: center ::: The transistor \\(M_1\\) now forms the head current source of constant current \\(I_{SS}\\) . Similar to tail current source, it is designed to split that difference between the two sides of the amplifier. -4ex -1ex -.4ex 1ex .2ex Differential Voltage Gain Now let's go over how voltage is amplified in a differential amplifier. Using the principle of superposition, we can express the differential signaling as such ::: center ::: The differential input is \\( \\(v_{in} = v_{in}^+ - v_{in}^-\\) \\) where it is made up of two signals \\(180^\\circ\\) out of phase \\( \\(v_{in}^+ = \\frac{v_s}{2} + V_{b2} \\hspace{2cm} v_{in}^- = -\\frac{v_s}{2} + V_{b2}\\) \\) The differential output is \\( \\(\\label{eq:diff_output} v_o = v_o^+ - v_o^-\\) \\) which we can derive from the small-signal equivalent circuit. -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Equivalent Circuit For reference, the circuit below is an NMOS differential pair. ::: center ::: By performing KVL, where \\(V_{SS}\\) is the node voltage of \\(SS\\) then \\( \\(V_{b2} = V_{GS2,3} + V_{SS} \\qquad\\text{ or }\\qquad V_{GS2,3} = V_{b2} - V_{SS}\\) \\) Recall the equation for transconductance (in ), then we can express \\(g_{m2,3}\\) as \\( \\(g_{m2,3} = \\frac{2I_{DS2,3}}{\\underbrace{V_{b2} - V_{SS}}_{V_{GS2,3}} - V_T}\\) \\) Now we'll separate our DC and AC components of the differential amplifier. ::: center ::: The \\(I_{SS}/2\\) represents our DC current, which we already know from and . The \\(\\Delta i\\) represents our AC current. Then, the current of \\(M_2\\) and \\(M_3\\) is \\( \\(i_{DS2} = \\frac{I_{SS}}{2} + \\Delta i \\hspace{2cm} i_{DS3} = \\frac{I_{SS}}{2} - \\Delta i\\) \\) If you remember, a transconductor maps a voltage to a current. So in AC, we're mapping the \\(v_s/2\\) to \\(\\Delta i\\) , which we can say \\( \\(\\Delta i = g_{m2,3}\\bigg(\\frac{v_s}{2}\\bigg)\\) \\) If we perform KCL at the node \\(SS\\) , then the current flowing through \\(M_1\\) is constant. \\( \\(i_{DS1} = i_{DS2} + i_{DS3} = I_{SS}\\) \\) It is independent of the input source, that is \\(v_s\\) , or in other words, it is constantcurrent doesn't change. This means we can turn the node \\(SS\\) into an AC ground. ::: center ::: So in small-signal analysis, the differential amplifier can be split into two identical common-source amplifiers whose AC outputs are given by \\( \\(v_o^+ = g_{m3}(r_{o3} \\parallel r_{o5})\\bigg(-\\frac{v_s}{2}\\bigg)\\) \\) \\( \\(v_o^- = g_{m2}(r_{o2} \\parallel r_{o4})\\bigg(\\frac{v_s}{2}\\bigg)\\) \\) Note that \\(g_{m2} = g_{m3}\\) and \\(r_{o2,3} = r_{o4,5}\\) . From , the differential output is \\( \\(v_o = v_o^+ - v_o^- = g_{m2,3}(r_{o2,3} \\parallel r_{o4,5})v_s\\) \\) where the voltage gain is \\( \\(A_v = \\frac{v_o}{v_i}\\times\\frac{v_s}{v_i} = g_{m2}(r_{o2} \\parallel r_{o4}) = g_{m3}(r_{o3} \\parallel r_{o5})\\) \\) Notice it has the same voltage gain as that of corresponding common-source amplifier (in ). It doesn't provide any additional gain, which might not make sense at first. But what this provides is a way to deal with supply voltage noise, without having to worry about the gain being modified for using differential signaling. -3ex -0.1ex -.4ex 0.5ex .2ex Power Consumption This, however, comes at the cost of uses twice the power consumption. Since the voltage gain is the same, under the condition that the both transistors have the same transconductance \\( \\(g_{m} = \\frac{2I_{DS}}{{V_{b2} - V_{SS}} - V_T}\\) \\) Then the current \\(I_{DS}\\) of each branch of the differential pair, \\(M_2\\) and \\(M_3\\) , must be the same as that of CS amplifier. ::: center ::: Therefore, the tail current of the differential pair is twice that of the DC current of CS amplifier. As result, the power consumption of differential part is \\(2\\times\\) that of CS amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Differential-Input Single-Ended-Output Amplifiers In most scenarios, we only want one output \\(v_o\\) , instead of two outputs \\(v_o^+\\) and \\(v_o^-\\) . So how can we modify the differential amplifier to do such task? ::: center ::: It is pretty simple, which involves using a current mirror to perform differential-to-single ended conversion. ::: center ::: The output current is \\( \\(i_o = 2 \\times g_m\\bigg(\\frac{v_s}{2}\\bigg) = g_mv_s\\) \\) and the output voltage is \\( \\(v_o = (r_{o3} \\parallel r_{o5})i_o = g_m(r_{o3} \\parallel r_{o5})v_s\\) \\) As you can see, we have a non-inverting amplifier with the same voltage gain as that of the differential-input differential-output amplifier. -4ex -1ex -.4ex 1ex .2ex Rejection of Supply Noise Let's now go over the process of rejecting noise from the supply voltage. We can represent the supply voltage as \\(V_{DD} + v_{dd}\\) , where \\(v_{dd} \\ll V_{DD}\\) represents the supply voltage noise. ::: center ::: Our goal is to know whether the differential output of the differential pair is affected by \\(v_{dd}\\) or not. We'll first look at a differential-input differential-output amplifierstwo inputs and two outputs. ::: center ::: You might have notice we also short-circuited \\(v_s/2\\) , even though it's not constant. The purpose is that we are only interested in the response of the amplifier due to \\(v_{dd}\\) . Performing KCL at the output nodes \\(v_o^+\\) and \\(v_o^-\\) results in \\( \\(v_o^+ \\approx g_{m5}(r_{o3} \\parallel r_{o5})v_{dd}\\) \\) \\( \\(v_o^- \\approx g_{m4}(r_{o2} \\parallel r_{o4})v_{dd}\\) \\) As you can see, the output voltages are a function of the supply voltage noise \\(v_{dd}\\) . However, when we calculate the differential output \\( \\(v_o = v_o^+ - v_o^- = 0\\) \\) It is \\(0\\) , which means the differential output is insensitive to supply voltage noise. You can refer to the diagram below. ::: center ::: When there's noise, it's gonna look the same on both output voltages. And so when we compare the two output voltages, \\(v_o = v_o^+ - v_o^-\\) , the noise will cancels each other out. ::: center ::: Now let's look at a differential-input single-ended-output amplifierstwo inputs and one outputs. ::: center ::: As we covered in the previous section, we can modify the differential amplifier, such that there is only one output, but comes at a cost. If we perform KCL and node \\(A\\) and node \\(B\\) , we can derive the single-ended output voltage \\( \\(v_o = \\frac{g_{m5}v_A + g_{o5}v_{dd}}{g_{m5} + g_{o3} + g_{o5}}\\) \\) Then \\(v_o\\) is a function of the \\(v_{dd}\\) , meaning that differential-input single-ended output amplifiers cannot reject voltage noise. -3ex -0.1ex -.4ex 0.5ex .2ex Ground Noise As of now we only covered noise coming from supply voltage, but there's also noise coming from the ground, that is caused by improper grounding such as a large resistance of ground rails. ::: center ::: When a noise-current enters the ground rail, the potential of the ground rail is no longer at \\(\\SI{0}{\\volt}\\) fluctuating from \\(0\\) to \\(v_{ss}\\) , where \\(v_{ss}\\) is typically tens of \\(\\si{\\milli\\volt}\\) . Using small-signal analysis, we can find the output voltage of the differential pair caused by \\(v_{ss}\\) . ::: center ::: Performing KCL at the output nodes \\(v_o^+\\) and \\(v_o^-\\) results in \\( \\(v_o^+ \\approx g_{m3}(r_{o3} \\parallel r_{o5})v_{ss}\\) \\) \\( \\(v_o^- \\approx g_{m2}(r_{o2} \\parallel r_{o4})v_{ss}\\) \\) \\( \\(v_o = v_o^+ - v_o^- = 0\\) \\) As with supply voltage noise, the output is also insensitive to ground noise. -4ex -1ex -.4ex 1ex .2ex Common-Mode Referring to the diagram below, we mentioned previously, \\(v_{in,\\,cm}\\) and \\(v_{o,\\,cm}\\) depicts the DC operating point, whereas \\(v_{in,\\,AC}^{+,-}\\) and \\(v_{o,\\,AC}^{+,-}\\) carry the information. ::: center ::: Ideally, we want the common-mode input \\(v_{in,\\,cm}\\) to be stable, as it provides the DC biasing voltage. However, that is something we have no control over, so instead we should design our differential amplifier to be insensitive to \\(v_{in,\\,cm}\\) fluctuations. -3ex -0.1ex -.4ex 0.5ex .2ex Common-Mode Voltage Gain The common-mode voltage gain is defined as \\( \\(A_{v,\\,cm} = \\frac{v_{o,\\,cm}}{v_{in,\\,cm}}\\) \\) which can be obtained by applying both \\(v_{in,\\,cm}\\) to both inputs and measuring either ends of the single-ended outputs of the differential pair, shown below. ::: center ::: Ideally, we prefer a common-mode voltage gain \\(A_{v,\\,cm}\\) of \\(0\\) , which means there's zero fluctuations from the input to output. Similar to what we did with voltage noise, we will perform small-signal analysis. ::: center ::: A few things to make note of: Consider \\(r_{o1}\\) as the result of two resistors, \\(2r_{o1}\\) , to be connected in parallel. Then we have that node \\(SS1\\) and node \\(SS2\\) to be identical, such that no current flows in between the nodes, since they have same voltages, \\(v_{SS2} - v_{SS1} = 0\\) . Therefore, we can separate it, forming two sub-circuits, that can be identified as a common-source amplifier with source degeneration. From , we can modify it to obtain the common-mode voltage gain, that is \\( \\(A_{v,\\,cm} \\approx \\frac{-g_{m2,3}(r_{o2} \\parallel r_{o4})}{1 + g_{m2,3}(2r_{o1})}\\) \\) In order to have a \\(A_{v,\\,cm} \\approx 0\\) , the denominator must be very large. In other words, \\(r_{o1}\\) forms source degeneration and significantly lowers \\(A_{v,\\,cm}\\) . ::: list Note that \\(r_{o1}\\) has no impact on the differential voltage gain. ::: -3ex -0.1ex -.4ex 0.5ex .2ex Voltage Gain Comparison To expand a bit more on the final bit, one of the reason for using a tail current source (or head current source for PMOS) is to reduce the common-mode voltage gain. In technicality, we can choose to not add it, as it is indeed optional. ::: center ::: Both amplifiers still have the same differential voltage gain. However, the circuit on the left will have a larger common-mode voltage gain. If \\(v_{in,\\,cm}\\) fluctuates, in turn, the circuit's input and output will fluctuate significantly as well. -3ex -0.1ex -.4ex 0.5ex .2ex CMRR This is one of the parameters we use quantify amplifiers. We won't go much in-depth regarding this topic, but a good amplifier should have: a large differential-mode voltage gain \\(A_{v,\\,\\text{diff}}\\) , and ... a small common-mode voltage gain \\(A_{v,\\,cm}\\) . CMRR stands for common-mode rejection ratio which is \\( \\(\\text{CMRR} = 20\\log{\\bigg(\\frac{A_{v,\\,\\text{diff}}}{A_{v,\\,cm}}\\bigg)}\\) \\) A typically value is around \\(\\SI{80}{\\decibel}\\) . -4ex -1ex -.4ex 1ex .2ex Mismatch In the design stage, we always assume that the two branches of a differential amplifier to be completely identical ::: center ::: such that: Load match: \\(R_{D1} = R_{D2} = R_D\\) Dimensions match: \\((W/L)_1 = (W/L)_2 = (W/L)\\) Threshold voltage match: \\(V_{T1} = V_{T2} = V_T\\) However, by the time the circuit is fabricated, you will see that they are different. This is due to the fabrication process uncertainties which give rise to mismatch between the branches. ::: center ::: While it is not possible to get rid of the mismatch, our goal is to minimize the impact it has on our circuit. We always assume the worst-case: Worst-case load mismatch: \\(\\pm\\Delta R_D\\) Worst-case dimension mismatch: \\(\\pm\\Delta (W/L)\\) Worst-case threshold mismatch: \\(\\pm\\Delta V_T\\) -2ex -0.1ex -.2ex .2ex .2ex Load Mismatch To calculate the load mismatch, we assume the worst-case, so let \\(R_{D1} = R_D + \\Delta R_D\\) and \\(R_{D2} = R_D - \\Delta R_D\\) with no other mismatch. Then apply a common-mode voltage \\(v_{in,\\,cm}\\) to both inputs. ::: center ::: Performing KVL, the output nodes \\(v_o^+\\) and \\(v_o^-\\) are \\( \\(v_o^+ = V_{DD} - (R_D - \\Delta R_D)i_{DS}\\) \\) \\( \\(v_o^- = V_{DD} - (R_D + \\Delta R_D)i_{DS}\\) \\) then the differential output is \\( \\(v_{o,\\,\\text{diff},\\,R_D} = v_o^+ - v_o^- = 2i_{DS}\\Delta R_D\\) \\) The output offset voltage caused by load mismatch is proportional to \\(\\Delta R_D\\) . -2ex -0.1ex -.2ex .2ex .2ex Dimension Mismatch Similarly, to calculate the dimension mismatch, let \\((W/L)_2 = (W/L) + \\Delta (W/L)\\) and \\((W/L)_3 = (W/L) - \\Delta (W/L)\\) with no other mismatch. Apply \\(v_{in,\\,cm}\\) to both inputs. ::: center ::: The differential output is \\( \\(v_{o,\\,\\text{diff},W/L} = v_o^+ - v_o^- = -2R_DI_{DS}\\bigg[\\frac{\\Delta (W/L)}{(W/L)}\\bigg]\\) \\) The output offset voltage caused by dimension mismatch is inversely proportional. This means that if we want to reduce the dimension mismatch, we need to increase \\((W/L)\\) . -2ex -0.1ex -.2ex .2ex .2ex Threshold Voltage Mismatch Lastly, we have the threshold voltage mismatch. As usual, let \\(V_{T2} = V_T + \\Delta V_T\\) and \\(V_{T3} = V_T - \\Delta V_T\\) with no other mismatch and apply \\(v_{in,\\,cm}\\) to both inputs. ::: center ::: The differential output is \\( \\(v_{o,\\,\\text{diff},\\,V_T} = v_o^+ - v_o^- = -4k_n \\frac{W}{L}(V_{in,\\,cm} - V_{SS} - V_T)\\Delta V_T\\) \\) The output offset voltage caused by threshold voltage mismatch is proportional to \\(\\Delta V_T\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Offset Voltage The load mismatch, dimension mismatch, and threshold voltage mismatch are uncorrelated. Therefore, the total mismatch-induced output offset voltage is obtained from \\( \\(v_{os,\\,\\text{output}} = \\sqrt{v_{o,\\,\\text{diff},\\,R_D}^2 + v_{o,\\,\\text{diff},W/L}^2 + v_{o,\\,\\text{diff},\\,V_T}^2}\\) \\) The mismatch results in a non-zero output voltage, \\(v_{os,\\,\\text{output}} \\neq 0\\) . In circuit analysis, we always want to bring this all the way back to the input, such that we can make comparisons at the input. ::: center ::: The way we do this, is by attaching an artificial input offset voltage source \\(v_{os,\\,input}\\) which we can be calculate via dividing by differential voltage gain \\( \\(v_{os,\\,input} = \\frac{v_{os,\\,output}}{A_{v,\\,\\text{diff}}}\\) \\) Typically, it is placed at the non-inverting input of the amplifier \\(v_{in}^+\\) . We will use a technique called auto-zeroing to remove the impact of \\(v_{os,\\,input}\\) . -2ex -0.1ex -.2ex .2ex .2ex Virtual Short Characteristics Before going over the technique, we must understand what exactly is a virtual short. ::: center ::: It refers to a condition of a differential input amplifier, in which its non-inverting and inverting inputs have almost the same voltage \\( \\(v_{in}^+ = v_{in}^-\\) \\) There's two conditions to be met: They form a negative feedback, \\(v_o^+\\) and \\(v_o^-\\) are connected to \\(v_{in}^-\\) and \\(v_{in}^+\\) respectivelypositive to negative and vice-versa. The differential voltage gain \\(A_{v,\\,\\text{diff}}\\) is sufficiently large. -2ex -0.1ex -.2ex .2ex .2ex Auto-Zeroing Technique To demonstrate the auto-zeroing, we have switches that are controlled by the input signal. When \\(\\phi = 1\\) , we have a virtual short, where \\(v^+ = v^-\\) , and the capacitor is charged to \\(v_{os}\\) with polarity shown. When \\(\\phi = 0\\) , it does some magic and the voltage of the capacitor cancels out \\(v_{os}\\) . Then poof, the offset voltage goes bye bye. ::: center ::: Some remarks: \\(v_{os}\\) needs to remain unchanged during both \\(\\phi = 0\\) and \\(\\phi 1\\) . \\(v_o\\) is not available during \\(\\phi = 1\\) . Switches are implemented using MOSFET.","title":"ELE404"},{"location":"W2022/ELE404/ELE404/#diodes-and-their-application","text":"","title":"Diodes and Their Application"},{"location":"W2022/ELE404/ELE404/#semiconductors","text":"The first few topics covered in beginning of the module, might be reminiscent of topics covered in PCS224, so it won't go to in-depth. As a recap, a link to the notes from Fall 2021:","title":"Semiconductors"},{"location":"W2022/ELE404/ELE404/#application-of-diodes","text":"We will now go over the actual application of diodes and ways to analyze them. -3ex -0.1ex -.4ex 0.5ex .2ex Diode Rectifier and Filter One of the most important applications of diodes is in the design of rectifier circuitsconvert an AC voltage to a DC voltage. There's two types: half-wave and full-wave rectifier. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Half-Wave Rectifier The half-wave rectifier utilizes alternate half-cycles of the input sinusoid. The circuit is shown below. ::: center ::: The circuit functions as follows: Positive Cycle: When \\(v_{in} \\geq v_D\\) , the output voltage \\(v_O = v_{in} - v_D\\) , due to the constant-voltage-drop diode. Negative Cycle: When \\(v_{in} < v_D\\) , the output voltage \\(v_O = 0\\) , since the diode is reverse-biased. Note that for an ideal diode, the voltage drop would be \\(v_D = 0\\) , as shown in the left. However, since these do not actually exists, we use a more realistic model, as shown in the right. ::: center ::: As you can see in the graph, a half-wave rectifier has a low energy efficiency as half of the input is wasted. -2ex -0.1ex -.2ex .2ex .2ex Half-Wave Rectifier with a Filter Capacitor In order to filter out the ripples, a large capacitor \\(C\\) is neededreducing the substantially the variations in the rectifier output voltage \\(v_O\\) . The circuit is shown below ::: center ::: As a result, when \\(v_{in}\\) drops from its peak, the capacitor will discharge, instead of a dropping all the way to \\(\\SI{0}{\\volt}\\) . ::: center ::: A closer look at the output voltage, we can derive a few equations. The peak voltage \\(V_P\\) is the maximum value of the DC source, where \\( \\(V_P = v_{in,\\ max} - v_D\\) \\) The ripple voltage \\(V_r\\) is the residual periodic variation of the DC voltagethe difference between \\(V_P\\) and the voltage when discharging ends. \\( \\(V_r = \\frac{T}{\\tau}V_P = \\frac{V_P}{fR_LC} = \\frac{i_L}{fC}\\) \\) The output DC voltage \\(\\overline{v_O}\\) can be obtained by taking the average of the extreme values of \\(v_O\\) . \\( \\(\\overline{v_O} = V_P - \\frac{1}{2}V_r\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Full-Wave Rectifier The full-wave rectifier has improve efficiency, by utilizing both halves of the sinusoid. The circuit is shown below. ::: center ::: The circuit functions as follows: Positive Cycle: When \\(v_{in} \\geq v_D\\) , the upper diode \\(D_1\\) will be ON, while lower diode \\(D_2\\) will be OFF, and the output voltage \\(v_O = v_{in} - v_D\\) . Negative Cycle: When \\(v_{in} \\leq -v_D\\) , the upper diode \\(D_1\\) will be OFF, while lower diode \\(D_2\\) will be ON, and the output voltage \\(v_O = |v_{in}| - v_D\\) . ::: center ::: The output voltage is shown below. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Full-Wave Rectifier with a Filter Capacitor Likewise, a large shunt capacitor is used to filter out voltage ripples. The circuit is shown below. ::: center ::: The only notable difference that can be seen from the graph is the period \\(T\\) cut in half. ::: center ::: The peak voltage \\(V_P\\) is still the same as previously shown, where \\( \\(V_P = v_{in,\\ max} - v_D\\) \\) For the ripple voltage \\(V_r\\) , the period \\(T\\) is replaced by \\(T/2\\) or \\(2f\\) , resulting in \\( \\(V_r = \\frac{V_P}{2fR_LC} = \\frac{i_L}{2fC}\\) \\) As in the half-wave case, the output DC voltage \\(\\overline{v_O}\\) doesn't change. \\( \\(\\overline{v_O} = V_P - \\frac{1}{2}V_r\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Bridge Rectifier A cheaper implementation of the full-wave rectifier with no center-tapped transformer. However, it uses two diodes in conjunction, resulting in a bigger voltage loss. The circuit is shown below. ::: center ::: The circuit functions as follows: Positive Cycle: When \\(v_{in} \\geq 2v_D\\) , the diodes \\(D_1\\) , \\(D_2\\) will be ON, while diodes \\(D_3\\) , \\(D_4\\) will be OFF, and the output voltage \\(v_O = v_{in} - 2v_D\\) . Negative Cycle: When \\(v_{in} \\leq 2v_D\\) , the diodes \\(D_1\\) , \\(D_2\\) will be OFF, while diodes \\(D_3\\) , \\(D_4\\) will be ON, and the output voltage \\(v_O = |v_{in}| - 2v_D\\) . ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Bridge Rectifier with a Filter Capacitor Let's apply a capacitor again to filter out the ripples. The circuit is shown below. ::: center ::: Like the full-wave rectifier, the period is also half \\(T/2\\) of the input wave. ::: center ::: However, the peak voltage has a bigger voltage loss of \\(2v_D\\) due to the two diodes. \\( \\(V_P = v_{in,\\ max} - 2v_D\\) \\) The ripple voltage \\(V_r\\) is the same as stated in the full-wave rectifier \\( \\(V_r = \\frac{V_P}{2fR_LC} = \\frac{i_L}{2fC}\\) \\) The same case for the output DC voltage \\(\\overline{v_O}\\) which also doesn't change. \\( \\(\\overline{v_O} = V_P - \\frac{1}{2}V_r\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Voltage Regulator A voltage regulator is a circuit whose purpose is to provide a constant DC voltage between its output terminalsto further reduce the voltage ripple. -2ex -0.1ex -.2ex .2ex .2ex Resistor-based Voltage Regulator The resistor-based voltage regulator is not the most ideal as you'll see why. The circuit is shown below ::: center ::: The output voltage \\(v_O\\) is given by \\( \\(v_O = \\frac{R_L \\parallel R_2}{R_L \\parallel R_2 + R_1}V_{CC}\\) \\) For an ideal voltage regulator, we want \\(v_O\\) to be independent of \\(R_L\\) , as it's something we have no control over. Though one may consider the case if \\(R_L \\gg R_1, R_2\\) , then \\(R_L \\parallel R_2 \\approx R_2\\) \\( \\(v_O \\approx \\frac{R_2}{R_1 + R_2}V_{CC}\\) \\) While \\(v_O\\) is independent of \\(R_L\\) , there are still some flaws in the design: (1) It is still proportional to \\(V_{CC}\\) resulting in \\(v_O\\) fluctuating based on \\(V_{CC}\\) (2) Since \\(R_1\\) and \\(R_2\\) is very small, a very large current will flow through them; resulting in a rapid increase of temperature and smoke. -2ex -0.1ex -.2ex .2ex .2ex Diode-based Voltage Regulator Suppose we now replace \\(R_2\\) with a forward-biased diode, as shown below. ::: center ::: If you recall, the equivalent of a forward-biased diode is to a small voltage source \\(\\SI{0.7}{\\volt}\\) in series with small resistance \\(r_D\\) . The ideal output voltage \\(v_O\\) is given by \\( \\(v_O = 0.7N\\) \\) where \\(N\\) is the number of diodes cascaded and \\(r_D = \\SI{0}{\\ohm}\\) . Note that \\(V_{CC} > 0.7N\\) is required. However, the actual value of \\(v_O\\) is determined by including \\(r_D\\) . By performing KCL at node \\(v_O\\) , we get \\( \\(G_L(v_O) + G(v_O - V_{CC}) + g_D(v_O - 0.7) = 0\\) \\) Note that the resistance is converted to conductance to minimize the use of fraction, where \\( \\(G = \\frac{1}{R} \\to G_L = \\frac{1}{R_L} \\text{ and } g_D = \\frac{1}{r_D}\\) \\) Isolating for \\(v_O\\) , we get the actual output voltage which is given by \\( \\(v_O \\approx 0.7 \\bigg[1 + \\frac{GV_{CC}}{0.7g_D} - \\frac{G_L + G}{g_D}\\bigg]\\) \\) We can determine the sensitivity of \\(v_O\\) to \\(R_L\\) and \\(V_{CC}\\) to see how much it fluctuates by \\( \\(\\frac{dv_O}{dR_L} = \\frac{0.7}{g_D} = 0.7r_D \\hspace{4cm} \\frac{dv_O}{dV_{CC}} = \\frac{G}{g_D} = \\frac{r_D}{R}\\) \\) Since \\(r_D\\) is typically a small value, where \\(r_D \\ll R, R_L\\) or \\(g_D \\gg G, G_L\\) , the output voltage is less sensitive to both \\(V_{CC}\\) and \\(R_L\\) , thus making it a good voltage regulator. -2ex -0.1ex -.2ex .2ex .2ex Zener-diode-based Voltage Regulator Let's now consider a Zener diode, which is reverse-biased, as shown below. ::: center ::: Likewise, we can also draw the equivalent circuit of a Zener diode as such. Then by performing KCL at node \\(v_O\\) , we get \\( \\(G_Lv_O + G(v_O - V_{CC}) + g_Z(v_O - V_{Z0}) = 0\\) \\) and isolating for \\(v_O\\) , the actual output voltage is given by \\( \\(v_O \\approx V_{Z0}\\bigg[1 + \\frac{GV_{CC}}{V_{Z0}g_Z} - \\frac{G_L + G}{g_Z}\\bigg]\\) \\) The sensitivity of \\(v_O\\) to \\(R_L\\) and \\(V_{CC}\\) are \\( \\(\\frac{dv_O}{dG_L} = -\\frac{V_{Z0}}{g_Z} = -V_{Z0}r_Z \\hspace{4cm} \\frac{dv_O}{dV_{CC}} = -\\frac{G}{g_Z} = \\frac{r_Z}{R}\\) \\) If you notice, the sensitivity of \\(v_O\\) for forward-bias and Zener diode are almost identical, so how exactly do we determine which is better? ::: center ::: If we were to compare the slope side-by-side, you will notice that \\(g_Z \\gg g_D\\) , which means there's less fluctuations when using a Zener diode voltage regulator oppose to a forward-biased diode. -3ex -0.1ex -.4ex 0.5ex .2ex Limiting and Clamping Circuits In this section, we will present other circuit applications of diodes apart from rectifier circuits. -2ex -0.1ex -.2ex .2ex .2ex Voltage Clipper As the name suggest, it takes an input waveform and clips or cuts off its top half, bottom half or both halves depending on the orientation and number of diode. ::: center ::: The circuit works by alternating between the two diodes in forward and reverse-bias, resulting in both halves being clipped. The sinusoidal wave is displayed below. ::: center ::: A few other variety of basic limiting circuits with respect to its transfer characteristics: ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Peak Detector A peak detector is used to detect maximum value of the input voltage. The circuit looks similar to that of a half-wave rectifier, just without a load connected onto. ::: center ::: Since there's no load to discharge on, the output voltage \\(v_O\\) remains constant. It will only rise when in forward-biased and there exists a \\(v_{in} > v_O\\) . The sinusoidal wave is displayed below. ::: center ::: Keep in mind, we assumed the diode is an ideal diode. For an actual diode, there will be some voltage drop \\(v_D\\) across the peak. Similar to a real half-wave rectifier, as discussed previously. -2ex -0.1ex -.2ex .2ex .2ex Voltage Doubler The last application of diode which will cover is a voltage doubler. ::: center ::: The circuit can be broken down into two sections: the positive and negative phase. ::: center ::: During the negative phase, \\(D_1\\) will be ON and \\(D_2\\) will be OFF, which leaves the capacitor \\(C_1\\) to charge up. ::: center ::: Then when it enters the positive phase, \\(D_1\\) will be OFF and \\(D_2\\) will be ON, the output voltage \\(V_O\\) is the sum of the input voltage and the voltage of \\(C_1\\) obtained in the previous half cycle.","title":"Application of Diodes"},{"location":"W2022/ELE404/ELE404/#bipolar-junction-transistors","text":"A bipolar transistor is a semiconductor device commonly used for amplificationbipolar as in both holes and electrons serve as current carriers. -4ex -1ex -.4ex 1ex .2ex Structure of BJT They are of two types of bipolar junction transistors (BJTs) namely; NPN transistor ::: center ::: PNP transistor ::: center ::: ::: list The direction of the arrow in the transistor symbol (between base and emitter) tells you the direction of current. ::: The BJT consists of three differently doped semiconductor region: the emitter region (e), the base region (b), and the collector region (c). Emitter: \\(n^+\\) for NPN transistors and \\(p^+\\) for PNP transistors. Base: \\(p\\) for NPN transistors and \\(n\\) for PNP transistors. Collector: \\(n\\) for NPN transistor and \\(p\\) for PNP transistors. If you notice the width is not symmetrical due to the doping ratios, where the emitter is heavily doped, the collector is moderately doped and the base is lightly doped. -4ex -1ex -.4ex 1ex .2ex Operation of BJT If you notice from the diagram earlier, the transistor consists of two pn junctions, the emitter--base junction (EBJ) and the collector--base junction (CBJ). Depending on the bias condition of each of these junctions, different modes of operation of the BJT are obtained, as shown below. ::: tabu c c c Mode & Emitter-Base Junction & Collector-Base Junction \\ Cut-Off & Reverse & Reverse\\ Active & Forward & Reverse\\ Saturation & Forward & Forward\\ ::: Let's define the bias conditions for modes of operation in the EBJ. ::: center ::: The order of subscript matter, which implies the order in which they are calculated. For NPN, \\(V_{BE} = V_B - V_E\\) , whereas for PNP, \\(V_{EB} = V_E - V_B\\) . Likewise, the same implies for \\(V_{CB}\\) and \\(V_{BC}\\) . Forward Bias: When \\(V_{BE} > \\SI{0.7}{\\volt}\\) or \\(V_{EB} > \\SI{0.7}{\\volt}\\) , electrons or holes in the emitter will exit and move to the base or collector. Reverse Bias: When \\(V_{BE} < \\SI{0.7}{\\volt}\\) or \\(V_{EB} < \\SI{0.7}{\\volt}\\) , no flow of electrons or holes from the emitter to the base or collector. ::: list Technically, the diode is forward-biased when \\(V > 0\\) and the diode is negative biased when \\(V < 0\\) . However, we account for the voltage drop in diode, in this scenario, which is \\(\\SI{0.7}{\\volt}\\) . ::: Figuratively, we can describe it using the \\(i\\) - \\(v\\) relationship of the emitter-base junction: ::: center ::: Note that the small region in between cut-off and active region is not the saturation regionwill get to this in a bit. Just know that we do not operate in this region. -3ex -0.1ex -.4ex 0.5ex .2ex Cut-Off Mode In cut-off mode, the EBJ is in reverse bias ( \\(V_{BE} < \\SI{0.7}{\\volt}\\) for NPN and \\(V_{EB} < \\SI{0.7}{\\volt}\\) for PNP), which means no emitter current, \\(i_E = 0\\) . Since \\( \\(i_E = i_C + i_B\\) \\) where the emitter current \\(i_E\\) is equal to the sum of the collector current \\(i_C\\) and base current \\(i_B\\) . We have \\(i_C = 0\\) and \\(i_B = 0\\) . The transistor is essentially inactive, thus appears as an open-circuit. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Active Mode Of the three modes, the active mode is the most important. In active mode, the EBJ is in forward bias ( \\(V_{BE} > \\SI{0.7}{\\volt}\\) for NPN and \\(V_{EB} > \\SI{0.7}{\\volt}\\) for PNP) and the CBJ is in reverse bias. We'll focus on the NPN transistor, but note that PNP transistor operates in the similar manner. As a reminder, current and electron flow are backwards: Since the EBJ is forward-biased (the diode is ON), free electrons from the emitter region can easily cross the emitter-base junction. ::: center ::: If you remember, the base is lightly doped, so only a small percentage of the free electrons can recombine with the holes in the base region, which produces a small \\(i_B\\) . ::: center ::: The free electrons that entered the base region but didn't recombine with the holes move toward the CBJ which is reverse-biased (the diode is OFF), producing \\(i_C\\) , such that \\(i_C \\gg i_B\\) . ::: center ::: ::: list The collector region is connected to the \\(+\\) of \\(V_{CB}\\) , so free electrons in the base region are attracted to the \\(+\\) side and are swept across into the collector region, despite the diode being reverse-biased. Refer to the zoomed-in diagram in the right. ::: In order to use the BJT as an amplifier, it should be operated in the active region. The conceptual circuit for NPN is shown below to illustrate the operation of the transistor as an amplifier. ::: multicols 2 ::: flushleft ::: The total base-emitter voltage becomes \\( \\(v_{BE} = V_{bias} + v_{in}\\) \\) where \\(V_{bias}\\) and \\(v_{in}\\) are the DC and AC base-emitter voltage respectively. Each serve a different purpose. ::: The DC voltage, \\(V_{bias} > \\SI{0.7}{\\volt}\\) , allows us to operate where \\(i_C-v_{BE}\\) relation is linear. The input signal to be amplified is represented by the AC voltage \\(v_{in}\\) that is superimposed on \\(V_{bias}\\) . ::: center ::: We need to perform two types of analysis: DC analysis, to determine the operating point and ... AC analysis, to determine voltage gain. In order to do these analysis, we need to model the BJT using two transistor models: large-signal equivalent circuit and small-signal equivalent circuit. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Large-Signal Equivalent Circuit (without Base-Width Modulation) We'll begin by only considering the DC component, so the small-signal source is eliminated for DC analysis, such that \\(v_{BE} = V_{bias}\\) . ::: center ::: An important thing to make note of, since \\(i_B \\ll i_C\\) , we have the approximation \\( \\(i_E \\approx i_C\\) \\) So you may often see \\(i_C\\) and \\(i_E\\) interchange. The collector current can be expressed as \\( \\(\\label{eq:collectorcurrent} i_C \\approx I_Se^{v_{BE}/V_t}\\) \\) If you notice, this is equivalently the equation for diode current. The base current can be expressed as a fraction of the collector current \\( \\(i_B = \\frac{i_C}{\\beta} = \\bigg[\\frac{I_S}{\\beta}\\bigg]e^{v_{BE}/V_t}\\) \\) where \\(\\beta\\) is a transistor parameter. Note for PNP, replace \\(v_{BE}\\) with \\(v_{EB}\\) . We can form a set of relations between \\(i_C\\) , \\(i_B\\) , and \\(i_E\\) : \\( \\(i_C = \\alpha i_E \\hspace{2cm} i_C = \\beta i_B\\) \\) where \\(\\alpha\\) is related to \\(\\beta\\) by \\( \\(\\alpha = \\frac{\\beta}{\\beta + 1}\\) \\) We can express the emitter current as \\( \\(\\label{eq:emitter_nobase} i_E = i_B + i_C = \\frac{i_C}{\\alpha} = \\bigg[\\frac{I_S}{\\alpha}\\bigg]e^{v_{BE}/V_t}\\) \\) Thus, the large-signal equivalent circuit of the BJT in active mode would look like ::: center ::: ::: list This circuit does not account for the base-width modulation, which occurs when the connector-emitter voltage is increased. As noted in , the collector current is only affected by \\(v_{BE}\\) , but not by \\(v_{CE}\\) . ::: -2ex -0.1ex -.2ex .2ex .2ex Large-Signal Equivalent Circuit (with Base-Width Modulation) Previously, we assumed that if we increased the connector-emitter voltage \\(v_{CE}\\) , that the collector current \\(i_C\\) will remain constant. However, this is not the case as you'll see from the \\(i_C\\) - \\(v_{CE}\\) characteristics of a BJT. ::: center ::: In reality, \\(i_C\\) does depends on \\(v_{CE}\\) . It varies linearly with \\(v_{CE}\\) . This is due to the Early effect, which we can derive the equation of the slope, by ::: center ::: So our collector current can be expressed as \\( \\(\\label{eq:collectorcurrentbw} i_C \\approx I_Se^{v_{BE}/V_t} + I_Se^{v_{BE}/V_t}\\bigg[\\frac{v_{CE}}{V_A}\\bigg] = I_Se^{v_{BE}/V_t}\\bigg[1 + \\frac{v_{CE}}{V_A}\\bigg]\\) \\) Alternatively, the preferred way of writing is expressed as \\( \\(i_C \\approx I_Se^{v_{BE}/V_t} + g_ov_{CE}\\) \\) where \\(g_o\\) is the output conductance \\( \\(g_o = \\frac{1}{r_o} = \\frac{I_Se^{v_{BE}/V_t}}{V_A} \\approx \\frac{I_C}{V_A}\\) \\) One very important thing to make note of, \\(r_o\\) is not a physical resistorits intended to quantify the impact of base-width modulation onto the circuit; more-so an artificial parameter. Thus, the large-signal equivalent circuit of the BJT in active mode would now look like ::: center ::: We'll discuss the small-equivalent circuit in a separate section, as they're a lot to talk about. -3ex -0.1ex -.4ex 0.5ex .2ex Saturation Mode In saturation mode, the EBJ is in forward bias ( \\(V_{BE} > \\SI{0.7}{\\volt}\\) for NPN and \\(V_{EB} > \\SI{0.7}{\\volt}\\) for PNP) and the CBJ is in forward bias ( \\(V_{BC} > \\SI{0.7}{\\volt}\\) for NPN and \\(V_{CB} > \\SI{0.7}{\\volt}\\) for PNP). The BJT behaves as a resistor in saturation region. Looking back at the \\(i_C-v_{CE}\\) characteristics of a BJT, the active and saturation region is separated by the blue dashed line. ::: center ::: This is the boundary voltage, \\(v_{CE,\\, sat}\\) , typically around \\(\\SI{0.1}{\\volt}\\) to \\(\\SI{0.2}{\\volt}\\) . A closer look at the BJT in saturation: ::: center ::: In deep saturation, a better \\(i_C-v_{CE}\\) linear relation exists, where \\(v_{CE}\\) is smaller. As mentioned earlier, the BJT behaves likes a resistor which has a resistance of: \\( \\(r_{CE,\\, sat} = \\frac{1}{\\tfrac{di_C}{dv_{CE}}}\\) \\) A few things to keep note of: \\(r_{CE,\\, sat}\\) is a physical resistor, unlike \\(r_o\\) , with a small value of a few ohms to a few tens of ohms. In comparison to the output resistance in active mode: \\(r_{CE,\\, sat} \\ll r_o\\) . Increasing \\(v_{BE}\\) , increases the slope of \\(i_C-v_{CE}\\) relation and \\(r_{CE,\\, sat}\\) drops. The resistance, \\(r_{CE,\\, sat}\\) , is due to the resistance of the emitter \\(R_E\\) and that of the collector \\(R_c\\) , which can be modelled as: ::: center ::: -4ex -1ex -.4ex 1ex .2ex Design of Amplifiers Previously, we discussed the large-signal equivalent circuit, so now we will go over the small-signal equivalent. When passing a small-signal, some components function differently, as shown below: Linear resistor: The small-signal equivalent circuit of a linear resistor is the resistor itself. ::: center ::: Linear capacitor: The small-signal equivalent circuit of a linear capacitor is the capacitor itself. ::: center ::: Independent voltage source: The small-signal equivalent circuit of an independent voltage source is a short-circuit. ::: center ::: Independent current source: The small-signal equivalent circuit of an independent current source is an open-circuit. ::: center ::: Controlled sources: The small-signal equivalent circuit of a controlled source is the same controlled source. ::: center ::: Referring back to \\(i_C-v_{BE}\\) relation, the large slope gives rise to a large transconductance. ::: multicols 2 ::: flushleft ::: The transconductance \\(g_m\\) at DC operating point \\( \\(g_m = \\bigg[\\frac{i_C}{dv_{BE}}\\bigg]_{DC} \\approx \\frac{d}{dv_{BE}}\\bigg[I_Se^{v_{BE}/V_t}\\bigg] = \\frac{I_C}{V_t}\\) \\) where \\(I_C = I_Se^{v_{bias}/V_t}\\) denotes the DC current. ::: As a follow up, BJT needs to have a large \\(g_m\\) , which is one reason we do not operate in saturation region due to the small \\(g_m\\) . ::: center ::: If we compare \\(\\Delta i_C\\) in active and saturation mode, you can see that \\(\\Delta i_{C,\\, active} \\gg i_{C,\\, sat}\\) . Referring back to the equation for transconductance \\(g_m\\) , we can approximate it to be equivalently the formula for the slope: \\( \\(g_m = \\frac{di_C}{dv_{BE}} \\approx \\frac{\\Delta i_C}{\\Delta v_{BE}}\\) \\) Since in both cases, \\(\\Delta v_{BE} = v_{BE2} - v_{BE1}\\) is the same, with the only difference being \\(\\Delta i_C\\) , then \\(g_{m,\\, active} \\gg g_{m,\\, sat}\\) This all relates back to how voltage is amplified or rather how a large voltage gain is obtained. ::: center ::: In the first curve, the \\(i_C-v_{BE}\\) relation, a small \\(v_{in}\\) is mapped to a large \\(i_C\\) via \\(g_m\\) : \\(i_C = g_mv_{in}\\) Notice how our amplified output is current, so we need to map it back to voltage, which is the second curve, the \\(i_C-v_{CE}\\) relation. The resultant \\(i_C\\) is then mapped to a large \\(v_{ce}\\) via \\(r_o\\) where \\(g_o = 1/r_o\\) : \\(v_{ce} = r_oi_C\\) As a result, voltage amplification is achieved. -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Equivalent Circuit Refer to the lecture slides for the in-depth derivation. The collector current can be expressed as \\( \\(i_C = g_ov_{ce} + g_mv_{be}\\) \\) where \\(i_C\\) , \\(v_{ce}\\) , and \\(v_{be}\\) are small-signal (AC) quantities denoted by the lower-case. Another version is using the relationship \\(\\beta i_b = i_C\\) , the equation becomes \\( \\(i_C \\approx \\beta i_b + g_ov_{ce}\\) \\) Thus, the small-signal equivalent model of the BJT in active mode using a voltage-controlled current source ::: center ::: and the other using a current-controlled current source ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Parameters The base-emitter resistance \\(r_{be}\\) , equivalently \\(1/g_{be}\\) , can be expressed as \\( \\(r_{be} = \\bigg[\\frac{dv_{BE}}{di_B}\\bigg]_{DC} = \\beta\\frac{V_t}{I_C} = \\beta\\frac{1}{g_m} = \\frac{V_t}{I_B}\\) \\) A few things to note, \\(r_{be}\\) consist of both emitter \\(r_e\\) and base resistance \\(r_b\\) \\( \\(r_{be} = r_b + r_e\\) \\) But, since the emitter is heavily doped, \\(r_b \\gg r_e\\) , which means the base-emitter is dominated by the base resistance, \\(r_{be} \\approx r_b\\) . The emitter resistance \\(r_e\\) can be expressed as \\( \\(r_e = \\bigg[\\frac{dv_{BE}}{di_E}\\bigg] = \\frac{1}{\\beta + 1}\\bigg[\\frac{dv_{BE}}{di_B}\\bigg]_{DC} = \\frac{r_{be}}{\\beta + 1}\\) \\) Each terminal have different characteristics shown below: ::: center ::: We are basically expressing the resistance looking into the circuit. It maybe clearer to see what the purpose of determining these equations are, by using the following diagram instead ::: center ::: As a recap, there are four small-signal parameters associated with BJT: ::: center ::: Transconductance \\(\\displaystyle g_m = \\bigg[\\frac{di_C}{dv_{BE}}\\bigg]_{DC} = \\frac{I_C}{V_t}\\) Output conductance \\(\\displaystyle g_o = \\bigg[\\frac{di_C}{dv_{CE}}\\bigg]_{DC} = \\frac{I_C}{V_A}\\) Base-emitter resistance \\(\\displaystyle r_{be} = \\bigg[\\frac{dv_{BE}}{di_B}\\bigg]_{DC} = \\frac{V_t}{I_B}\\) Transistor parameter \\(\\displaystyle \\beta = \\frac{I_C}{I_B}\\) ::: list The transistor parameter \\(\\beta\\) is typically given, which usually has a value of \\(100\\) . The other three parameters are what varies in ways due to the DC operating point. :::","title":"Bipolar Junction Transistors"},{"location":"W2022/ELE404/ELE404/#bjt-voltage-amplifiers","text":"-4ex -1ex -.4ex 1ex .2ex Load Line and Maximum Signal Swing Recall that a transistor is optimal as an amplifier when operating in the active region. The load line is a line drawn on the characteristics curve. ::: center ::: The load line can be expressed as \\( \\(i_C = \\frac{V_{CC} - v_{CE}}{R_c}\\) \\) The intersection of the load line and \\(i_C-v_{CE}\\) curve with \\(v_{BE} = V_{bias}\\) gives us the DC operating point (or the bias point). Once the operating point is established you essentially know how to bias the transistor optimallyeither by varying \\(V_{bias}\\) or \\(R_c\\) . ::: list Note that it is normally done by varying \\(V_{bias}\\) in the circuit, as varying \\(R_c\\) affects gain, which will get to in a bit. ::: Two important considerations in deciding the location of the DC operating point are the gain and allowable signal. In deciding the value for \\(V_{bias}\\) , it is useful to refer to the \\(i_C-v_{CE}\\) curve: ::: center ::: If DC operating point is too close to the boundary of active and saturation regions, the max signal swing will be smaller (shown on the left). If DC operating point is too close to \\(V_{CC}\\) , the max signal swing will be smaller (shown on the right). From the DC operating point, we can determine the lower and upper bounds of \\(v_o\\) signal swing coming from \\(v_{in}\\) in the circuit: The lower bound can be expressed as \\(v_{o,\\,total,\\,min} = V_{sat}\\) The upper bound can be expressed as \\(v_{o,\\,total,\\,max} = V_{CC}\\) ::: center ::: From the two equations, we can determine the max voltage swing, which is \\( \\(\\label{eq:maxvoltswing} v_{o,\\,AC,\\,max} = \\frac{V_{CC} - V_{sat}}{2}\\) \\) There are three basic configurations for connecting BJT as an amplifier: Common-Emitter (CE) Amplifier Common-Base (CB) Amplifier Common-Collector (CC) Amplifier Each has distinctly different attributes and hence areas of application, which will cover in each section. -4ex -1ex -.4ex 1ex .2ex Common-Emitter (CE) Amplifier The common-emitter amplifier is the most widely use of the three. The common-emitter amplifier circuit is shown below: ::: center ::: The purpose of each component are explained below: The signal source represents the Thevenin equivalent of the signal generator with an internal resistance \\(R_s\\) . The two isolating capacitors \\(C_1\\) and \\(C_2\\) are used to separate the AC signals from the DC biasing voltageby passing AC signals and blocking any DC component. It uses two resistors \\(R_1\\) and \\(R_2\\) to generate DC biasing voltage \\(V_B\\) at the basereferred to as voltage divider biasing. Suppose \\(R_c\\) is known, we can express the optimal \\(V_B\\) that yields the max output voltage swing shown in which is \\( \\(V_B = V_t\\ln{\\bigg[\\frac{V_{CC}-V_{sat}}{2I_SR_c}\\bigg]}\\) \\) Since \\(V_B\\) is generated by a DC biasing voltage, we'll rewrite \\(V_B\\) in terms of \\(R_1\\) and \\(R_2\\) . In order to derive this equation, we need to consider two possible cases: Case 1: If \\(R_1\\) and \\(R_2\\) small, \\(I_B\\) can ignored since \\(I_{R1},I_{R2} \\gg I_B\\) . Therefore, we can apply voltage division to determine \\(V_B\\) , such that \\( \\(V_B = \\frac{R_2}{R_1 + R_2}V_{CC}\\) \\) One downside to this is that it will consume more (DC) power. Case 2: If \\(R_1\\) and \\(R_2\\) are large, \\(I_B\\) can no longer be ignored since \\(I_{R1},I_{R2}\\) is now comparable to \\(I_B\\) , where the current splits from \\(I_B\\) and \\(I_{R2}\\) . Performing KCL at node B yields \\( \\(V_B = \\frac{G_1V_{CC} - (I_C/\\beta)}{G_1 + G_2}\\) \\) Note that \\(G_1 = 1/R_1\\) and \\(G_2 = 1/R_2\\) and \\(I_C = \\beta I_B\\) . Since we always prefer to reduce power consumption, we make \\(R_1\\) and \\(R_2\\) large. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CE Amplifier By replacing the BJT with an equivalent circuit shown below, we can use this circuit to determine the characteristics parameters of the amplifier \\(R_{in}\\) , \\(R_{out}\\) , and \\(A_{v}\\) as follows. In small-signal analysis, the capacitors \\(C_1\\) , \\(C_2\\) are replaced by effective shorts and the DC source \\(V_{CC}\\) is replaced by a ground. ::: center ::: ::: list The circuit above is shown without a voltage-divider bias, \\(R_1\\) and \\(R_2\\) , instead uses \\(V_b\\) . ::: The input resistance \\(R_{in}\\) is the resistance \\\"seen\\\" by the AC source connected to the input. ::: multicols 2 ::: flushleft ::: The input resistance is expressed by the following formula: \\( \\(R_{in} = r_{be}\\) \\) and with DC biasing, the equation can be expressed as: \\( \\(R_{in} = R_1 \\parallel R_2 \\parallel r_{be}\\) \\) ::: The output resistance \\(R_{out}\\) is the resistance looking in at the collector. ::: multicols 2 ::: flushleft ::: The output resistance is expressed by the following formula: \\( \\(R_{out} = R_c \\parallel r_o\\) \\) where \\(r_o\\) is intended to quantify the impact of base-width modulation. ::: The voltage gain \\(A_v\\) is the ratio of the output voltage \\(v_o\\) at the collector to the input voltage \\(v_{in}\\) . \\( \\(A_v = \\frac{v_o}{v_{in}}\\) \\) Since \\(v_{in} = v_{be}\\) and by performing KCL at output node (refer to the diagram used for \\(R_{out}\\) ) \\( \\(\\frac{v_o}{R_c} + \\frac{v_o}{r_o} + g_mv_{be} = \\bigg(\\frac{1}{R_c}+\\frac{1}{r_o}\\bigg)v_o + g_mv_{in} = 0 \\quad \\Longleftrightarrow \\quad \\frac{v_o}{v_{in}} = -g_m\\bigg(\\frac{R_cr_o}{R_c + r_o}\\bigg)\\) \\) The voltage gain is expressed by the following formula: \\( \\(\\label{eq:ce_voltgain} A_v = -g_mR_{out} = -g_m(R_c \\parallel r_o)\\) \\) ::: list Notice how \\(A_v\\) is negative, which indicates a phase inversion from input to output. In other words, CE amplifier is an inverting amplifier. ::: One issue with the design is when we try to fabricate this on-chip, resistors take up a ton of space and becomes quite expensive, which brings up the next point. So an alternative is by replacing the resistor \\(R_c\\) with an active load as shown below. ::: center ::: The purpose of \\(Q_2\\) is to behave like a resistor with no amplification: It can be achieved by passing through a constant voltage \\(V_{b2}\\) at the base. Since \\(V_{b2}\\) is constant, the current of \\(Q_2\\) is constant and therefore, it can be represented by a constant source \\(I_{CC2}\\) with an output resistance \\(r_{o2}\\) . If we can use \\(r_{o2}\\) to fabricate \\(R_c\\) , then a large \\(A_v\\) is obtained without using a large resistor. ::: list Keep in mind, \\(Q_1\\) is an NPN transistor, so \\(Q_2\\) is a PNP transistor for \\(R_{out}\\) to be large. ::: The characteristics parameters are the same as before, with the only difference changing \\(R_c\\) to \\(r_{o2}\\) in the output resistance: Input resistance: \\(R_{in} = r_{be}\\) Output resistance: \\(R_{out} = r_{o1} \\parallel r_{o2}\\) Voltage gain: \\(A_v = -g_mR_{out} = -g_m(r_{o1}\\parallel r_{o2})\\) -2ex -0.1ex -.2ex .2ex .2ex Summary The CE amplifier gives high input resistance (draws little current), moderately high output resistance (easier to match for maximum power transfer), and high voltage gain, \\(A_{v}\\) (a desirable feature of an amplifier). It is an inverting amplifier, by increasing \\(g_m\\) or \\(R_{out}\\) , increases the voltage gain \\(A_v\\) , but it comes at a cost: Increasing \\(g_m\\) results in more power consumption. It is costly to fabricate a large resistor on chip. By replacing the resistor with an active load, one can increase the voltage gain without using a large resistor, with an extra cost of providing an additional biasing voltage. -3ex -0.1ex -.4ex 0.5ex .2ex Emitter Degeneration When we include a resistance \\(R_E\\) in the emitter as shown below, it can lead to significant changes in the amplifier characteristics. ::: center ::: The emitter degeneration forms a negative feedback mechanism that stabilizes \\(I_C\\) . In other words, it stabilizes the DC operating point, accounting for the fluctuations which can change the value of \\(V_B\\) . Without \\(R_E\\) , let's say \\(V_{b1}\\) increases, then \\(V_{BE1}\\) increases as well, since \\( \\(V_{b1} = V_{BE1}\\) \\) As a result, \\(I_C\\) increases, changing the DC operating point and affecting \\(v_o\\) . \\( \\(I_C = I_Se^{V_{BE1}/V_t}\\) \\) When \\(R_E\\) is present, \\(V_{BE1}\\) will be smaller subsequently, since \\( \\(V_{b1} = V_{BE1} + R_EI_E \\qquad \\Longleftrightarrow \\qquad V_{BE1} = V_{b1} - R_EI_E\\) \\) Then \\(I_C\\) decreases, making it less susceptible to fluctuations. ::: center ::: With the addition of \\(R_E\\) , the characteristics parameters changed. ::: center ::: The input resistance is: \\( \\(R_{in} \\approx (1 + g_{m1}R_E)r_{be1}\\) \\) and with DC biasing: \\( \\(\\label{eq:ce_rindc} R_{in} \\approx R_1 \\parallel R_2 \\parallel (1 + g_{m1}R_E)r_{be1}\\) \\) The output resistance is: \\( \\(R_{out} = (r_{o1} \\parallel r_{o2})(1 + g_{m1}R_E)\\) \\) The voltage gain is: \\( \\(\\label{eq:ce_evoltgain} A_v = \\frac{-g_{m1}(r_{o1} \\parallel r_{o2})}{1 + g_{m1}R_E}\\) \\) The emitter degeneration reduces the power consumption, but comes at a cost, which lowers the voltage gain by \\(1 + g_{m1}R_E\\) in the denominator. A way to preserve the voltage gain is by adding a shunt capacitor \\(C_E\\) in parallel to \\(R_E\\) . ::: center ::: If you recall from 202, the impedance of a capacitor is \\( \\(z_{C_E} = \\frac{1}{j\\omega C_E}\\) \\) where \\(\\omega\\) is the input frequency: In DC, emitter degeneration is active because \\(C_E\\) behaves as an open-circuit ( \\(z_{C_E} = \\infty\\) ), since \\(\\omega = 0\\) . In AC, emitter degeneration is inactive at \\(\\omega\\) because \\(C_E\\) behaves as a short-circuit ( \\(z_{C_E} = 0\\) ), since \\(C_E\\) designed as such to be sufficiently large. As a result, the voltage gain is no longer affected by emitter degeneration, such that \\( \\(A_v \\approx g_m(R_c \\parallel r_o)\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Summary With emitter degeneration, it posses four unique characteristics: Increased input resistance by a factor of \\(1 + g_{m1}R_E\\) Reduced voltage gain by a factor of \\(1 + g_{m1}R_E\\) Increased output resistance Reduces power consumption To preserve voltage gain at the frequency of the input, shunt capacitor \\(C_E\\) that behaves as a short circuit at input frequency is added. ::: center ::: -4ex -1ex -.4ex 1ex .2ex Common-Base (CB) Amplifier The common-base amplifier circuit is shown below: ::: center ::: The input is now located at the emitter, while the output is still at the collector. The components still perform the same purpose as described in CE amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CB Amplifier In small-signal analysis, the following circuit is obtained. ::: center ::: The input resistance is: \\( \\(R_{in} \\approx \\frac{r_{be}}{\\beta + 1} = r_e\\) \\) and with DC biasing: \\( \\(R_{in} \\approx R_1 \\parallel R_2 \\parallel r_e\\) \\) The output resistance is the same as that of CE amplifier: \\( \\(R_{out} = R_c \\parallel r_o\\) \\) The voltage gain is: \\( \\(A_v \\approx g_{m1}(R_c \\parallel r_{o})\\) \\) ::: list Since \\(A_v\\) is positive, CB amplifier is a non-inverting amplifier, unlike CE amplifier which is an inverting amplifier. ::: One thing to add, we can also replace \\(R_c\\) with an active load, like we did with CE amplifier. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Summary The CB amplifier has a low input resistance. This is undesirable as it will draw a large current when driven by a voltage input. Refer to the next section. It is a non-inverting amplifier, with the same voltage gain as that of CE amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Characterizing Amplifiers In general, CB amplifiers should not be used as a voltage amplifiers due to its low input resistance, \\(R_{in} = r_e\\) . To understand why, refer to the characterization of an amplifier below. ::: center ::: For amplifiers, a large input resistance is desirable as it reduces the loading effect of the amplifier on the signal source. ::: center ::: The larger the input resistance \\(R_{in}\\) , the more the input signal \\(v_s\\) will appear at the input of the amplifier. \\( \\(\\label{eq:input_volt} v_{in} = \\frac{R_{in}}{R_s + R_{in}}v_s \\approx \\bigg(1 - \\frac{R_s}{R_{in}}\\bigg)v_s\\) \\) If you recall, CE amplifiers have a large input resistance, thus making it a desirable voltage amplifier. \\( \\(\\frac{R_s}{R_{in}} \\approx 0 \\qquad \\Longrightarrow \\qquad v_{in} \\approx v_s\\) \\) However, CB amplifiers have a small input resistance, since \\(r_e = r_{be}/(\\beta + 1)\\) , where \\(\\beta\\) is around \\(100\\) , resulting to a huge voltage loss from \\(v_s\\) to \\(v_{in}\\) . Suppose we use a Norton equivalent source, instead of a Thevenin equivalent source. ::: center ::: Now in this scenario, the signal of the source is current. \\(R_N\\) is designed to be very large, so all the current will be delivered to the load, oppose to \\(R_s\\) in Thevenin equivalent. \\( \\(i_{in} = \\frac{R_N}{R_{in} + R_N}i_N \\approx \\bigg(1 - \\frac{R_{in}}{R_N}\\bigg)i_N\\) \\) So if \\(R_N\\) is large enough or \\(R_{in}\\) is very small, then all the current will be delivered to the load. \\( \\(\\frac{R_{in}}{R_N} \\approx 0 \\qquad \\Longrightarrow \\qquad i_{in} \\approx i_N\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Additional Remarks When designing the circuit, we might not have a current generator, so how can we apply them? ::: center ::: By placing a common-emitter, the transistor \\(Q_1\\) maps \\(v_{in}\\) to \\(I_{C1}\\) and we know it has an output resistance \\(r_{o1}\\) due to base width modulationthe equivalent circuit is shown on the right. ::: list This isn't really covered in the scope of the course, just intended to demonstrate how it would look like. It is referred to as a cascode amplifier. ::: -4ex -1ex -.4ex 1ex .2ex Common-Collector (CC) Amplifier The common-collector amplifier is usually referred to as an emitter follower. The common-collector amplifier circuit is shown below: ::: center ::: Since the output voltage is at the emitter, it is in phase with the base voltage, so there is no inversion from input to output. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CC Amplifier In small-signal analysis, the following circuit is obtained. ::: center ::: The input resistance is the same as that of CE amplifier: \\( \\(R_{in} = r_{be}\\) \\) The output resistance is: \\( \\(R_{out} \\approx \\frac{1}{g_{m1}} = r_e\\) \\) The voltage gain is: \\( \\(A_v \\approx 1\\) \\) ::: list Because there is no inversion and \\(A_v \\approx 1\\) , the output voltage closely follows the input voltage in both phase and amplitudethus the term emitter-follower. ::: Suppose we replace \\(R_E\\) with an active load, as we have done with the previous two amplifiers. ::: center ::: An NPN-based source follower functions as a voltage-down shifter as \\(v_o = v_{in} - v_{BE1}\\) . It shifts the level down by \\(v_{BE1}\\) . An PNP-based source follower functions as a voltage-up shifter as \\(v_o = v_{in} + v_{EB1}\\) . It shifts the level up by \\(v_{EB1}\\) . ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Characterizing Amplifiers Due to its high input resistance and a low output resistance, it can be used a voltage buffer. To understand the use of this, let's look back at the diagram we used for CB amplifiers. ::: center ::: Previously, we covered how a high input resistance is more favorable. Now, we'll focus on the output resistance. ::: center ::: For amplifiers, a small output resistance is desirable, as it increases the output voltage \\(v_{out}\\) that will be delivered to the load resistor \\(R_L\\) . \\( \\(v_L = \\frac{R_L}{R_L + R_{out}}v_{out} \\approx \\bigg(1 - \\frac{R_{out}}{R_L}\\bigg)v_{out}\\) \\) Since CC amplifiers have a small output resistance, such that \\(R_{out} \\ll R_L\\) , then \\( \\(\\frac{R_{out}}{R_L} \\approx 0 \\qquad \\Longrightarrow \\qquad v_L = v_{out}\\) \\) Thus, comes down to the reason we use an emitter follower. As an example, we have a CE amplifier with a load resistor attached at the output. ::: center ::: When a load resistor is present, the voltage gain in is now: \\( \\(A_{v,\\ \\text{w/ load}} = -g_{m1}(R_{out} \\parallel R_L) = -g_{m1}(r_{o1} \\parallel r_{o2} \\parallel R_L)\\) \\) Depending on \\(R_L\\) : Case 1: If \\(R_L \\gg R_{out}\\) , then \\(A_{v,\\ \\text{w/ load}} = A_{v,\\ \\text{w/o load}}\\) , where \\(R_L\\) will have no impact on the voltage gain. Case 2: If \\(R_L \\ll R_{out}\\) , then \\(A_{v,\\ \\text{w/ load}} \\ll A_{v,\\ \\text{w/o load}}\\) , where \\(R_L\\) will have a severe loading impact on voltage gain. A solution to Case 2, if we add an emitter follower stage to lower the output resistance, then \\(R_L\\) will have no impact on the voltage gain. ::: center ::: If you recall an emitter follower has a voltage gain of \\(A_v \\approx 1\\) , which is good, since it does not alter or change the voltage gain (More on this in the next section). By applying an emitter follower, we can make \\(R_{out}\\) even smaller, such that \\(R_L \\gg R_{out}\\) . -4ex -1ex -.4ex 1ex .2ex Multi-stage Amplifiers [Two or more (CE or CB) amplifiers can be connected in a cascaded arrangement with the output of one amplifier driving the input of the next.]{#bjt_multistage} ::: center ::: Isolation capacitors exists at the input, output, and in between stages to: Isolate the amplifier from both input source and output load DC bias each stage individually The overall voltage gain of cascaded amplifiers is the product of the individual voltage gains. \\( \\(A_v = A_{v1}A_{v2}A_{v3} \\cdots A_{vn}\\) \\) where \\(n\\) is the number of stages. ::: center ::: Each triangular symbol represents a separate amplifier. As an example, we have a two-stage BJT amplifier below: ::: center ::: The input voltage \\(v_1\\) can be obtained using voltage division (refer to and ). \\( \\(v_1 = \\frac{R_{in1}}{R_{in1}+R_s}v_s\\) \\) where \\(R_{in1} = R_1 \\parallel R_2 \\parallel (1 + g_{m1}R_{E1})r_{be1}\\) The voltage gain of stage 1, \\(A_{v1}\\) , is equivalently the voltage gain of a CE amplifier with emitter degeneration (refer to ). The load resistance is \\(R_{c1} \\parallel R_{in2}\\) . \\( \\(A_{v1} = \\frac{-g_{m1}(R_{c1} \\parallel R_{in2})}{1 + g_{m1}R_{E1}}\\) \\) where \\(R_{in2} = R_3 \\parallel R_4 \\parallel (1 + g_{m2}R_{E2})r_{be2}\\) Likewise, the same goes for voltage gain of stage 2, \\(A_{v2}\\) . The load resistance is \\(R_{c2}\\) . \\( \\(A_{v2} = \\frac{-g_{m2}R_{c2}}{1 + g_{m2}R_{E2}}\\) \\) Combining all three, we can determine the overall voltage gain: \\( \\(A_v = \\bigg[\\frac{R_{in1}}{R_{in1}+R_s}v_s\\bigg] \\times \\bigg[\\frac{-g_{m1}(R_{c1} \\parallel R_{in2})}{1 + g_{m1}R_{E1}}\\bigg] \\times \\bigg[\\frac{-g_{m2}R_{c2}}{1 + g_{m2}R_{E2}}\\bigg]\\) \\) ::: list For load resistance, we didn't include \\(r_{o1}\\) or \\(r_{o2}\\) because a large resistor ( \\(r_{o1},r_{o2}\\) is \\(\\SI{10}{\\kilo\\ohm}\\) to \\(\\SI{100}{\\kilo\\ohm}\\) ) in parallel with a small resistor has the resistance of the smaller one roughly./ ::: -4ex -1ex -.4ex 1ex .2ex Current Mirrors A current mirror is a circuit designed to copy a current through one active device by controlling the current in another active device of a circuit, keeping the output current constant regardless of loading. -3ex -0.1ex -.4ex 0.5ex .2ex Diode-Connected BJT It uses a diode-connected BJT, where the base and collector are tied together. ::: center ::: The reason we call it a diode-connected BJT is because if you remember we have a diode \\(D_1\\) between base and emitter then a diode \\(D_2\\) between base and collector. ::: center ::: When a wire is connected between the base and collector, the diode \\(D_2\\) is short-circuited. As a result, the entire circuit functions like a diode \\(D_1\\) . ::: center ::: Looking into the circuit, the input resistance is simply the resistance of the diode \\(D_1\\) , which is: \\( \\(R_{in} \\approx \\frac{1}{g_m} \\approx r_{e}\\) \\) And so a diode-connected BJT synthesizes a resistor of low resistance \\(r_e\\) . Equivalently, we constructed a resistor without physically adding one to the circuit. -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror without Base-Width Modulation Let's now look at a BJT current mirror. Refer to the circuit below. ::: center ::: For the sake of simplicity, let's first ignore base-width modulation. In other words, we ignore \\(r_o\\) . If you recall from , the collector current \\(I_{C1}\\) and \\(I_{C2}\\) can be expressed as \\( \\(i_{C1} \\approx I_{s1}e^{v_{BE}/V_t}\\) \\) \\( \\(i_{C2} \\approx I_{s2}e^{v_{BE}/V_t}\\) \\) and by dividing both equations we have \\( \\(\\frac{i_{C2}}{i_{C1}} = \\frac{I_{s2}}{I_{s1}}\\) \\) When \\(Q_1\\) and \\(Q_2\\) are identical, we have that \\(I_{s2} = I_{s1}\\) , thus \\( \\(i_{C2} = i_{C1}\\) \\) which defines it to be a current mirror. If we further make further simplifications, which we also ignore \\(i_B\\) , since the base current is very small, such that \\(i_{B1}, i_{B2} \\ll i_{C_1}\\) , the current source \\( \\(J = i_{C1} + i_{B1} + i_{B2}\\) \\) can be expressed as \\(J \\approx i_{C1}\\) or \\( \\(i_{C2} \\approx \\frac{I_{s2}}{I_{s1}}J\\) \\) Though for a more accurate analysis where base currents \\(i_{B1}\\) and \\(i_{B2}\\) are accounted for while \\(r_o\\) is not accounted for, we have \\( \\(i_{C2} = \\frac{1}{1 + (2/\\beta)}J \\approx \\bigg(1 - \\frac{2}{\\beta}\\bigg)J\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror with Base-Width Modulation Let's now consider the impact of base-width modulation \\(r_o\\) . When accounted for, the circuit can be depicted as ::: center ::: From , the collector current \\(I_{C1}\\) and \\(I_{C2}\\) can be expressed as \\( \\(i_{C1} \\approx I_{s1}e^{v_{BE}/V_t}\\bigg(1 + \\frac{v_{CE1}}{V_A}\\bigg)\\) \\) \\( \\(i_{C2} \\approx I_{s2}e^{v_{BE}/V_t}\\bigg(1 + \\frac{v_{CE2}}{V_A}\\bigg)\\) \\) and by dividing both equations we have \\( \\(\\frac{i_{C2}}{i_{C1}} = \\frac{I_{s2}}{I_{s1}}\\frac{1 + (v_{CE2}/V_A)}{1 + (v_{CE1}/V_A)}\\) \\) However, \\(v_{CE1}\\) is not the same as \\(v_{CE2}\\) or in other words, \\(v_{CE1} \\neq v_{CE2}\\) because they are located in two different branches, so there's no way we can set them to the same voltage. If we let \\(v_{CE1} = v_{CE}\\) and \\(v_{CE2} = v_{CE} + \\Delta v_{CE}\\) , then we can express the difference in voltage between \\(v_{CE1}\\) and \\(v_{CE2}\\) by \\(\\Delta v_{CE}\\) , thus \\( \\(\\frac{i_{C2}}{i_{C1}} = \\frac{I_{s2}}{I_{s1}}\\underbrace{\\bigg(1 + \\frac{\\Delta v_{CE}/V_A}{1 + (v_{CE}/V_A)}\\bigg)}_{\\neq 1}\\) \\) Since \\(v_{CE1} \\neq v_{CE2}\\) in general, the impact of \\(v_{CE}\\) mismatch cannot be neglected for precision current mirrors. -3ex -0.1ex -.4ex 0.5ex .2ex Current Sinks and Current Sources The purpose of current mirrors is to construct current sources and current sinks from a master current source whose value is stableindependent of temperature and supply voltage fluctuations. ::: center ::: The current sources and current sinks allows us to very easily generate DC biasing current across a circuit. The currents are set by the ratio of transistor size, specifically the area of the emitter. For example, we have a two-stage amplifier consists consists of two CE amplifiers formed by \\(Q_1\\) and \\(Q_2\\) . The circuit is denoted in blue. ::: center ::: Ideally we're suppose to use a master current source, as the one we have does not provide a constant current source. ::: multicols 2 ::: flushleft ::: In reality it's not even actual current source, but two resistors using basic voltage division, such that \\( \\(V_A = \\frac{r_{e4}}{r_{e4} + r_{e3}}V_{CC}\\) \\) If \\(Q_3\\) and \\(Q_4\\) have the same emitter dimensions, then \\(r_{e4} = r_{e3}\\) \\( \\(V_A = \\frac{V_{CC}}{2}\\) \\) ::: Since \\(v_{BE4} = V_A\\) , we can express the collector current \\(i_{C4}\\) as \\( \\(i_{C4} = I_{s4}e^{V_{A}/V_t}\\) \\) Once \\(V_{A}\\) is known, we can determine collector currents of the current sources and sinks. For example, \\(i_{C5}\\) is \\( \\(i_{C5} = \\frac{I_{s5}}{I_{s4}}i_{C4}\\) \\) If \\(Q_4\\) and \\(Q_5\\) have the same emitter dimension, then \\(I_{s5} = I_{s4}\\) and \\(i_{C5} = i_{C4}\\) . We can perform the same analysis for \\(i_{C6}\\) to \\(i_{C8}\\) .","title":"BJT Voltage Amplifiers"},{"location":"W2022/ELE404/ELE404/#mosfets","text":"Most of the stuff covered should be a recap from PC224, though the format of some formulas might looks different, but nonetheless are the same. -4ex -1ex -.4ex 1ex .2ex Structure of MOSFET Similar to BJTs, there are two types of Metal-Oxide Field-Effect transistors (MOSFETs) namely: NMOS transistor ::: center ::: PMOS transistor ::: center ::: It's a four-terminal symmetrical device: the source terminal (S), the drain terminal (D), the gate terminal (G), and the substrate or bulk terminal (B). Source: Heavily doped n-typed semiconductor for NMOS and heavily doped p-typed silicon for PMOS Drain: Heavily doped n-typed semiconductor for NMOS and heavily doped p-typed silicon for PMOS Gate: Polysilicon compound (highly conductive and yet more stable than metal) Substrate: Lightly doped p-type for NMOS and lightly doped p-type silicon for PMOS Note how the following references are denoted by their subscripts: \\(v_{GS}\\) is the gate to source voltage, otherwise known as \\(v_{GS} = v_G - v_S\\) . \\(v_{SG}\\) is the source to gate voltage, otherwise known as \\(v_{SG} = v_S - v_G\\) . \\(i_{DS}\\) is the current flowing from drain to source. \\(V_T\\) is the threshold voltagedefines the boundary of MOSFET being ON or OFF: Often the source terminal is connected to the ground, so they remove it from the subscript, such as \\(i_D\\) , but should still be the same thing as \\(i_{DS}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Dimensions of MOSFET Refer to the diagram below: ::: center ::: The dimensions of MOSFET are defined as such: \\(L_d\\) is the drawn length \\(L\\) or \\(L_{\\text{eff}}\\) is the effective length \\(W\\) is the width \\(t_{ox}\\) is the gate oxide thickness The dimensions of source and drain are dictated by the number of contacts A capacitor formed by the gate (top plate) and negative ions (bottom plate) is created \\( \\(C_g = C_{ox}(WL)\\) \\) and \\(C_{ox}\\) is the gate capacitance per area and is given by \\( \\(C_{ox} = \\frac{\\epsilon_{ox}}{t_{ox}}\\) \\) where \\(\\epsilon_{ox} = \\SI{3.45e-11}{\\farad\\per\\meter}\\) is the permittivity of oxide. -4ex -1ex -.4ex 1ex .2ex Operation of MOSFET There are three mode of MOSFET operation: cut-off mode, triode mode, and saturation modeeach mode have different applications. To get a better understanding of the different modes, we will be referencing the \\(i_{D}-v_{DS}\\) characteristics of the MOSFET, which looks similar to that of the \\(i_C-v_{CE}\\) characteristics of a BJT. ::: center ::: Note that the graph shown is for NMOS. For PMOS, you would have to reverse all the formulas, so instead of \\(v_{DS}\\) it would be \\(v_{SD}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Cut-Off Mode When \\(v_{GS} < V_T\\) , the MOSFET is OFF and no channel is induced. ::: center ::: The MOSFET will be in cut-off mode, where no current flows, such that \\( \\(i_{DS} = 0\\) \\) In cut-off, the MOSFET functions as an open switch with no current flow. -3ex -0.1ex -.4ex 0.5ex .2ex Triode Mode When \\(v_{GS} > V_T\\) , the MOSFET is ON and a channel will be induced where current starts flowing if \\(v_{DS} > 0\\) . ::: center ::: The triode mode and saturation mode is bounded by the pinch-off condition, \\(v_{GS} - V_T\\) . The MOSFET will operate in the triode mode as long as \\( \\(v_{DS} < v_{GS} - V_T\\) \\) For NMOS, the channel current is given by \\( \\(i_{DS,\\,\\text{triode}} = \\mu_nC_{ox}\\frac{W}{L}\\bigg[(v_{GS} - V_T)v_{DS} - \\frac{1}{2}v_{DS}^2 \\bigg]\\) \\) For PMOS, the channel current is given by \\( \\(i_{SD,\\,\\text{triode}} = \\mu_pC_{ox}\\frac{W}{L}\\bigg[(v_{SG} - |V_{Tp}|)v_{SD} - \\frac{1}{2}v_{SD}^2 \\bigg]\\) \\) If we look back at the \\(i_{D}-v_{DS}\\) characteristics of the MOSFET in the triode mode, you'll notice it behaves like a linear resistor when a small \\(v_{DS}\\) is applied ::: center ::: such that we can represent the channel conductance as \\( \\(g_{DS} = \\bigg[\\frac{di_{DS}}{dv_{DS}}\\bigg]_{DC} \\approx \\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)\\) \\) In triode mode, the MOSFET functions as a voltage-controlled resistor with conductance \\(g_{DS}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Saturation Mode When \\(v_{GS} > V_T\\) and \\(v_{DS} = v_{GS} - V_T\\) , the MOSFET is ON and a channel will pinch-off at the drain. ::: center ::: For NMOS, the channel current at pinch-off is given by \\( \\(i_{DS,\\,\\text{pinch-off}} = \\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)\\) \\) For PMOS, the channel current at pinch-off is given by \\( \\(i_{SD,\\,\\text{pinch-off}} = \\mu_pC_{ox}\\frac{W}{L}(v_{SG} - |V_{Tp}|)\\) \\) At pinch-off, the channel current \\(i_{DS}\\) becomes independent of \\(v_{DS}\\) , where \\( \\(g_o = \\frac{di_{DS}}{dv_{DS}} = 0\\) \\) It behaves as a voltage-controlled current source with transconductance ::: center ::: which can be defined as \\( \\(g_m = \\bigg[\\frac{di_{DS}}{dv_{GS}}\\bigg]_{DC} = \\frac{2i_{DS}}{v_{GS} - V_T}\\) \\) so to increase pinch-off, one can: increase the width \\(W\\) , size of the transistor, or Increase \\(v_{GS} - V_T\\) , otherwise referred to as over-drive voltage. When \\(v_{DS}\\) exceeds pinch-off, such that \\(v_{DS} > v_{GS} - V_T\\) , the MOSFET will operate in the saturation mode. ::: center ::: From the previous \\(i_{D}-v_{DS}\\) characteristics of the MOSFET, we assumed that as we increase \\(v_{DS}\\) , the channel current \\(i_{DS}\\) will remain constant. ::: center ::: If you recall for BJTs, we had something called the \\\"Early effect\\\", likewise the same thing can occur in MOSFET. ::: center ::: The output conductance \\(g_o\\) can redefined to be \\( \\(g_o = i_{DS,\\,\\text{pinch-off}}\\lambda\\) \\) For MOSFETs, we use \\(\\lambda = 1/V_A\\) , as it is most widely used, where \\(V_A\\) is Early voltage. For NMOS, the channel current at saturation is given by \\(i_{DS,\\,\\text{pinch-off}} + g_ov_{DS}\\) or \\( \\(i_{DS,\\,\\text{sat}} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)^2(1 + \\lambda v_{DS})\\) \\) For PMOS, the channel current at saturation is given by \\( \\(i_{SD,\\,\\text{sat}} = \\frac{1}{2}\\mu_pC_{ox}\\frac{W}{L}(v_{SG} - |V_{Tp}|)^2(1 + \\lambda v_{SD})\\) \\) In saturation mode, the MOSFET functions as a voltage-controlled current source with finite output resistance. -2ex -0.1ex -.2ex .2ex .2ex Summary For NMOS: \\( \\(i_{DS} = 0 \\begin{cases}v_{GS} < V_T\\end{cases}\\) \\) \\( \\(i_{DS,\\,\\text{triode}} = \\mu_nC_{ox}\\frac{W}{L}\\bigg[(v_{GS} - V_T)v_{DS} - \\frac{1}{2}v_{DS}^2 \\bigg] \\begin{cases}v_{GS} > V_T \\\\ v_{DS} < v_{GS} - V_T\\end{cases}\\) \\) \\( \\(\\label{eq:nmos_currentsat} i_{DS,\\,\\text{sat}} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(v_{GS} - V_T)^2(1 + \\lambda v_{DS}) \\begin{cases}v_{GS} > V_T \\\\ v_{DS} > v_{GS} - V_T\\end{cases}\\) \\) For PMOS: \\( \\(i_{SD} = 0 \\begin{cases}v_{SG} < V_T\\end{cases}\\) \\) \\( \\(i_{SD,\\,\\text{triode}} = \\mu_nC_{ox}\\frac{W}{L}\\bigg[(v_{SG} - |V_{Tp}|)v_{SD} - \\frac{1}{2}v_{SD}^2 \\bigg] \\begin{cases}v_{SG} > V_T \\\\ v_{SD} < v_{SG} - |V_{Tp}|\\end{cases}\\) \\) \\( \\(i_{SD,\\,\\text{sat}} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(v_{SG} - |V_{Tp}|)^2(1 + \\lambda v_{DS}) \\begin{cases}v_{SG} > V_T \\\\ v_{SD} > v_{SG} - |V_{Tp}|\\end{cases}\\) \\) -4ex -1ex -.4ex 1ex .2ex Design of Amplifiers The same idea kinda be applied for MOSFETs when doing small-signal analysis, as we have previously covered with BJTs. ::: center ::: The total gate to source voltage becomes \\( \\(v_{GS} = V_{bias} + v_{in}\\) \\) where \\(V_{bias}\\) ensures that the MOSFET operates in saturation, which gives rise to a large \\(g_m\\) . ::: center ::: In the first curve, the \\(i_{DS}-v_{GS}\\) relation, a small \\(v_{in}\\) is mapped to a large \\(i_{DS}\\) via \\(g_m\\) . Notice how our amplified output is current, so we need to map it back to voltage, which is the second curve, the \\(i_{DS}-v_{DS}\\) relation. The resultant \\(i_{DS}\\) is then mapped to a large \\(v_{DS}\\) via \\(r_o\\) where \\(g_o = 1/r_o\\) . As a result, voltage amplification is achieved. -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Equivalent Circuit The small-signal equivalent circuit is shown below. ::: center ::: The drain current can be expressed as combination of the DC and AC signal of the circuit \\( \\(i_{DS} = I_{DS} + i_{ds}\\) \\) For NMOS, the small-signal drain current can be expressed as \\( \\(i_{ds} \\approx g_mv_{gs} + g_ov_{ds}\\) \\) where \\( \\(\\label{eq:nmos_transconductance} g_m = \\frac{2I_{DS}}{V_{GS} - V_T} \\qquad g_o = \\lambda I_{DS}\\) \\) For PMOS, the small-signal drain current can be expressed as \\( \\(i_{sd} \\approx g_mv_{sg} + g_ov_{sd}\\) \\) where \\( \\(\\label{eq:pmos_transconductance} g_m = \\frac{2I_{SD}}{V_{SG} - |V_{Tp}|} \\qquad g_o = \\lambda I_{SD}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Voltage Gain With the output resistor \\(r_o\\) added, the voltage gain becomes \\( \\(A_v = \\frac{v_o}{v_{in}} = -g_m(R_D \\parallel r_o)\\) \\) and by increasing \\(I_{DS}\\) or \\(R_D\\) , boost the voltage gain. In the next module, we will go over more in-depth, covering the three basic configurations for connecting the MOSFET as an amplifier, similar to what we covered for BJTs.","title":"MOSFETs"},{"location":"W2022/ELE404/ELE404/#mosfet-voltage-amplifiers","text":"-4ex -1ex -.4ex 1ex .2ex Load Line and Maximum Signal Swing Similarly, in MOSFET voltage amplifiers, we need to pick the proper DC operating point given the \\(V_{bias}\\) in order to achieve maximum voltage swing allowed. ::: center ::: The load line can be expressed as: \\( \\(\\label{eq:mosfet_loadline} i_{DS} = \\frac{V_{DD} - v_{DS}}{R_D}\\) \\) The intersection of the load line and \\(i_{DS}-v_{DS}\\) curve with \\(v_{GS} = V_{bias}\\) gives us the DC operating point (or the bias point). Once the operating point is established you essentially know how to bias the transistor optimallyeither by varying \\(V_{bias}\\) or \\(R_D\\) . ::: list Note that it is normally done by varying \\(V_{bias}\\) in the circuit, as varying \\(R_D\\) affects gain, which will get to in a bit. ::: From the DC operating point, we can determine the lower and upper bounds of \\(v_o\\) signal swing coming from \\(v_{in}\\) in the circuit. ::: center ::: For a common-source amplifier with an NMOS: The lower bound can be expressed as \\(v_{o,\\,total,\\,min} = V_{sat}\\) The upper bound can be expressed as \\(v_{o,\\,total,\\,max} = V_{DD}\\) For a common-source amplifier with a PMOS: The lower bound can be expressed as \\(v_{o,\\,total,\\,min} = 0\\) The upper bound can be expressed as \\(v_{o,\\,total,\\,max} = V_{DD} - V_{sat}\\) From the upper and lower bound equations, we can derive a few equations. The max voltage swing \\( \\(v_{o,\\,AC,\\,max} = \\frac{V_{DD} - V_{sat}}{2}\\) \\) The optimal DC voltage at output \\( \\(v_{o,\\,DC} = \\frac{V_{DD} + V_{sat}}{2} = V_{DD} - R_DI_{DS}\\) \\) The optimal drain resistance by solving for \\(R_D\\) \\( \\(\\frac{V_{DD} + V_{sat}}{2} = V_{DD} + - R_D\\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}(V_{bias} - V_T)^2\\) \\) There are three basic configurations for connecting MOSFET as an amplifier: Common-Source (CS) Amplifier Common-Gate (CG) Amplifier Common-Drain (CD) Amplifier Each has distinctly different attributes and hence areas of application, which will cover in each section. -4ex -1ex -.4ex 1ex .2ex Common-Source (CS) Amplifier Of the three basic MOS amplifier configurations, the common source is the most widely used. The common-source amplifier circuit is shown below: ::: center ::: The purpose of each component are explained below: \\(R_s\\) is the internal resistance of the signal source \\(v_s\\) . \\(R_1\\) and \\(R_2\\) generate a DC biasing voltage \\(V_{DC}\\) at the gatereferred to as voltage divider biasing. \\( \\(V_{DC} = \\frac{R_2}{R_1 + R_2}V_{DD}\\) \\) \\(C_1\\) and \\(C_2\\) are isolating capacitors used to separate the AC signals from the DC biasing voltageby passing AC signals and blocking any DC component. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CS Amplifier The small-signal equivalent of the CS amplifier with resistor load is shown below: ::: center ::: Because there is no gate current flow, the input resistance \\(R_{in}\\) is: \\( \\(R_{in} = \\infty\\) \\) The output resistance is: \\( \\(R_{out} = R_D \\parallel r_o\\) \\) The voltage gain is: \\( \\(A_v = - g_mR_{out} = -g_m(R_D \\parallel r_o)\\) \\) The negative voltage gain indicates that the it is an inverting amplifierphase difference between \\(v_o\\) and \\(v_{in}\\) is \\(180^\\circ\\) . ::: center ::: To increase the voltage amplification, we can modify one of two components: \\(g_m\\) or \\(R_{out}\\) . Increasing \\(g_m\\) is not ideal, as a large \\(g_m\\) will increase either \\(W/L\\) or \\(V_{GS} - V_T\\) , which result in a large drain current \\(I_{DS}\\) where \\( \\(g_m\\uparrow = k_n\\frac{W}{L}\\uparrow(V_{GS} - V_T)\\uparrow \\quad\\Longrightarrow\\quad I_{DS} = \\frac{1}{2}\\mu_nC_{ox}\\frac{W}{L}\\uparrow(v_{GS} - V_T)^2\\uparrow\\) \\) thus, equivalently increase the power consumption \\( \\(P\\uparrow = V_{DD}I_{DS}\\uparrow\\) \\) From \\(R_{out}\\) , we can increase \\(R_D\\) , which is more ideal as it doesn't affect power consumption. However, increasing \\(R_D\\) affects the load line shown in . ::: center ::: The output can result in distortion if \\(R_D\\) is increases, as for an NMOS for example ::: center ::: thus, increasing \\(R_D\\) needs to be chosen carefully to maximize output voltage swing. In order to increase \\(R_D\\) in an effective manner, we can use a current-resistor load instead. The small-signal equivalent of the CS amplifier with current-source load is shown below: ::: center ::: The purpose of the transistor \\(M_2\\) is to behave like a resistor with no amplification: It can be achieve by passing through a constant voltage \\(V_{b2}\\) at the gate. Since \\(V_{b2}\\) is constant, the current of \\(M_2\\) is constant and therefore, it can be represented by a constant source \\(I_{SD2}\\) with an output resistance \\(r_{o2}\\) . If we can use \\(r_{o2}\\) to fabricate \\(R_D\\) , then a large \\(A_v\\) is obtained without using a large resistor. ::: list Keep in mind, \\(M_1\\) is an NMOS, so \\(M_2\\) must be a PMOS, otherwise it will result no voltage gain of \\(A_v \\approx -1\\) . Refer below for an in-depth explanation. ::: ::: {#corollary:mos_res} ::: cBox ::: corollaryT Corollary 5.1 . The purpose of using a PMOS is regarding the resistance looking into MOSFET. Refer to the diagram below: ::: center ::: If we use an NMOS for \\(M_2\\) , the output resistance would be: ( \\(R_{out} = r_{o1} \\parallel \\frac{1}{g_{m2}} \\approx \\frac{1}{g_{m2}}\\) \\) Since \\(r_{o1} \\gg (1/g_{m2})\\) , the voltage gain would be: ( \\(A_v = -g_{m1}\\frac{1}{g_{m2}} \\approx -1\\) \\) ::: ::: ::: Now, the output resistance can be expressed using \\(r_{o2}\\) instead of \\(R_D\\) , where: \\( \\(R_{out} = r_{o1} \\parallel r_{o2}\\) \\) Likewise, the voltage gain is: \\( \\(\\label{eq:cs_voltgain} A_v = -g_{m1}R_{out} = -g_{m1}(r_{o1} \\parallel r_{o2})\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Summary The CS amplifiers has infinite input impedance (draws no current at DC), and a moderately high output resistance (easier to match), and a high voltage gain (a desirable feature of an amplifier). It is an inverting amplifier. The voltage gain can increases, by increasing \\(g_m\\) or \\(R_D\\) : Increasing \\(g_m\\) will increases the power consumption. Increasing \\(R_D\\) is ideal, but needs to be chosen carefully to maximize output voltage swing. A current-source load is used in place of the resistor load, where a large load resistance \\(r_{o2}\\) of the amplifier is obtained without using an expensive resistor \\(R_D\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Source Degeneration A small resistor is added at the source to form source degeneration. ::: center ::: The purpose for adding source degeneration is similar to an emitter degeneration of CE amplifiers. The source degeneration forms a negative feedback. When the input voltage \\(V_{GS1}\\) attempts to increase, the voltage drop across \\(R_s\\) increases reducing \\(V_{GS1}\\) . \\( \\(V_{b1} = V_{GS1} + R_sI_{DS1}\\) \\) When \\(V_{DC}\\) (or \\(V_{b1}\\) ) is fixed, adding \\(R_s\\) reduce \\(I_{DS1}\\) subsequently DC power consumption. This is at a cost of reduced voltage gain by a factor of \\(1 + g_{m1}R_s\\) . \\( \\(\\label{eq:cs_voltgaindegen} A_v \\approx \\frac{-g_{m1}(r_{o1} \\parallel r_{o2})}{1 + g_{m1}R_s}\\) \\) Alternatively, we can mitigate the reduce voltage gain by introducing a shunt capacitor \\(C_s\\) in parallel with \\(R_s\\) , shown below. ::: center ::: -2ex -0.1ex -.2ex .2ex .2ex Summary The voltage gain is reduced by a factor of \\(1 + g_mR_s\\) as seen in . To preserve voltage gain at the frequency of the input, a shunt capacitor \\(C_s\\) is added in parallel with \\(R_s\\) . -4ex -1ex -.4ex 1ex .2ex Common-Gate (CG) Amplifier The common-gate amplifier circuit is shown below: ::: center ::: The properties of a CG amplifier: The input signal enters the amplifier from the source. The output of the amplifier is taken at the drain. The gate is routed to a DC voltage \\(V_{b1}\\) and hence acts as an AC ground in small-signal equivalent circuit, as shown in the next section. The transistor operates in saturation mode, \\(V_{GS} = V_b - v_s > V_T\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CG Amplifier The small-signal equivalent of the CG amplifier with resistor load is shown below: ::: center ::: The input resistance is: \\( \\(R_{in} \\approx \\frac{1}{g_m}\\) \\) The output resistance is: \\( \\(R_{out} = R_D \\parallel r_o\\) \\) The voltage gain is: \\( \\(A_v \\approx g_m(R_{out}) = g_m(R_D \\parallel r_o)\\) \\) Notice that the gain expression is the same as for the CS amplifier without the negative sign, which makes it a non-inverting amplifier. However, CS has a larger overall voltage gain, since CG suffers a signal loss due to its small input impedance: The larger the input resistance \\(R_{in}\\) , the more the input signal \\(v_s\\) will appear at the input of the amplifier. \\( \\(v_{in} = \\frac{R_{in}}{R_s + R_{in}}v_s \\approx \\bigg(1 - \\frac{R_s}{R_{in}}\\bigg)v_s\\) \\) If you recall, CS amplifiers have a large input resistance, \\(R_{in} \\gg R_s\\) , thus making it a desirable voltage amplifier. \\( \\(\\frac{R_s}{R_{in}} \\approx 0 \\qquad \\Longrightarrow \\qquad v_{in} \\approx v_s\\) \\) While CG amplifiers have a small input resistance, \\(1/g_m\\) , resulting to a huge voltage loss from \\(v_s\\) to \\(v_{in}\\) . Despite having the same voltage gain, the CS amplifier have a lower overall voltage gain. The small-signal equivalent of the CG amplifier with current-source load is shown below: ::: center ::: Refer to . The purpose of \\(M_2\\) is identical to the one described for CS amplifier, such that the output resistance can be expressed using \\(r_{o2}\\) instead of \\(R_D\\) , where: \\( \\(R_{out} = r_{o1} \\parallel r_{o2}\\) \\) Then, the voltage gain is: \\( \\(A_v \\approx g_m(R_{out}) = g_m(r_{o1} \\parallel r_{o2})\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Summary Unlike CS amplifiers, the CG amplifiers has a low input impedance, which is undesirable, as it will draw large current when driven by a voltage input. It is an non-inverting amplifier. The voltage gain is made similar in magnitude to that of the CS amplifier, but suffers signal loss, thus posses a lower overall voltage gain. -4ex -1ex -.4ex 1ex .2ex Common-Drain (CD) Amplifier The common-drain amplifier, more commonly known as the source follower, is similar to the emitter follower for the BJT. The common-drain amplifier circuit is shown below: ::: center ::: The properties of a CD amplifier: The input signal enters the amplifier from the gate. The output of the amplifier is taken at the source. -3ex -0.1ex -.4ex 0.5ex .2ex Characteristic Parameter of the CD Amplifier The small-signal equivalent of the CD amplifier with resistor load is shown below: ::: center ::: Likewise with CS amplifiers, because there is no gate current flow, the input resistance \\(R_{in}\\) is: \\( \\(R_{in} = \\infty\\) \\) The output resistance is: \\( \\(R_{out} = \\frac{1}{g_m}\\) \\) The voltage gain is: \\( \\(A_v \\approx 1\\) \\) As the name suggests, the output follows the input, thus the voltage gain of \\(1\\) . The small-signal equivalent of the CG amplifier with current-source load is shown below: ::: center ::: A NMOS-based source follower functions as a voltage down-shifter as \\(v_o = v_{in} - v_{GS1}\\) . It shifts the voltage level down by \\(v_{GS1}\\) . A PMOS-based source follower functions as a voltage up-shifter as \\(v_o = v_{in} + v_{SG1}\\) . It shifts the voltage level down by \\(v_{SG1}\\) . Since \\(v_{GS1,\\,min} = V_{Tn}\\) and \\(v_{GS1,\\,min} = |V_{Tp}|\\) , the minimum voltage shift is the threshold voltage. -2ex -0.1ex -.2ex .2ex .2ex Summary The CD amplifier has infinite input impedance (draws no current at DC), a relatively low output resistance, and a voltage gain that is near unity, \\(A_v \\approx 1\\) . The source follower is used as the output (or last) stage in a multistage amplifier, refer to the next section. Its function is to equip the overall amplifier with a low output resistanceenabling it to supply relatively large load currents without loss of gain. -4ex -1ex -.4ex 1ex .2ex Multi-Stage Amplifiers Two or more (CS or CG) amplifiers can be connected in a cascaded arrangement with the output of one amplifier driving the input of the next. ::: center ::: Isolation capacitors exists at the input, output, and in between stages to: Isolate the amplifier from both input source and output load DC bias each stage individually The overall voltage gain of cascaded amplifiers is the product of the individual voltage gains. \\( \\(A_v = A_{v1}A_{v2}A_{v3} \\cdots A_{vn}\\) \\) where \\(n\\) is the number of stages, as previously described in . The process of solving them is still the same as shown before, but the equations you use differ as you are now using MOSFET instead of BJT. -4ex -1ex -.4ex 1ex .2ex Current Mirrors As a recap, a current mirror is a circuit designed to copy a current through one active device by controlling the current in another active device of a circuit. -3ex -0.1ex -.4ex 0.5ex .2ex Diode-Connected MOSFET In MOSFET, we have something very similar to the diode-connected BJT, where the drain and gate are tied together. ::: center ::: When a wire is connected between the drain and gate, the diode \\(D_2\\) is short-circuited. As a result, the entire circuit functions like a diode \\(D_1\\) . ::: center ::: Looking into the circuit, the input resistance is simply the resistance of the diode \\(D_1\\) , which is: \\( \\(R_{in} \\approx \\frac{1}{g_m}\\) \\) Thus, a diode-connected MOSFET synthesizes a resistor of low resistance. -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror without Base-Width Modulation Let's now look at a MOSFET current mirror. Refer to the circuit below. ::: center ::: Since the drain and gate are connected, the MOSFET operates in saturation region, because \\( \\(v_D = v_G\\) \\) Therefore, we can say \\( \\(v_{DS} \\geq v_{GS} - V_T\\) \\) From , if we neglect \\(r_o\\) , then the drain current is given by: \\( \\(i_{DS1} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_1(v_{GS1} - V_T)^2\\) \\) \\( \\(i_{DS2} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_2(v_{GS2} - V_T)^2\\) \\) and by dividing both equations we have \\( \\(i_{DS2} = \\frac{(W/L)_2}{(W/L)_1}i_{DS1}\\) \\) If \\(M_1\\) and \\(M_2\\) are identical, such that \\((W/L)_1 = (W/L)_2\\) , then it functions as current mirror as \\(i_{DS1} = i_{DS2}\\) . By adjusting the \\(W/L\\) ratio, it becomes a current amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Analysis of Current Mirror with Base-Width Modulation Let's now consider the impact of base-width modulation \\(r_o\\) . When accounted for, the circuit can be depicted as: ::: center ::: Now the equation for drain current becomes \\( \\(i_{DS1} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_1(v_{GS1} - V_T)^2(1 + \\lambda v_{DS1})\\) \\) \\( \\(i_{DS2} \\approx \\frac{1}{2}\\mu_nC_{ox}\\bigg(\\frac{W}{L}\\bigg)_2(v_{GS2} - V_T)^2(1 + \\lambda v_{DS2})\\) \\) and by dividing both equations we have \\( \\(\\frac{i_{DS2}}{i_{DS1}} = \\frac{(W/L)_2}{(W/L)_1}\\bigg(\\frac{1 + \\lambda v_{DS1}}{1 + \\lambda v_{DS2}}\\bigg)\\) \\) Note that we cannot determine the current ratio simply by \\(W/L\\) alone, as it introduces an uncertainty, since \\(v_{DS1} \\neq v_{DS2}\\) . If we let \\( \\(v_{DS1} = v_{DS} \\hspace{2cm} v_{DS2} = v_{DS} + \\Delta v_{DS}\\) \\) where \\(\\Delta v_{DS}\\) is the difference in voltage between \\(v_{DS1}\\) and \\(v_{DS2}\\) . As you can see, the current ratio is directly proportional to \\(\\Delta v_{DS}\\) \\( \\(\\Delta\\bigg(\\frac{i_{DS2}}{i_{DS1}}\\bigg) = \\frac{(W/L)_2}{(W/L)_1}\\bigg(\\frac{\\lambda\\Delta v_{DS}}{1 + \\lambda v_{DS}}\\bigg)\\) \\) So in order to produce a very accurate current mirror, you must reduce \\(\\Delta v_{DS}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Current Sinks and Current Sources The purpose of current mirrors is to construct current sources and current sinks from a master current source whose value is stable---independent of temperature and supply voltage fluctuations. ::: center ::: Solving them are fairly straightforward, as the currents are set by the ratio of transistor size, specifically the area of the emitter \\(W/L\\) . For example, we have a two-stage CS amplifier. The circuit is denoted in black and the current sources and sinks are in blue. ::: center ::: Given \\(J\\) , then \\(i_{DS1} = J\\) . the current sinks are: \\(\\displaystyle i_{DS2} = \\frac{(W/L)_2}{(W/L)_1}J\\) \\(\\displaystyle i_{DS7} = \\frac{(W/L)_7}{(W/L)_1}J\\) Given \\(i_{DS2} = i_{SD3}\\) , the current sources are: \\(\\displaystyle i_{SD4} = \\frac{(W/L)_4}{(W/L)_3}I_{SD3} = \\frac{(W/L)_4}{(W/L)_3}\\frac{(W/L)_2}{(W/L)_1}J\\) Finally, the current flowing through the circuit: \\(\\displaystyle i_{DS5} = i_{SD4}\\) \\(\\displaystyle i_{SD6} = i_{DS7}\\)","title":"MOSFET Voltage Amplifiers"},{"location":"W2022/ELE404/ELE404/#differential-mosfet-voltage-amplifiers","text":"-4ex -1ex -.4ex 1ex .2ex Introduction As the name suggests, the differential amplifier amplifies the difference between two input signals (or the differential input signal). They are useful for suppressing noise, as the supply voltage is generally not constant. ::: center ::: Its fluctuation is known as supply voltage noise, which affects the load line subsequently the DC operating point. Varying DC operating point causes \\(g_m\\) to vary, resulting in varying \\(v_{out}\\) not caused by \\(v_{in}\\) . ::: center ::: In other words, if there is noise in the supply voltage, then it will also get coupled with the output signal. Therefore, the output of the amplifier that you design should be insensitive to supply voltage noise, ground noise, and the fluctuation of the input of the amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Single-Ended Signaling The amplifiers we have encountered so far are single-ended amplifiers, which amplifies a single input signal. ::: center ::: Signals are represented by nodal voltages with reference to a constant voltage, typically the ground. And as we demonstrated earlier, single-ended signaling is much more susceptible to noise. -4ex -1ex -.4ex 1ex .2ex Differential Amplifiers Thus, we introduce differential signaling. ::: center ::: Compared to single-ended signaling, signals are represented by the difference between two single-ended nodal voltages. It's consists of a DC component \\(v_{in,\\, cm}\\) and AC components \\(v_{in,\\, AC}^+\\) and \\(v_{in,\\, AC}^-\\) , where \\( \\(v_{in}^+ = v_{in,\\, AC}^+ + v_{in,\\, cm} \\hspace{2cm} v_{in}^- = v_{in,\\, AC}^- + v_{in,\\, cm}\\) \\) You can think of \\(v_{in,\\, cm}\\) as our DC biasing voltage of the input transistors of the differential amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex NMOS Differential Pair It consists of a pair of identical common-source amplifiers with the source of the input transistors \\(M_2\\) and \\(M_3\\) tied together. ::: center ::: As you can see \\(V_{b2}\\) acts as our DC biasing voltage (or \\(v_{in,\\,cm}\\) ) and the AC component \\(v_{in}^+\\) , \\(v_{in}^-\\) . ::: center ::: The transistors \\(M_4\\) and \\(M_5\\) function as constant current-source load, which if you recall from the previous module are used in place of a resistor load. Though, one component you may not be particular familiar with is the transistor \\(M_1\\) , located at the source of the CS amplifier. \\(M_1\\) forms the tail current source of constant current \\(I_{SS}\\) , where \\( \\(\\label{eq:nmos_tailcurrent} I_{DS2} = I_{DS3} = \\frac{I_{SS}}{2}\\) \\) The purpose of the tail current source is to split that difference between the two sides of the amplifier. As shown from the equation above, \\(I_{SS}\\) is split evenly between \\(M_2\\) and \\(M_3\\) . -3ex -0.1ex -.4ex 0.5ex .2ex PMOS Differential Pair Suppose we use PMOS as the input transistor. ::: center ::: Similar to NMOS, \\(V_{b2}\\) makes up the DC component and \\(v_{in}^+\\) , \\(v_{in}^-\\) which make up the AC components. ::: center ::: The transistor \\(M_1\\) now forms the head current source of constant current \\(I_{SS}\\) . Similar to tail current source, it is designed to split that difference between the two sides of the amplifier. -4ex -1ex -.4ex 1ex .2ex Differential Voltage Gain Now let's go over how voltage is amplified in a differential amplifier. Using the principle of superposition, we can express the differential signaling as such ::: center ::: The differential input is \\( \\(v_{in} = v_{in}^+ - v_{in}^-\\) \\) where it is made up of two signals \\(180^\\circ\\) out of phase \\( \\(v_{in}^+ = \\frac{v_s}{2} + V_{b2} \\hspace{2cm} v_{in}^- = -\\frac{v_s}{2} + V_{b2}\\) \\) The differential output is \\( \\(\\label{eq:diff_output} v_o = v_o^+ - v_o^-\\) \\) which we can derive from the small-signal equivalent circuit. -3ex -0.1ex -.4ex 0.5ex .2ex Small-Signal Equivalent Circuit For reference, the circuit below is an NMOS differential pair. ::: center ::: By performing KVL, where \\(V_{SS}\\) is the node voltage of \\(SS\\) then \\( \\(V_{b2} = V_{GS2,3} + V_{SS} \\qquad\\text{ or }\\qquad V_{GS2,3} = V_{b2} - V_{SS}\\) \\) Recall the equation for transconductance (in ), then we can express \\(g_{m2,3}\\) as \\( \\(g_{m2,3} = \\frac{2I_{DS2,3}}{\\underbrace{V_{b2} - V_{SS}}_{V_{GS2,3}} - V_T}\\) \\) Now we'll separate our DC and AC components of the differential amplifier. ::: center ::: The \\(I_{SS}/2\\) represents our DC current, which we already know from and . The \\(\\Delta i\\) represents our AC current. Then, the current of \\(M_2\\) and \\(M_3\\) is \\( \\(i_{DS2} = \\frac{I_{SS}}{2} + \\Delta i \\hspace{2cm} i_{DS3} = \\frac{I_{SS}}{2} - \\Delta i\\) \\) If you remember, a transconductor maps a voltage to a current. So in AC, we're mapping the \\(v_s/2\\) to \\(\\Delta i\\) , which we can say \\( \\(\\Delta i = g_{m2,3}\\bigg(\\frac{v_s}{2}\\bigg)\\) \\) If we perform KCL at the node \\(SS\\) , then the current flowing through \\(M_1\\) is constant. \\( \\(i_{DS1} = i_{DS2} + i_{DS3} = I_{SS}\\) \\) It is independent of the input source, that is \\(v_s\\) , or in other words, it is constantcurrent doesn't change. This means we can turn the node \\(SS\\) into an AC ground. ::: center ::: So in small-signal analysis, the differential amplifier can be split into two identical common-source amplifiers whose AC outputs are given by \\( \\(v_o^+ = g_{m3}(r_{o3} \\parallel r_{o5})\\bigg(-\\frac{v_s}{2}\\bigg)\\) \\) \\( \\(v_o^- = g_{m2}(r_{o2} \\parallel r_{o4})\\bigg(\\frac{v_s}{2}\\bigg)\\) \\) Note that \\(g_{m2} = g_{m3}\\) and \\(r_{o2,3} = r_{o4,5}\\) . From , the differential output is \\( \\(v_o = v_o^+ - v_o^- = g_{m2,3}(r_{o2,3} \\parallel r_{o4,5})v_s\\) \\) where the voltage gain is \\( \\(A_v = \\frac{v_o}{v_i}\\times\\frac{v_s}{v_i} = g_{m2}(r_{o2} \\parallel r_{o4}) = g_{m3}(r_{o3} \\parallel r_{o5})\\) \\) Notice it has the same voltage gain as that of corresponding common-source amplifier (in ). It doesn't provide any additional gain, which might not make sense at first. But what this provides is a way to deal with supply voltage noise, without having to worry about the gain being modified for using differential signaling. -3ex -0.1ex -.4ex 0.5ex .2ex Power Consumption This, however, comes at the cost of uses twice the power consumption. Since the voltage gain is the same, under the condition that the both transistors have the same transconductance \\( \\(g_{m} = \\frac{2I_{DS}}{{V_{b2} - V_{SS}} - V_T}\\) \\) Then the current \\(I_{DS}\\) of each branch of the differential pair, \\(M_2\\) and \\(M_3\\) , must be the same as that of CS amplifier. ::: center ::: Therefore, the tail current of the differential pair is twice that of the DC current of CS amplifier. As result, the power consumption of differential part is \\(2\\times\\) that of CS amplifier. -3ex -0.1ex -.4ex 0.5ex .2ex Differential-Input Single-Ended-Output Amplifiers In most scenarios, we only want one output \\(v_o\\) , instead of two outputs \\(v_o^+\\) and \\(v_o^-\\) . So how can we modify the differential amplifier to do such task? ::: center ::: It is pretty simple, which involves using a current mirror to perform differential-to-single ended conversion. ::: center ::: The output current is \\( \\(i_o = 2 \\times g_m\\bigg(\\frac{v_s}{2}\\bigg) = g_mv_s\\) \\) and the output voltage is \\( \\(v_o = (r_{o3} \\parallel r_{o5})i_o = g_m(r_{o3} \\parallel r_{o5})v_s\\) \\) As you can see, we have a non-inverting amplifier with the same voltage gain as that of the differential-input differential-output amplifier. -4ex -1ex -.4ex 1ex .2ex Rejection of Supply Noise Let's now go over the process of rejecting noise from the supply voltage. We can represent the supply voltage as \\(V_{DD} + v_{dd}\\) , where \\(v_{dd} \\ll V_{DD}\\) represents the supply voltage noise. ::: center ::: Our goal is to know whether the differential output of the differential pair is affected by \\(v_{dd}\\) or not. We'll first look at a differential-input differential-output amplifierstwo inputs and two outputs. ::: center ::: You might have notice we also short-circuited \\(v_s/2\\) , even though it's not constant. The purpose is that we are only interested in the response of the amplifier due to \\(v_{dd}\\) . Performing KCL at the output nodes \\(v_o^+\\) and \\(v_o^-\\) results in \\( \\(v_o^+ \\approx g_{m5}(r_{o3} \\parallel r_{o5})v_{dd}\\) \\) \\( \\(v_o^- \\approx g_{m4}(r_{o2} \\parallel r_{o4})v_{dd}\\) \\) As you can see, the output voltages are a function of the supply voltage noise \\(v_{dd}\\) . However, when we calculate the differential output \\( \\(v_o = v_o^+ - v_o^- = 0\\) \\) It is \\(0\\) , which means the differential output is insensitive to supply voltage noise. You can refer to the diagram below. ::: center ::: When there's noise, it's gonna look the same on both output voltages. And so when we compare the two output voltages, \\(v_o = v_o^+ - v_o^-\\) , the noise will cancels each other out. ::: center ::: Now let's look at a differential-input single-ended-output amplifierstwo inputs and one outputs. ::: center ::: As we covered in the previous section, we can modify the differential amplifier, such that there is only one output, but comes at a cost. If we perform KCL and node \\(A\\) and node \\(B\\) , we can derive the single-ended output voltage \\( \\(v_o = \\frac{g_{m5}v_A + g_{o5}v_{dd}}{g_{m5} + g_{o3} + g_{o5}}\\) \\) Then \\(v_o\\) is a function of the \\(v_{dd}\\) , meaning that differential-input single-ended output amplifiers cannot reject voltage noise. -3ex -0.1ex -.4ex 0.5ex .2ex Ground Noise As of now we only covered noise coming from supply voltage, but there's also noise coming from the ground, that is caused by improper grounding such as a large resistance of ground rails. ::: center ::: When a noise-current enters the ground rail, the potential of the ground rail is no longer at \\(\\SI{0}{\\volt}\\) fluctuating from \\(0\\) to \\(v_{ss}\\) , where \\(v_{ss}\\) is typically tens of \\(\\si{\\milli\\volt}\\) . Using small-signal analysis, we can find the output voltage of the differential pair caused by \\(v_{ss}\\) . ::: center ::: Performing KCL at the output nodes \\(v_o^+\\) and \\(v_o^-\\) results in \\( \\(v_o^+ \\approx g_{m3}(r_{o3} \\parallel r_{o5})v_{ss}\\) \\) \\( \\(v_o^- \\approx g_{m2}(r_{o2} \\parallel r_{o4})v_{ss}\\) \\) \\( \\(v_o = v_o^+ - v_o^- = 0\\) \\) As with supply voltage noise, the output is also insensitive to ground noise. -4ex -1ex -.4ex 1ex .2ex Common-Mode Referring to the diagram below, we mentioned previously, \\(v_{in,\\,cm}\\) and \\(v_{o,\\,cm}\\) depicts the DC operating point, whereas \\(v_{in,\\,AC}^{+,-}\\) and \\(v_{o,\\,AC}^{+,-}\\) carry the information. ::: center ::: Ideally, we want the common-mode input \\(v_{in,\\,cm}\\) to be stable, as it provides the DC biasing voltage. However, that is something we have no control over, so instead we should design our differential amplifier to be insensitive to \\(v_{in,\\,cm}\\) fluctuations. -3ex -0.1ex -.4ex 0.5ex .2ex Common-Mode Voltage Gain The common-mode voltage gain is defined as \\( \\(A_{v,\\,cm} = \\frac{v_{o,\\,cm}}{v_{in,\\,cm}}\\) \\) which can be obtained by applying both \\(v_{in,\\,cm}\\) to both inputs and measuring either ends of the single-ended outputs of the differential pair, shown below. ::: center ::: Ideally, we prefer a common-mode voltage gain \\(A_{v,\\,cm}\\) of \\(0\\) , which means there's zero fluctuations from the input to output. Similar to what we did with voltage noise, we will perform small-signal analysis. ::: center ::: A few things to make note of: Consider \\(r_{o1}\\) as the result of two resistors, \\(2r_{o1}\\) , to be connected in parallel. Then we have that node \\(SS1\\) and node \\(SS2\\) to be identical, such that no current flows in between the nodes, since they have same voltages, \\(v_{SS2} - v_{SS1} = 0\\) . Therefore, we can separate it, forming two sub-circuits, that can be identified as a common-source amplifier with source degeneration. From , we can modify it to obtain the common-mode voltage gain, that is \\( \\(A_{v,\\,cm} \\approx \\frac{-g_{m2,3}(r_{o2} \\parallel r_{o4})}{1 + g_{m2,3}(2r_{o1})}\\) \\) In order to have a \\(A_{v,\\,cm} \\approx 0\\) , the denominator must be very large. In other words, \\(r_{o1}\\) forms source degeneration and significantly lowers \\(A_{v,\\,cm}\\) . ::: list Note that \\(r_{o1}\\) has no impact on the differential voltage gain. ::: -3ex -0.1ex -.4ex 0.5ex .2ex Voltage Gain Comparison To expand a bit more on the final bit, one of the reason for using a tail current source (or head current source for PMOS) is to reduce the common-mode voltage gain. In technicality, we can choose to not add it, as it is indeed optional. ::: center ::: Both amplifiers still have the same differential voltage gain. However, the circuit on the left will have a larger common-mode voltage gain. If \\(v_{in,\\,cm}\\) fluctuates, in turn, the circuit's input and output will fluctuate significantly as well. -3ex -0.1ex -.4ex 0.5ex .2ex CMRR This is one of the parameters we use quantify amplifiers. We won't go much in-depth regarding this topic, but a good amplifier should have: a large differential-mode voltage gain \\(A_{v,\\,\\text{diff}}\\) , and ... a small common-mode voltage gain \\(A_{v,\\,cm}\\) . CMRR stands for common-mode rejection ratio which is \\( \\(\\text{CMRR} = 20\\log{\\bigg(\\frac{A_{v,\\,\\text{diff}}}{A_{v,\\,cm}}\\bigg)}\\) \\) A typically value is around \\(\\SI{80}{\\decibel}\\) . -4ex -1ex -.4ex 1ex .2ex Mismatch In the design stage, we always assume that the two branches of a differential amplifier to be completely identical ::: center ::: such that: Load match: \\(R_{D1} = R_{D2} = R_D\\) Dimensions match: \\((W/L)_1 = (W/L)_2 = (W/L)\\) Threshold voltage match: \\(V_{T1} = V_{T2} = V_T\\) However, by the time the circuit is fabricated, you will see that they are different. This is due to the fabrication process uncertainties which give rise to mismatch between the branches. ::: center ::: While it is not possible to get rid of the mismatch, our goal is to minimize the impact it has on our circuit. We always assume the worst-case: Worst-case load mismatch: \\(\\pm\\Delta R_D\\) Worst-case dimension mismatch: \\(\\pm\\Delta (W/L)\\) Worst-case threshold mismatch: \\(\\pm\\Delta V_T\\) -2ex -0.1ex -.2ex .2ex .2ex Load Mismatch To calculate the load mismatch, we assume the worst-case, so let \\(R_{D1} = R_D + \\Delta R_D\\) and \\(R_{D2} = R_D - \\Delta R_D\\) with no other mismatch. Then apply a common-mode voltage \\(v_{in,\\,cm}\\) to both inputs. ::: center ::: Performing KVL, the output nodes \\(v_o^+\\) and \\(v_o^-\\) are \\( \\(v_o^+ = V_{DD} - (R_D - \\Delta R_D)i_{DS}\\) \\) \\( \\(v_o^- = V_{DD} - (R_D + \\Delta R_D)i_{DS}\\) \\) then the differential output is \\( \\(v_{o,\\,\\text{diff},\\,R_D} = v_o^+ - v_o^- = 2i_{DS}\\Delta R_D\\) \\) The output offset voltage caused by load mismatch is proportional to \\(\\Delta R_D\\) . -2ex -0.1ex -.2ex .2ex .2ex Dimension Mismatch Similarly, to calculate the dimension mismatch, let \\((W/L)_2 = (W/L) + \\Delta (W/L)\\) and \\((W/L)_3 = (W/L) - \\Delta (W/L)\\) with no other mismatch. Apply \\(v_{in,\\,cm}\\) to both inputs. ::: center ::: The differential output is \\( \\(v_{o,\\,\\text{diff},W/L} = v_o^+ - v_o^- = -2R_DI_{DS}\\bigg[\\frac{\\Delta (W/L)}{(W/L)}\\bigg]\\) \\) The output offset voltage caused by dimension mismatch is inversely proportional. This means that if we want to reduce the dimension mismatch, we need to increase \\((W/L)\\) . -2ex -0.1ex -.2ex .2ex .2ex Threshold Voltage Mismatch Lastly, we have the threshold voltage mismatch. As usual, let \\(V_{T2} = V_T + \\Delta V_T\\) and \\(V_{T3} = V_T - \\Delta V_T\\) with no other mismatch and apply \\(v_{in,\\,cm}\\) to both inputs. ::: center ::: The differential output is \\( \\(v_{o,\\,\\text{diff},\\,V_T} = v_o^+ - v_o^- = -4k_n \\frac{W}{L}(V_{in,\\,cm} - V_{SS} - V_T)\\Delta V_T\\) \\) The output offset voltage caused by threshold voltage mismatch is proportional to \\(\\Delta V_T\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Offset Voltage The load mismatch, dimension mismatch, and threshold voltage mismatch are uncorrelated. Therefore, the total mismatch-induced output offset voltage is obtained from \\( \\(v_{os,\\,\\text{output}} = \\sqrt{v_{o,\\,\\text{diff},\\,R_D}^2 + v_{o,\\,\\text{diff},W/L}^2 + v_{o,\\,\\text{diff},\\,V_T}^2}\\) \\) The mismatch results in a non-zero output voltage, \\(v_{os,\\,\\text{output}} \\neq 0\\) . In circuit analysis, we always want to bring this all the way back to the input, such that we can make comparisons at the input. ::: center ::: The way we do this, is by attaching an artificial input offset voltage source \\(v_{os,\\,input}\\) which we can be calculate via dividing by differential voltage gain \\( \\(v_{os,\\,input} = \\frac{v_{os,\\,output}}{A_{v,\\,\\text{diff}}}\\) \\) Typically, it is placed at the non-inverting input of the amplifier \\(v_{in}^+\\) . We will use a technique called auto-zeroing to remove the impact of \\(v_{os,\\,input}\\) . -2ex -0.1ex -.2ex .2ex .2ex Virtual Short Characteristics Before going over the technique, we must understand what exactly is a virtual short. ::: center ::: It refers to a condition of a differential input amplifier, in which its non-inverting and inverting inputs have almost the same voltage \\( \\(v_{in}^+ = v_{in}^-\\) \\) There's two conditions to be met: They form a negative feedback, \\(v_o^+\\) and \\(v_o^-\\) are connected to \\(v_{in}^-\\) and \\(v_{in}^+\\) respectivelypositive to negative and vice-versa. The differential voltage gain \\(A_{v,\\,\\text{diff}}\\) is sufficiently large. -2ex -0.1ex -.2ex .2ex .2ex Auto-Zeroing Technique To demonstrate the auto-zeroing, we have switches that are controlled by the input signal. When \\(\\phi = 1\\) , we have a virtual short, where \\(v^+ = v^-\\) , and the capacitor is charged to \\(v_{os}\\) with polarity shown. When \\(\\phi = 0\\) , it does some magic and the voltage of the capacitor cancels out \\(v_{os}\\) . Then poof, the offset voltage goes bye bye. ::: center ::: Some remarks: \\(v_{os}\\) needs to remain unchanged during both \\(\\phi = 0\\) and \\(\\phi 1\\) . \\(v_o\\) is not available during \\(\\phi = 1\\) . Switches are implemented using MOSFET.","title":"Differential MOSFET Voltage Amplifiers"},{"location":"W2022/MTH314/MTH314/","text":"Introduction This will cover various topics for the course MTH314: Discrete Mathematics for Engineering, using the textbook, Discrete Mathematics , by A. Bonato, the textbook, Discrete Mathematics with Applications , by S. Epp, and lectures notes provided by the professor, Dr. Changping Wang. Other resources used: Discrete Math (Full Course: Sets, Logic, etc) - Dr. Trefor Bazett Last Updated: 2022-04-16 Intro to Sets and Logic \u00b6 The Language of Sets \u00b6 One of the most important fundamentals revolves around sets. Definition 1.1 (Set). A set refers to a collection of objects, written in set-roster notation; using curly brackets \\(\\{ \\}\\) or set-builder notation, which will be discussed later. We use the following notation \\(\\in\\) to represent an element of a set and \\(\\notin\\) when it is not an element of a set. Example 1.1 . *Given \\(A = \\{1,2,3,4,5\\}\\) , we can write it as: \\(1 \\in A\\) \\(3 \\in A\\) \\(6 \\notin A\\) \\(\\pi \\notin A\\) There are certain sets of numbers referred to frequently, so they are given common set notations: Symbol Set of ... Example \\(\\mathbb{N}\\) Non-negative integers or natural numbers \\(\\{0,1,2,3, \\cdots\\}\\) \\(\\mathbb{Z}\\) Integers \\(\\{\\cdots, -2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q}\\) Rational numbers \\(\\{\\frac{p}{q} \\mid p,q \\in \\mathbb{Z}, q \\neq 0\\}\\) \\(\\mathbb{R}\\) Real numbers All of the above number sets Addition to a superscript \\(+\\) or \\(-\\) indicates that only positive or negative elements of the sets: \\(\\mathbb{Z}^+ = \\{1,2,\\cdots\\}\\) \\(\\mathbb{Z}^- = \\{-1,-2,\\cdots\\}\\) \\(\\mathbb{R}^+\\) is a set of positive real numbers \\(\\mathbb{R}^-\\) is a set of negative real numbers Another way to describe a set is using a set-builder notation, which characterize all the elements in the set by stating the property or properties they must have to be members. Definition 1.2 (Set-Builder Notation). *Let \\(S\\) denote a set and let \\(P(x)\\) be a property that elements of \\(S\\) may or may not satisfy. \\[\\{x \\in S \\mid P(x)\\}\\] We may define the following to be the set of all elements \\(x\\) in \\(S\\) such that \\(P(x)\\) is true. Example 1.2 . Let \\(A = \\{x \\in \\mathbb{Z} \\mid -2 < x < 5\\}\\) , the following set can be described as the set of all elements \\(x\\) are integers such that \\(-2 < x < 5\\) , where: \\( \\(A = \\{-1,0,1,2,3,4\\}\\) \\) * Definition 1.3 (Cardinality). *The cardinality of set denotes the number of elements of the set, usually denoted with a vertical bar on each side. Example 1.3 . Let \\(B = \\{2,5,7,9,12\\}\\) , then the cardinality of set \\(B\\) is: \\[|B| = 5\\] As defined earlier, a set is a collection of objects and so how do we define a set with no objects? Definition 1.4 (Empty Set). *A special set that contains no elements is called an empty set, which uses the notation \\(\\varnothing\\) . For an empty set, the cardinality would be \\(|\\varnothing| = 0\\) , since it contains no elements; \\(\\varnothing = \\{\\}\\) . Subsets \u00b6 A basic relation between sets is that of subset, which introduces a new notation \\(\\subseteq\\) for subset and \\(\\subsetneq\\) for proper subset. $$A \\subseteq B$$ $$A \\subsetneq B$$ Definition 1.5 (Subset). If \\(A\\) and \\(B\\) are sets, then \\(A\\) is a subset of \\(B\\) , written as \\(A \\subseteq B\\) , if and only if every element of \\(A\\) is also an element of \\(B\\) . Definition 1.6 (Proper subset). *If \\(A\\) is a subset of \\(B\\) , but not equal to \\(B\\) ; where there is at least one element of \\(B\\) not in \\(A\\) , then \\(A\\) is a proper subset of \\(B\\) , written as \\(A \\subsetneq B\\) . Some online sources or textbooks may use the following notation \\(\\subset\\) instead of \\(\\subsetneq\\) , to represent a proper subset. They are the same thing, but to avoid confusion, I'll be using this notation \\(\\subsetneq\\) . Note that every proper subset is a subset, but not every subset is a proper subset. So the diagram shown in (b) can also be used to demonstrate what a subset looks like. Example 1.4 . *Let \\(A = \\{1,2,3,4,5\\}\\) , \\(B = \\{1,3,5\\}\\) , \\(C = \\{2,3,5\\}\\) , and \\(D = \\{2,3,5\\}\\) then: \\(B \\subseteq A\\) and \\(B \\subsetneq A\\) \\(C \\subseteq A\\) and \\(C \\subsetneq A\\) \\(C\\subseteq D\\) and \\(D \\subseteq C\\) Two sets are equal when they share the exact element or in other words, subsets of each other: \\(C \\subseteq D\\) and \\(D \\subseteq C \\Longleftrightarrow C = D\\) . Power Sets \u00b6 Previously, we went over what a subset is and so will use that to define the following set. Definition 1.7 (Power Set). Let \\(S\\) denote a set, the power set of \\(S\\) , denoted \\(\\mathcal{P}(S)\\) , is the set of all subsets of \\(S\\) . In general, for any finite set \\(S\\) , where \\(|S| = n\\) , we have that \\(|\\mathcal{P}(S)| = 2^n\\) . If you recall every proper subset is considered a subset and so we may use that in the following example. Any set is gonna have various subsets. Example 1.5 . Given a set \\(A = \\{1,3,5\\}\\) , we can define the following to be subsets of \\(A\\) : \\(\\{1\\} \\subseteq A\\) \\(\\{1,3\\} \\subseteq A\\) \\(\\{1,5\\} \\subseteq A\\) \\(\\{1,3,5\\} \\subseteq A\\) and so on The power set is basically all the possible subsets of \\(A\\) that can be formed given a set. The empty set \\(\\varnothing\\) and the set \\(A\\) is always included in the \\(\\mathcal{P}(A)\\) . Example 1.6 . Using the set \\(A = \\{1,3,5\\}\\) , then: \\[\\mathcal{P}(A) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,3\\}, \\{1,5\\}, \\{3,5\\}, \\{1,3,5\\}\\}\\] It becomes quite tricky when we use an empty set. The power set of an empty set, \\(\\mathcal{P}(\\varnothing) = \\{\\varnothing\\}\\) with cardinality of \\(1\\) . We can double check this by \\(|\\varnothing| = 0\\) and so \\(|\\mathcal{P}(\\varnothing)| = 2^0 = 1\\) or cardinality of \\(1\\) . Singleton and Doubleton \u00b6 When a set only has one or two elements, it can be classified into one of two ways: Definition 1.8 (Singleton). A singleton is a set with a single element, \\(\\{x\\}\\) . Definition 1.9 (Doubleton). A doubleton (or unordered pair) is a set with two elements, \\(\\{x,y\\}\\) . An ordered pair uses round brackets instead of curly brackets to indicate that order matters, \\((x,y)\\) , which will be discussed more in later sections. Operations on Sets \u00b6 There are four main set operations to be discussed. Let \\(A\\) and \\(B\\) be subsets of a universal set \\(U\\) . Universal set, denoted \\(U\\) , is the collection of all objects that can occur as elements of the sets under consideration. All other sets are subsets of the universal set. Definition 1.10 (Union). The union of \\(A\\) and \\(B\\) , denoted \\(A \\cup B\\) , is the set of all elements that are at least in one of \\(A\\) or \\(B\\) . \\[A\\ \\cup\\ B = \\{x \\in U \\mid x \\in A \\text{ or } x \\in B\\}\\] Definition 1.11 (Intersection). The intersection of \\(A\\) and \\(B\\) , denoted \\(A \\cap B\\) , is the set of all elements that are common to both \\(A\\) and \\(B\\) . \\[A\\ \\cap\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\in B\\}\\] Definition 1.12 (Difference). The difference of \\(A\\) minus \\(B\\) , denoted \\(A \\smallsetminus B\\) , is the set of all elements that are in \\(A\\) and not in \\(B\\) . \\( \\(A\\ \\smallsetminus\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\notin B\\}\\) \\) * Definition 1.13 (Complement). The complement of \\(A\\) , denoted \\(A^c\\) , is the set of all elements in \\(U\\) that are not in \\(A\\) . \\[A^c = \\{x \\in U | x \\notin A\\}\\] There's one other operation not included, which is the symmetric difference, denoted \\(A \\triangle B\\) , is the set of elements which are in either of the sets A and B, but are not common to both \\(A\\) and \\(B\\) . Set Properties \u00b6 The following theorem consists of set identities, some of which you might be familiar with: Theorem 1.1 . *Let \\(A\\) , \\(B\\) , and \\(C\\) be subsets of a universal set \\(U\\) : Commutative law: Associative law: Distributive law: Complement law: Double complement law: De Morgan's law: Identity law: Idempotent law: Set difference law: Indexed Collection of Sets \u00b6 The definitions of unions and intersections for more than two sets are very similar to the definitions for two sets, which we may generalize in the following way. Definition 1.14 (Union and Intersection). *Let \\(A_i\\) be a subset of a universal set \\(U\\) where \\(i \\geq 1\\) and given a non-negative integer \\(n\\) . \\[\\bigcup\\limits_{i=1}^n\\ A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } 1 \\leq i \\leq n\\}\\] \\[\\bigcap\\limits_{i=1}^n\\ A_i = A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } 1 \\leq i \\leq n\\}\\] ... and generalize to infinite unions and intersections. \\[\\bigcup\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } i \\geq 1 \\}\\] \\[\\bigcap\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } i \\geq 1\\}\\] Recap of the interval notation, \\((\\ )\\) means the endpoints are excluded and \\([\\ ]\\) means they are included. There are three specific types you'll encounter when solving for infinite intersections of a set: \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg[0, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (n, \\infty)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ [n, \\infty)\\) There might be slight variations of the questions, but it should give you a general idea on how to solve for them. Example 1.7 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : Let's think logically, by drawing out the number lines for the first three sets. If you notice the number line slowly decreases in size. As it reaches infinity, it will eventually reach \\(0\\) , which will be the only thing they all share in common. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = \\{0\\}\\] Example 1.8 . *Solve the following set, \\(\\bigcup\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : 1. We are now interested in all of the possible elements of the set. Using the number line from before, if you notice, \\((-1,1)\\) already contains all the elements in \\(A_2\\) and \\(A_3\\) . So we can say that \\((-1,1)\\) contains all the elements shared in the infinite set. \\[\\bigcup\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = (-1,1)\\] The notation for the open interval \\((a,b)\\) is identical to the notation for ordered pair \\((a,b)\\) , context makes it unlikely that the two will be confused. Example 1.9 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(0, \\frac{1}{i}\\Big)\\) : Let's draw out the number lines again for the first three sets. Likewise, as it reaches infinity, it will eventually reach \\(0\\) , however, note that \\(0\\) is not included, from the round brackets, so we say it's an empty set. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg) = \\varnothing\\] If instead it was \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\Big[0, \\frac{1}{i}\\Big)\\) , then our answer would be \\(\\{0\\}\\) , as it is included in the set. ::: ::: exampleT Example 1.10 . Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty)\\) : Let's draw out the first number line when \\(i = 1\\) and when \\(i = 5\\) . ::: center ::: As \\(i\\) goes from \\(1\\) to \\(5\\) , you notice that \\(\\{1,2,3,4\\}\\) is no longer common for all sets. As \\(i \\to \\infty\\) , logically there exists no real number which are common for all sets. We can conclude that the intersection of the infinite set is an empty set. ( \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty) = \\varnothing\\) \\) ::: ::: list Note that the answer doesn't change even if we have \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (i,\\infty)\\) , our answer is still \\(\\varnothing\\) . ::: If you are interested in the mathematical proof, click on the following link: [Archimedean property]{.underline} . -3ex -0.1ex -.4ex 0.5ex .2ex Cartesian Products on Sets In this section, we'll first focus on the notion of ordered pairs and how they work. ::: dBox ::: definitionT Definition 1.15 (Ordered pair). The symbol \\((a,b)\\) denotes the ordered pair with the specification that \\(a\\) is the first element and \\(b\\) is the second element of the pair. ( \\((a,b) = (c,d) \\longrightarrow a = c \\text{ and } b = d\\) \\) ::: ::: ::: dBox ::: definitionT Definition 1.16 (Cartesian product). For sets \\(A\\) and \\(B\\) , the Cartesian product of \\(A\\) and \\(B\\) , denoted \\(A \\times B\\) , is the set of all ordered pairs \\((a,b)\\) . ( \\(A \\times B = \\{(a,b) \\mid a \\in A \\text{ and } b \\in B\\}\\) \\) ::: ::: ::: exampleT Example 1.11 . Let \\(A = \\{x,y\\}\\) and \\(B = \\{1,2,3\\}\\) , find \\(A \\times B\\) and \\(B \\times A\\) : \\(A \\times B = \\{(x,1), (y,1), (x,2), (y,2), (x,3), (y,3)\\}\\) \\(B \\times A = \\{(1,x), (1,y), (2,x), (2,y), (3,x), (3,y)\\}\\) ::: ::: list Note how the order matters, such that \\(A \\times B \\neq B \\times A\\) in the following example above. ::: -4ex -1ex -.4ex 1ex .2ex Logic It's important to first establish one thing, which we define as: ::: dBox ::: definitionT Definition 1.17 (Statement). A statement (or proposition) is a sentence that is true or false, but not both. ::: ::: There are different ways to express a statement as shown below. ::: exampleT Example 1.12 . Determine whether the following are statements and if so, are they true or false? \\\" \\(2\\) is greater than \\(5\\) \\\" is a statement and is logically false. \\\" \\(x > 5\\) \\\" is not a statement because of the variable \\(x\\) , it is undetermined. \\\" \\(\\sqrt{9}\\) is an integer\\\" is a statement and is logically true. \\\"There are \\(7\\) days in a week\\\" is a statement and is logically true. ::: We will now introduce five logical connectives, used to build more complicated logical expressions out of simpler ones. Let \\(P\\) and \\(Q\\) be statement variables. ::: dBox ::: definitionT Definition 1.18 (Negation). The statement \\\"not \\(P\\) \\\", denoted by \\(\\lnot\\ P\\) , is true when \\(P\\) is false. ::: ::: ::: dBox ::: definitionT Definition 1.19 (Conjunction). The statement \\\" \\(P\\) and \\(Q\\) \\\", denoted by \\(P\\ \\land\\ Q\\) , is true when, and only when, both \\(P\\) and \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.20 (Disjunction). The statement \\\" \\(P\\) or \\(Q\\) \\\", denoted by \\(P\\ \\lor\\ Q\\) , is true when at least one of \\(P\\) or \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.21 (Conditional). The statement \\\"If \\(P\\) then \\(Q\\) \\\", denoted by \\(P\\ \\rightarrow\\ Q\\) , is false when \\(P\\) is true and \\(Q\\) is false; otherwise it is true. ::: ::: ::: dBox ::: definitionT Definition 1.22 (Biconditional). The statement \\\" \\(P\\) if and only if \\(Q\\) \\\", denoted by \\(P\\ \\leftrightarrow\\ Q\\) , is true exactly when either \\(P\\) and \\(Q\\) are both true, or when \\(P\\) and \\(Q\\) are both false. ::: ::: ::: list The order of operations goes from \\(\\lnot\\) , \\(\\land\\) , \\(\\lor\\) , \\(\\rightarrow\\) then \\(\\leftrightarrow\\) , if no parenthesis are present. ::: The five logical connectives has the following truth table: \\(P\\) \\(\\lnot\\ P\\) T F F T \\(P\\) \\(Q\\) \\(P\\ \\land\\ Q\\) \\(P\\ \\lor\\ Q\\) \\(P\\ \\rightarrow\\ Q\\) \\(P\\ \\leftrightarrow\\ Q\\) T T T T T T T F F T F F F T F T T F F F F F T T However, when statement is always true or false we can define to be the following: ::: dBox ::: definitionT Definition 1.23 (Tautology). A tautology is a statement form that is always true regardless of the truth values of the individual statement substituted for its statement variables ::: ::: ::: dBox ::: definitionT Definition 1.24 (Contradiction). A contradiction is a statement form that is always false regardless of the truth values of the individual statements substituted for its statement variables. ::: ::: ::: exampleT Example 1.13 . Show that \\(P\\ \\lor\\ \\lnot\\ P\\) is a tautology and that \\(P\\ \\land\\ \\lnot\\ P\\) is a contradiction. \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\lor\\ \\lnot\\ P\\) *T* *F* *T* *F* *T* *T* \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\land\\ \\lnot\\ P\\) *T* *F* *F* *F* *T* *F* ::: -3ex -0.1ex -.4ex 0.5ex .2ex Conditional Statements When asked to rewrite the following sentences using \\(\\rightarrow\\) , they would use the phrases \\\"necessary condition\\\" and \\\"sufficient condition\\\", which implies: The statement \\(P\\) is a necessary condition for \\(Q\\) means that \\(Q\\ \\rightarrow\\ P\\) . The statement \\(P\\) is a sufficient condition for \\(Q\\) means that \\(P\\ \\rightarrow\\ Q\\) . For a conditional statement, \\(P\\ \\rightarrow\\ Q\\) , we can form two related statements: ::: dBox ::: definitionT Definition 1.25 (Inverse). The inverse of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ P\\ \\rightarrow\\ \\lnot\\ Q\\) . ::: ::: ::: dBox ::: definitionT Definition 1.26 (Converse). The converse of \\(P\\ \\rightarrow\\ Q\\) is \\(Q\\ \\rightarrow\\ P\\) . ::: ::: ::: dBox ::: definitionT Definition 1.27 (Contrapositive). The contrapositive of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ Q\\ \\rightarrow\\ \\lnot\\ P\\) . ::: ::: ::: exampleT Example 1.14 . Rewrite the following statement: \\\"If \\(n\\) is prime, then \\(n\\) is odd or \\(n\\) is \\(2\\) \\\" using the: Inverse: If \\(n\\) is not prime, then \\(n\\) is not odd and \\(n\\) is not \\(2\\) . Converse: If \\(n\\) is odd or \\(n\\) is \\(2\\) , then \\(n\\) is a prime. Contrapositive: If \\(n\\) is not odd and \\(n\\) is not \\(2\\) , then \\(n\\) is not a prime. ::: ::: list When writing the negation, watch out for other logical connectives present in the sentences, like \\(\\land\\) and \\(\\lor\\) and make sure to apply De Morgan's laws properly. ::: -3ex -0.1ex -.4ex 0.5ex .2ex Logical Equivalences Two statements are logically equivalent, denoted by \\(\\equiv\\) , if they share the same truth table. ::: tBox ::: theoremeT Theorem 1.1 . Let \\(P\\) and \\(Q\\) be statement variables: Commutative law: Associative law: Distributive law: Double negation law: De Morgan's law: Idempotent law: Implication law: ::: ::: You might notice some similarities to the previous theorem for unions and intersections of a set. ::: tabu *2X[c] Logical Equivalences & Set Properties \\ ::: flushleft For all statements variables \\(P\\) , \\(Q\\) , and \\(R\\) : ::: & ::: flushleft For all sets \\(A\\) , \\(B\\) , and \\(C\\) : ::: \\ (a) \\(P\\ \\lor\\ Q \\equiv Q\\ \\lor\\ P\\) (b) \\(P\\ \\land\\ Q \\equiv Q\\ \\land\\ P\\) & (a) \\(A\\ \\cup\\ B = B\\ \\cup\\ A\\) (b) \\(A\\ \\cap\\ B = B\\ \\cap\\ A\\) \\ (a) \\(P\\ \\land\\ (Q \\land R) \\equiv (P\\ \\land Q)\\ \\land\\ R\\) (b) \\(P\\ \\lor\\ (Q \\lor R) \\equiv (P\\ \\lor Q)\\ \\lor\\ R\\) & (a) \\(A\\ \\cap\\ (B\\ \\cap\\ C) = (A\\ \\cap\\ B)\\ \\cap\\ C\\) (b) \\(A\\ \\cup\\ (B\\ \\cup\\ C) = (A\\ \\cup\\ B)\\ \\cup\\ C\\) \\ (a) \\(P\\ \\land\\ (Q\\ \\lor R) \\equiv (P\\ \\land Q)\\ \\lor\\ (P\\ \\land\\ R)\\) (b) \\(P\\ \\lor\\ (Q\\ \\land R) \\equiv (P\\ \\lor Q)\\ \\land\\ (P\\ \\lor\\ R)\\) & (a) \\(A\\ \\cap\\ (B\\ \\cup\\ C) = (A\\ \\cap\\ B)\\ \\cup\\ (A\\ \\cap\\ C)\\) (b) \\(A\\ \\cup\\ (B\\ \\cap\\ C) = (A\\ \\cup\\ B)\\ \\cap\\ (A\\ \\cup\\ C)\\) \\ (a) \\(\\lnot(\\lnot\\ P) \\equiv P\\) & (a) \\((A^c)^c = A\\) \\ (a) \\(P\\ \\lor\\ P \\equiv P\\) (b) \\(P\\ \\land\\ P \\equiv P\\) & (a) \\(A\\ \\cup\\ A = A\\) (b) \\(A\\ \\cap\\ A = A\\) \\ (a) \\(\\lnot(P\\ \\lor\\ Q) \\equiv \\lnot\\ P\\ \\land\\ \\lnot\\ Q\\) (b) \\(\\lnot(P\\ \\land\\ Q) \\equiv \\lnot\\ P\\ \\lor\\ \\lnot\\ Q\\) & (a) \\((A\\ \\cup\\ B)^c = A^c\\ \\cap\\ B^c\\) (b) \\((A\\ \\cap\\ B)^c = A^c\\ \\cup\\ B^c\\) \\ ::: ::: tabu *2X[c] (a) \\(P\\ \\lor\\ (P\\ \\land\\ Q) \\equiv P\\) (b) \\(P\\ \\land\\ (P\\ \\lor\\ Q) \\equiv P\\) & (a) \\(A\\ \\cup\\ (A\\ \\cap\\ B) = A\\) (b) \\(A\\ \\cap\\ (A\\ \\cup\\ B) = A\\) \\ ::: -3ex -0.1ex -.4ex 0.5ex .2ex Predicates and Quantified Statements We initially discussed that a logical statement is either true or false. So something like \\\" \\(x > 5\\) \\\" is not a statement, but what we define to be a predicate. ::: dBox ::: definitionT Definition 1.28 (Predicate). A predicate \\(P(x)\\) is a sentence that contains a finite number of variables and becomes a statement when specific values are substituted for variables. ::: ::: ::: dBox ::: definitionT Definition 1.29 (Domain). The domain \\(D\\) of a predicate variable is the set of all values that may be substituted in place of variable. ::: ::: A way to obtain statements from predicates is to add quantifiers. Let \\(P(x)\\) be a predicate and \\(D\\) the domain of \\(x\\) . ::: dBox ::: definitionT Definition 1.30 (Universal quantifier). The symbol \\(\\forall\\) is read as \\\"for every\\\" or \\\"for all.\\\" A universal statement is a statement of the form, \\(\\forall x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for every \\(x\\) in \\(D\\) . It is defined to be false if, and only if, \\(P(x)\\) is false for at least one \\(x\\) in \\(D\\) . ::: ::: ::: dBox ::: definitionT Definition 1.31 (Existential quantifier). The symbol \\(\\exists\\) is read as \\\"there exists\\\" or \\\"there is.\\\" An existential statement is a statement of the form, \\(\\exists x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for at least one \\(x\\) in \\(D\\) . It is false if, and only if, \\(P(x)\\) is false for all \\(x\\) in \\(D\\) . ::: ::: ::: exampleT Example 1.15 . Rewrite the following sentences using quantifiers. \\\"Every real number has a non-negative square\\\" rewritten as \\(\\forall x \\in \\mathbb{R},\\ x^2 \\geq 0\\) \\\"There is a positive integer whose square is equal to itself\\\" rewritten as \\(\\exists y \\in \\mathbb{Z}^+,\\ y^2 = y\\) ::: One final topic to discuss is the negation of universal and existential quantifiers. ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Universal statement). The negation of a universal statement is logically equivalent to an existential statement. Symbolically, ( \\(\\lnot(\\forall x \\in D,\\ P(x)) \\equiv \\exists x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Existential statement). The negation of a existential statement is logically equivalent to an universal statement. Symbolically, ( \\(\\lnot(\\exists x \\in D,\\ P(x)) \\equiv \\forall x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: exampleT Example 1.16 . Negate the following statements from the previous exercise. \\\"There exists a real number that has a negative square\\\" or \\(\\exists x \\in \\mathbb{R},\\ x^2 < 0\\) \\\"Every positive integer whose square is not equal to itself\\\" or \\(\\forall x \\in \\mathbb{Z}^+,\\ y^2 \\neq y\\) ::: ::: list In example 1.11, the first statement is true, while, in example 1.12, the first statement is false, since squaring a number will always be positive. ::: There are some cases where certain statements contain multiple quantifiers. ::: exampleT Example 1.17 . Negate the following statement: \\(\\forall x \\in \\mathbb{Z},\\ \\exists y \\in \\mathbb{Z},\\ y > x\\) . We can simply think of \\\" \\(\\exists y \\in \\mathbb{Z},\\ y > x\\) \\\" as the predicate \\(P(x)\\) . ( \\(\\lnot(\\forall x \\in \\mathbb{Z},\\ P(x)) \\equiv \\exists x \\in \\mathbb{Z},\\ \\lnot\\ P(x)\\) \\) Then we take the negation of \\(P(x)\\) . ( \\(\\lnot\\ P(x) \\equiv \\lnot(\\exists y \\in \\mathbb{Z},\\ y > x) \\equiv \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) And we put it all together. ( \\(\\exists x \\in \\mathbb{Z},\\ \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Order of Quantifiers In a statement containing both \\(\\forall\\) and \\(\\exists\\) , changing the order of the quantifiers can significantly change the meaning of the statementyou read from left to right. For example, the following statements are equivalent: \\( \\(\\forall x,\\ \\forall y,\\ P(x) \\equiv \\forall y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\exists y,\\ P(x) \\equiv \\exists y,\\ \\exists x,\\ P(x)\\) \\) However, now consider mixed quantifier and they are no longer equivalent: \\( \\(\\forall x,\\ \\exists y,\\ P(x) \\not\\equiv \\exists y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\forall y,\\ P(x) \\not\\equiv \\forall y,\\ \\exists x,\\ P(x)\\) \\) For a more dramatic context. let's use the following analogy: \\(\\forall x,\\ \\exists y,\\ x \\text{ loves } y\\) and \\(\\exists y,\\ \\forall x,\\ x \\text{ loves } y\\) Note that both statements looks identical, except the order of quantifiers. However, the first statement means everybody loves somebodywhom that somebody could be a different person for each \\(x\\) , ... whereas the second statement means there is one individual who is loved by all people. We can also visualize this using a directed graph, as a shown below: Relations and Functions \u00b6 -4ex -1ex -.4ex 1ex .2ex Binary Relations A relations is something that involves two different sets. A special kind of binary relation is a function. Suppose there are some elements inside \\(X\\) and \\(Y\\) , we can visualize an arrow diagram for our relation. The graph corresponds to the following ordered pairs: \\(\\{(x_1,y_1), (x_2, y_2), (x_3, y_3)\\}\\) ::: dBox ::: definitionT Definition 2.1 (Binary Relation). For sets \\(X\\) and \\(Y\\) , a binary relation \\(R\\) from \\(X\\) to \\(Y\\) is a subset of \\(X \\times Y\\) . Hence, \\(R\\) is a set of ordered pairs \\((x,y)\\) with \\(x \\in X\\) and \\(y \\in Y\\) . We write \\(x \\mathrel{R}y\\) if \\((x,y) \\in R\\) . We say that \\(R\\) is a binary relation on \\(X\\) if \\(X = Y\\) ; that is, \\(R\\subseteq X \\times X\\) . ::: ::: A relation can also be drawn as a directed graph which will prove to be more useful when explaining the properties of relation. Using the same set of ordered pairs from before, it can drawn as such: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Relation There exists various properties that some, but not all, relations satisfy. Let \\(R\\) be a binary relation on a set \\(A\\) : ::: dBox ::: definitionT Definition 2.2 (Reflexive). The relation \\(R\\) is reflexive, if for all \\(x \\in A\\) , \\(x \\mathrel{R}x\\) . \\(R\\) is reflexive \\(\\Leftrightarrow\\) for every \\(x\\) in \\(A\\) , \\((x,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.3 (Symmetric). The relation \\(R\\) is symmetric, if for all \\(x,y \\in A\\) , if \\(x \\mathrel{R}y\\) , then \\(y \\mathrel{R}x\\) . \\(R\\) is symmetric \\(\\Leftrightarrow\\) for every \\(x\\) and \\(y\\) in \\(A\\) , if \\((x,y) \\in R\\) then \\((y,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.4 (Transitive). The relation \\(R\\) is transitive, if for all \\(x,y,z \\in A\\) , if \\(x \\mathrel{R}z\\) and \\(y \\mathrel{R}z\\) , then \\(x \\mathrel{R}z\\) . \\(R\\) is transitive \\(\\Leftrightarrow\\) for every \\(x\\) , \\(y\\) , and \\(z\\) in \\(A\\) , if \\((x,y) \\in R\\) and \\((y,z) \\in R\\) then \\((x,z) \\in R\\) ::: center ::: ::: ::: ::: exampleT Example 2.1 . Suppose we have a set \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0), (0,1), (0,3), (1,0), (1,1), (2,2), (3,0), (3,3)\\}\\) \\) Let's draw out the directed graph. ::: center ::: \\(R\\) is reflexive because there is a loop for each element, as shown for \\((0,0), (1,1), (2,2), (3,3)\\) . \\(R\\) is symmetric because there is an arrow going from one point then back to the other, as shown for \\((0,1), (1,0)\\) and \\((0,3), (3,0)\\) \\(R\\) is not transitive because there's no arrow from \\((1,3)\\) or \\((3,1)\\) which would otherwise make it transitive. ::: Sometimes the set of relation is not given, instead the definition is provided. For example, suppose we have set a \\(A = \\{1,2,3,4\\}\\) and the relation \\(R\\) defined as follows: Properties of Div: \\((x,y) \\in R \\text{ if } x \\mid y\\) The line \\\" \\(\\mid\\) \\\" means \\(x\\) divisible by \\(y\\) . Equivalently, you can think of it as \\(y = xk\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,2),(1,3),(1,4),(2,2),(2,4),(3,3),(4,4)\\}\\) \\) Properties of Congruence Modulo n: \\((x,y) \\in R \\text{ if } n \\mid (x-y)\\) Sometimes, it referred to as \\(x \\equiv y \\ (\\mathrm{mod}\\ n)\\) . Let use \\(n = 2\\) , as an example. We can use the expression before, \\(x-y = 2k\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,3),(2,2),(2,4),(3,1),(3,3),(4,2),(4,4)\\}\\) \\) Properties of \\\"Greater Than\\\": \\((x,y) \\in R \\text{ if } x > y\\) In this case, we are only interested where \\(x\\) is greater than \\(y\\) . The set of relation would be: \\( \\(R = \\{(2,1),(3,1),(3,2),(4,1),(4,2),(4,3)\\}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Equivalence Relation There's are cases when the relation has all three properties discussed. ::: dBox ::: definitionT Definition 2.5 (Equivalence relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is an equivalence relation if it is reflexive, symmetric and transitive, \\((A, R)\\) . ::: ::: A notable example would be the equality sign, which might not make senses at first, but let's break it down. For all real numbers \\(x\\) and \\(y\\) , consider the relation \\(R\\) defined as follows \\( \\((x,y) \\in R \\text{ if } x = y\\) \\) Is \\(R\\) reflexive? Yes, because it is implying the statement \\(x = x\\) , which is true; every real number is equal to itself. Is \\(R\\) symmetric? Yes, because if \\(x = y\\) , then \\(y = x\\) is also true; if one number is equal to another, then the second is equal to the first. Is \\(R\\) transitive? Yes, because if \\(x = y\\) and \\(y = z\\) , then \\(x = z\\) is true; if one real number equals a second and the second equals a third, then the first must also equal the third. Let's introduce this new idea called the equivalence class, as an extension to equivalence relation. Suppose \\(A\\) is a set and \\(R\\) is an equivalence relation on \\(A\\) . ::: dBox ::: definitionT Definition 2.6 (Equivalence class). For each element \\(a\\) in \\(A\\) , the equivalence class of \\(a\\) , denoted \\([a]\\) is the set of all elements \\(x\\) in \\(A\\) such that \\(x\\) is related to \\(a\\) by \\(R\\) . ( \\(= \\{x \\in A \\mid x \\mathrel{R}a \\}\\) \\) ::: ::: ::: list Some textbooks may define it as \\([a] = \\{x \\in A \\mid a \\mathrel{R}x \\}\\) instead. Either one works, as you may recall, it's transitive. ::: ::: exampleT Example 2.2 . Let \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0),(0,4),(1,1),(1,3),(2,2),(3,1),(3,3),(4,0),(4,4)\\}\\) \\) Determine all the elements related to \\(0\\) or \\(x \\mathrel{R}0\\) , where \\(x \\in A\\) . ( \\(0 \\mathrel{R}0,\\ 4 \\mathrel{R}0\\) \\) The elements \\(0\\) and \\(4\\) are essentially what makes up the equivalence class of \\(0\\) , which more formally can be written as ( \\([0] = \\{x \\in A \\mid x \\mathrel{R}0\\} = \\{0,4\\}\\) \\) Thus, find the equivalence class for the rest of the elements of \\(A\\) . ( \\([1] = \\{x \\in A \\mid x \\mathrel{R}1\\} = \\{1,3\\}\\) \\) ( \\([2] = \\{x \\in A \\mid x \\mathrel{R}2\\} = \\{2\\}\\) \\) ( \\([3] = \\{x \\in A \\mid x \\mathrel{R}3\\} = \\{1,3\\}\\) \\) ( \\([4] = \\{x \\in A \\mid x \\mathrel{R}4\\} = \\{0,4\\}\\) \\) Note that \\([4] = [0]\\) and \\([1] = [3]\\) , so the distinct equivalent classes are ( \\(\\{0,4\\}, \\{1,3\\}, \\text{ and } \\{2\\}\\) \\) ::: If you notice, the distinct equivalence classes of an equivalence relation \\(R\\) on a set \\(A\\) form a partition of \\(A\\) . Likewise, the converse is also true, which will discuss in the next section. ::: tBox ::: theoremeT Theorem 2.1 . Let \\(R\\) be an equivalence relation on a set \\(A\\) , where we assume \\(A \\neq \\varnothing\\) . For all \\(x \\in A\\) , \\([x] \\neq \\varnothing\\) . If \\(x \\mathrel{R}y\\) , then \\([x] = [y]\\) . If \\((x,y) \\notin R\\) , then \\([x]\\ \\cap\\ [y] = \\varnothing\\) . ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Relation Induced by Partition As a recap, a partition of a set \\(A\\) is a finite or infinite collection of nonempty, mutually disjoint subsets whose union is \\(A\\) or illustratively, ::: center ::: where \\(A_i\\ \\cap\\ A_j = \\varnothing\\) whenever \\(i \\neq j\\) and \\(A_1\\ \\cup\\ \\cdots\\ \\cup\\ A_6 = A\\) . ::: dBox ::: definitionT Definition 2.7 . Let \\(P\\) be a partition of a set \\(A\\) and define the binary relation \\(R_P\\) , so that \\(x \\mathrel{R}y\\) if \\(x\\) and \\(y\\) are in the same part of the partition. ::: ::: Again, this definition might sound confusing, but let's use the set \\(A\\) from the previous example to get a better understanding. ::: exampleT Example 2.3 . Let \\(A = \\{0,1,2,3,4\\}\\) and consider the following be a partition \\(P\\) of \\(A\\) : ( \\(P = \\{\\{0,1\\},\\{2,3\\},\\{4\\}\\} % A_1 = \\{0,1\\} \\hspace{1cm} A_2 = \\{2,3\\} \\hspace{1cm} A_3 = \\{4\\}\\) \\) Let's consider the first subset of the partition, \\(A_1 = \\{0,1\\}\\) . Since both \\(0\\) and \\(1\\) are in the subset, we can form the following relation. ( \\(0 \\mathrel{R}1,\\ 1\\mathrel{R}0\\) \\) We can also do the following, since they are still in the same part of the partition. ( \\(0 \\mathrel{R}0,\\ 1 \\mathrel{R}1\\) \\) Thus, for the second subset, \\(A_2 = \\{2,3\\}\\) , we can form a similar set of relations. ( \\(2 \\mathrel{R}3,\\ 3 \\mathrel{R}2,\\ 2 \\mathrel{R}2,\\ 3 \\mathrel{R}3\\) \\) Finally for the third subset of the partition, \\(A_3 = \\{4\\}\\) , which only has one. ( \\(4 \\mathrel{R}4\\) \\) Hence, combining all of them makes up the binary relation \\(R_P\\) : ( \\(R_P = \\{\\underbrace{(0,1),(1,0),(0,0),(1,1)}_{\\{0,1\\}},\\underbrace{(2,3),(3,2),(2,2),(3,3)}_{\\{2,3\\}},\\underbrace{(4,4)}_{\\{4\\}}\\}\\) \\) ::: The fact is that a relation induced by a partition of a set satisfies all three properties, or in other words, \\(R_P\\) is an equivalence relation. -3ex -0.1ex -.4ex 0.5ex .2ex Partial Orders Partial orders provide one way of ranking objects. To define them, we define another property of relation. ::: dBox ::: definitionT Definition 2.8 (Antisymmetric). Let \\(R\\) be a binary relation on a set \\(A\\) . We say \\(R\\) is antisymmetric, if and only if, for every \\(a\\) and \\(b\\) in \\(A\\) , if \\(a \\mathrel{R}b\\) and \\(b \\mathrel{R}a\\) , where \\(a = b\\) . ::: ::: Equivalently, we are saying that the relation should not have the following: ::: center ::: For example, the relation on the left is not antisymmetric, whereas the relation on the right is antisymmetric. ::: center ::: ::: dBox ::: definitionT Definition 2.9 (Partial order relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is a partial order relation if it is reflexive, antisymmetric, and transitive. ::: ::: We call \\((A,R)\\) a partial ordered set or poset. We use \\(\\preceq\\) to represent the relation \\(R\\) . Two elements \\(x\\) and \\(y\\) are comparable if either \\(x \\mathrel{R}y\\) or \\(y \\mathrel{R}x\\) . Otherwise, the elements are incomparable. A set of pairwise incomparable is called an antichain . If every pair of element is comparable, then \\(R\\) is a total order , linear order , or chain . Back to previous example, the relation on the left has two elements, \\(0\\) and \\(1\\) , that are incomparable. The relation on the right is a linear order with three elements. ::: dBox ::: definitionT Definition 2.10 (Hasse diagram). A diagram used to represent partial order relations with sufficient information and an implied upward orientation. ::: ::: To obtain a Hasse diagram, proceed as follows: Construct a digraph (or directed graph) of the poset \\((A,R)\\) , so that all arrows point upward, except the loops. Eliminate all loops. Eliminate all directed edges that are redundant because of transitivity. Eliminate the arrows on the directed edge. Suppose we have a set \\(A = \\{1,2,3,9,18\\}\\) with the div relation, \\(x \\mid y\\) . The digraph of this poset has the following appearance on the left and the Hasse diagram on the right: ::: center ::: We can reference some extremal elements of posets using the following definitions. ::: dBox ::: definitionT Definition 2.11 . Let \\(R\\) be a partial order of on a set \\(A\\) : An element \\(u\\) is a least element if \\(\\forall x \\in A\\) , \\(u \\mathrel{R}x\\) . An element \\(v\\) is a greatest element if \\(\\forall x \\in A\\) , \\(x \\mathrel{R}v\\) . An element \\(u\\) is a minimal element if there does not exist an element \\(x \\in A \\smallsetminus \\{u\\}\\) , such that \\(x \\mathrel{R}u\\) . Alternatively, there exists no element \\\"below\\\" it. An element \\(u\\) is a maximal element if there does not exist an element \\(x \\in A \\smallsetminus \\{v\\}\\) , such that \\(v \\mathrel{R}x\\) . Alternatively, there exists no element \\\"above\\\" it. ::: ::: Note the difference between least and minimal element, likewise, with greatest and maximal element. A least element is a minimal, but a minimal element need not to be a least element. Similarly, a greatest element is a maximal, but a maximal element need not to be a greatest element. A poset can have at most one least and greatest element, but it may have more than one minimal or maximal element. Look at the following digraph of each poset: ::: center ::: -4ex -1ex -.4ex 1ex .2ex Introduction to Graph Theory Graph theory one of the most important topics in discrete mathematics. You may have heard of the famous bridge puzzle, known as \\\"The Seven Bridges of K\u00f6nigsberg\\\", which consists of the following problem. ::: problem Problem 2.1 . Is it possible to find a route through the graph that starts and ends at some vertex, one of \\(A\\) , \\(B\\) , \\(C\\) , or \\(D\\) , and traverses each edge exactly once? ::: center ::: ::: We can further simply this to the following graph. If you compare the two diagrams, they are equivalently the same. ::: center ::: If you aren't already aware of it, this problem is impossible to solve and it all relates back to graph theory. Let's start off by defining what a graph is. ::: dBox ::: definitionT Definition 2.12 (Graph). A graph \\(G\\) is a pair consisting of a vertex set \\(V(G)\\) and an edge set \\(E(G)\\) containing pairs of distinct vertices, such that \\(G = (V,E)\\) ::: ::: ::: list The bridge graph is an undirected graphthe order of the two connected vertices is not important, oppose to a directed graph. ::: The order of a graph \\(G\\) is \\(|V(G)|\\) and its size is \\(|E(G)|\\) . We can use the bridge example, to define our vertex and edge set, \\( \\(V(G) = \\{A,B,C,D\\} \\hspace{1cm} E(G) = \\{\\{A,B\\},\\{A,B\\},\\{B,D\\},\\{B,D\\},\\{A,C\\},\\{B,C\\},\\{C,D\\}\\}\\) \\) where \\(|V(G)| = 4\\) and \\(|E(G)| = 7\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Degrees There's no solution to the bridge problem because suppose we start and end at vertex \\(A\\) , then the degree of the other three vertices \\(B\\) , \\(C\\) , and \\(D\\) must be even. ::: dBox ::: definitionT Definition 2.13 (Degree). Given a graph with vertex \\(v\\) , the degree of \\(v\\) , denoted by \\(\\deg_G(v)\\) is the number of edges incident to \\(v\\) . ::: ::: From the graph earlier, we can define the degree of each vertex: \\(\\deg(A) = 3\\) , \\(\\deg(B) = 5\\) , \\(\\deg(C) = 3\\) , and \\(\\deg(D) = 3\\) . ::: dBox ::: definitionT Definition 2.14 (Neighbor set). The neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(\\{w \\in V(G) \\mid v,w \\in E(G)\\}\\) ; any \\(w \\in V(G)\\) is called a neighbor of \\(v\\) , equivalently \\(\\deg_G(v) = |N_G(v)|\\) . The closed neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(N_G(v)\\ \\cup\\ \\{v\\}\\) . ::: ::: ::: list As you'll see later on, the subscript \\(G\\) is often removed, if the graph is clear from context. ::: The definition may sound more confusing that what it is suppose to mean, but it is essentially the set of all vertices which are adjacent to \\(v\\) . Refer to the example below. Another set of terms will discuss in this section the minimum and maximum degree of a graph \\(G\\) . ::: dBox ::: definitionT Definition 2.15 (Minimum and Maximum Degree). The integer \\(\\delta(G)\\) is the minimum degree of \\(G\\) and the integer \\(\\Delta(G)\\) is the maximum degree of \\(G\\) . ::: ::: ::: exampleT Example 2.4 . Suppose we have the following graph \\(G\\) is defined as: ::: center ::: For each vertex: \\(\\deg(1) = 1\\) , \\(\\deg(2) = 2\\) , \\(\\deg(3) = 3\\) , \\(\\deg(4) = 2\\) , and \\(\\deg(5) = 2\\) . The vertex \\(3\\) has three vertices adjacent to it, which are \\(2\\) , \\(4\\) , and \\(5\\) , thus \\(N(3) = \\{2,4,5\\}\\) and \\(|N(3)| = \\deg(3) = 3\\) . The closed neighbor set includes \\(3\\) , since \\(N[3] = \\{2,4,5\\}\\ \\cup\\ \\{3\\} = \\{2,3,4,5\\}\\) . From 1. we can see the minimum degree is \\(1\\) and the maximum degree is \\(3\\) , thus \\(\\delta(G) = 1\\) and \\(\\Delta(G) = 3\\) . ::: The following theorem is pretty simple, but helpful in drawing conclusions when analyzing graphs. ::: tBox ::: theoremeT Theorem 2.1 (Handshake Theorem). Let \\(G\\) be a graph, then ( \\(\\sum_{u \\in V(G)} = \\deg_G(u) = 2|E(G)|\\) \\) ::: ::: The equivalent definition of this in text means, the sum of degree of all the vertices is twice the number of edges contained in it. Using the example from before, there's five vertices and a total degree of \\(10\\) . In general, any undirected graph has an even number of vertices of odd degree. -3ex -0.1ex -.4ex 0.5ex .2ex Walks, Paths, and Cycles Travel in a graph is accomplished by moving from one vertex to another along a sequence of adjacent edges. There various definitions used to describe the movement in a graph. ::: dBox ::: definitionT Definition 2.16 . Let \\(G\\) be a graph and let \\(u\\) and \\(v\\) be vertices in \\(G\\) . Walk: A walk from \\(u\\) to \\(v\\) is a finite sequence of adjacent vertices of \\(G\\) . Thus a walk has the form, \\(W = (v_0,v_1,v_2, \\cdots, v_n)\\) if \\(\\{v_i,v_{i+1}\\} \\in E(G)\\) for \\(0 \\leq i < n\\) , where \\(v_0 = u\\) and \\(v_n = v\\) . The length of a walk is the number of edges in \\(W\\) . Closed Walk: A closed walk is a walk that starts and ends at the same vertex, where \\(v_0 = v_n\\) . Path: A path is a walk without repeated vertices. The path of order \\(n \\geq 1\\) is denoted by \\(P_n\\) . Cycle: A cycle is a closed walk of at least \\(3\\) or more vertices. The cycle of order \\(n \\geq 3\\) is denoted by \\(C_n\\) . ::: ::: A path, \\(P_n\\) , where \\(n \\geq 1\\) , consist of \\(n\\) vertices and \\(n-1\\) edges. ::: center ::: A cycle, \\(C_n\\) , where \\(n \\geq 3\\) , consists of \\(n\\) vertices and \\(n\\) edges. ::: center ::: ::: exampleT Example 2.5 . Suppose we have the graph below, define the following walks: ::: center ::: ::: multicols 2 \\((v_0,v_1,v_2)\\) is a path, \\(P_3\\) , as neither vertices nor edges are repeated. ::: center ::: \\((v_2,v_6,v_4,v_5,v_1,v_2)\\) is a cycle, \\(C_5\\) , as we do not repeat a vertex nor edge, but started and ended at the same vertex. ::: center ::: ::: ::: We can now define what we mean by the diameter of a graph \\(G\\) . Note this will get a bit confusing. First, let's start with the distance between two vertices \\(u\\) and \\(v\\) . ::: dBox ::: definitionT Definition 2.17 (Graph distance). The distance between \\(u\\) and \\(v\\) is the minimum length of the paths in \\(G\\) connecting them, denoted by \\(d_G(u,v)\\) or \\(d(u,v)\\) , if \\(G\\) is clear from context. ::: ::: ::: list The distance between \\(u\\) and \\(v\\) is the same regardless of the start position, such that \\(d(u,v) = d(v,u)\\) . You can think of the distance as the number of edges traversed. ::: So in theory, what does this exactly mean? Let's use a simple graph for now. ::: center ::: The distance between \\(u\\) and \\(v\\) is \\(d(u,v) = 2\\) , as that's the only path to traverse. Another example, let's try and use the graph from before to see if you fully understand the definition. ::: center ::: As you can see, there are many possible paths from \\(u\\) to \\(v\\) , shown in red. Some examples are: ::: center ::: However, remember that we are only interested in the shortest path, which in this case is \\(d(u,v) = 3\\) . It's really important that you understand how to define the shortest path in \\(G\\) given two vertices \\(u\\) and \\(v\\) , as it will help in understand the next definition. ::: dBox ::: definitionT Definition 2.18 (Graph diameter). The diameter of a graph \\(G\\) is defined as ( \\(\\mathop{\\mathrm{diam}}(G) = \\max\\{d_G(v,w) \\mid v,w \\in V(G)\\}\\) \\) Equivalently, the largest number of vertices which must be traversed in order to travel from one vertex to another. ::: ::: ::: exampleT Example 2.6 . Consider the following graph \\(G\\) , determine the diameter of the graph: ::: center ::: We can start off by focusing on vertex \\(a\\) : ::: center ::: Then on vertex \\(b\\) : ::: center ::: Repeat for the rest of the vertices, for all \\(v,w \\in V(G)\\) . If you did it properly, you will see that \\(d(a,d)\\) and \\(d(f,e)\\) have the maximum distance. ( \\(\\mathop{\\mathrm{diam}}(G) = 3\\) \\) ::: You can also refer to this [video]{.underline} if you want a visual explanation of this example. -3ex -0.1ex -.4ex 0.5ex .2ex Subgraphs and Induced Subgraphs As the name suggests, the prefix \\\"sub\\\" refers to it being subsets of another graph. ::: dBox ::: definitionT Definition 2.19 (Subgraph). A graph \\(H\\) is said to be a subgraph of a graph \\(G\\) , written \\(H \\subseteq G\\) , if, and only if, \\(V(H) \\subseteq V(G)\\) and \\(E(H) \\subseteq E(G)\\) . The graph \\(H\\) is a spanning subgraph of \\(G\\) if \\(V(H) = V(G)\\) . ::: ::: It may be easier to understand visually. Suppose we have the following graph \\(G\\) : ::: center ::: As an example, each of the graphs are variations of graph \\(H\\) , which are subgraphs of graph \\(G\\) : ::: center ::: We only consider it to be spanning subgraph if and only if \\(V(H) = V(G)\\) , which in this case, if \\(V(H) = \\{a,b,c,d,e,f,g,h,i,j\\}\\) , which none of them are. On the other hand, these graphs are spanning subgraphs of \\(G\\) : ::: center ::: A final concept is induced subgraphs, which consists of the following property. ::: dBox ::: definitionT Definition 2.20 (Induced subgraph). If \\(S \\subseteq V(G)\\) , then the subgraph of \\(G\\) induced by \\(S\\) , denoted by \\(G[S]\\) , has vertices \\(S\\) and edges are those of \\(G\\) with endpoints in \\(S\\) . ::: ::: Note that none of the subgraphs shown so far are considered induced subgraphs of \\(G\\) . Though, we can modify it slightly to make it an induced subgraph, indicated by the red line. ::: center ::: Just focus on the induced subgraph in the far left. Notice how every possible edge that exists in graph \\(G\\) between the vertices, \\(\\{b,d,e,f,g,h,i,j\\}\\) , exists in this subgraph, thus making it an induced subgraph. Likewise, how the edge \\(a,b\\) is not here because \\(a\\) is not in the subset of vertices. -3ex -0.1ex -.4ex 0.5ex .2ex Special Graphs We consider some important examples of graphs. One important class of graphs consists of those that do not have any loops or parallel edges. ::: dBox ::: definitionT Definition 2.21 (Simple graph). A simple graph is a graph that does not have any loops or parallel edges. In a simple graph, an edge with endpoints \\(v\\) and \\(w\\) is denoted \\(\\{v,w\\}\\) . ::: ::: The following graphs can be depicted as simple graphs: ::: center ::: Another important class of graphs consists of those that are \"complete\" in the sense. ::: dBox ::: definitionT Definition 2.22 (Complete graph). A complete graph on \\(n\\) vertices, denoted \\(K_n\\) , is a simple graph with \\(n\\) vertices and exactly one edge connecting each pair of distinct vertices. ::: ::: This may sound confusing at first, but look at the following graphs below to get a general idea of how it works. ::: center ::: Then another class of graphs we can consider are complete bipartite graphs, which are as follows. ::: dBox ::: definitionT Definition 2.23 (Complete bipartite graph). A complete bipartite graph, denoted \\(K_{m.n}\\) , is a simple graph that has its vertex set partitioned into two subsets of \\(m\\) and \\(n\\) vertices. ::: ::: Note the difference between \\(K_5\\) and \\(K_{3,2}\\) , where there are no edges connected between \\(v_1\\) , \\(v_2\\) and \\(v_3\\) or similarly with \\(w_1\\) and \\(w_2\\) , which would otherwise just make it a complete graph. ::: center ::: The dashed-lines highlights the partition of two subsets. If there are some edges not present between the parts, then the graph is just a bipartite graph. -4ex -1ex -.4ex 1ex .2ex Functions If you think about it, functions are a special kind of binary relation. By definition: ::: dBox ::: definitionT Definition 2.24 (Function). A function \\(f\\) is a binary relation from sets \\(X\\) and \\(Y\\) . For each \\(x \\in X\\) , there is a unique \\(y \\in Y\\) , so that \\(x \\mathrel{f}y\\) . We write \\(f(x) = y\\) for \\(x \\mathrel{f}y\\) and say \\\" \\(f\\) of \\(x\\) equals \\(y\\) .\\\" Denoted as \\(f: X \\to Y\\) , it is a relation from \\(X\\) , the domain of \\(f\\) , to \\(Y\\) , the co-domain of \\(f\\) . ::: ::: Using an arrow diagrams, we can define a function by the following: Every element of \\(X\\) has an arrow that points to an element in \\(Y\\) . No element of \\(X\\) has two arrows that points to two different elements in \\(Y\\) . We can also define the range of a function \\(f\\) , which is a subset of the co-domain. ::: dBox ::: definitionT Definition 2.25 (Range). Let \\(f: X \\to Y\\) be a function. The range of \\(f\\) is: ( \\(\\{y \\in Y \\mid \\text{for some } x \\in X, f(x) = y\\}\\) \\) ::: ::: ::: exampleT Example 2.7 . Suppose a function \\(f\\) is defined from \\(X\\) to \\(Y\\) by the following arrow diagram: ::: center ::: The domain of \\(f = \\{a,b,c\\}\\) and co-domain of \\(f = \\{1,2,3,4\\}\\) . The range of \\(f\\) equals \\(\\{2,4\\}\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Functions Acting on Sets You can consider the set of images in \\(Y\\) of all the elements in a subset of \\(X\\) and the set of inverse images in \\(X\\) of all the elements in a subset of \\(Y\\) . ::: dBox ::: definitionT Definition 2.26 (Image and Inverse Image). Let \\(f: X \\to Y\\) be a function and \\(A \\subseteq X\\) and \\(C \\subseteq Y\\) . The image of \\(A\\) , denoted by \\(f(A)\\) , is ( \\(f(A) = \\{y \\in Y \\mid \\text{for some } x \\in A, f(x) = y\\}\\) \\) The inverse image of \\(C\\) , denoted by \\(f^{-1}(C)\\) , is ( \\(f^{-1}(C) = \\{x \\in X \\mid f(x) \\in C\\}\\) \\) ::: ::: Using the example from before, it might be easier to understand what these definition represent: \\(f(a) = 2\\) , \\(f(b) = 4\\) , and \\(f(c) = 2\\) \\(f^{-1}(1) = \\varnothing\\) , \\(f^{-1}(2) = \\{a,c\\}\\) , \\(f^{-1}(3) = \\varnothing\\) , and \\(f^{-1}(4) = \\{b\\}\\) Note that \\(C\\) is a subset of \\(Y\\) , so you maybe asked to find the inverse image of more than one element. For example, let \\(C = \\{2,4\\}\\) , then \\(f^{-1}(C) = \\{a,b,c\\}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex One-to-One, Onto and Inverse Functions We'll now discuss two important properties that functions may satisfy: the property of being one-to-one and the property of being onto. ::: dBox ::: definitionT Definition 2.27 (One-to-one). A function \\(f: X \\to Y\\) is one-to-one (or injective ) if for all \\(x_1,x_2 \\in X\\) , such that \\(x_1 \\neq x_2\\) , we have \\(f(x_1) \\neq f(x_2)\\) . If any two distinct elements of \\(X\\) are sent to two distinct elements of \\(Y\\) , then it is one-to-one. ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.28 (Onto). A function \\(f: X \\to Y\\) is one-to-one (or surjective ) if for all \\(y \\in Y\\) , there exists \\(x \\in X\\) , such that \\(f(x) = y\\) . If each elements of \\(Y\\) equals \\(f(x)\\) for at least one \\(x\\) in \\(X\\) , then it is onto. ::: center ::: ::: ::: For finite sets, it is pretty easy to determine whether a function is one-to-one or onto, just from the arrow diagram above. The tricky part comes when analyzing a function for an infinite set. ::: exampleT Example 2.8 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = 4x - 1\\) . Is \\(f\\) is onto? In order to prove something is onto, we need show that there exists a real number \\(x\\) , such that \\(y = 4x-1\\) . We can do so, by solving for \\(x\\) in this case, where \\(x = (y - 1)/4\\) . If you notice, \\(y\\) is not restricted to anything, meaning that \\(y\\) can be any \\(\\mathbb{R}\\) . Equivalently, we are saying \\\"There exists an \\(x\\) (which we determined from 3.), which is being mapped \\(\\forall y \\in \\mathbb{R}\\) .\\\" ::: ::: exampleT Example 2.9 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = x^2\\) . Is \\(f\\) one-to-one? Consider \\(f(x_1) = f(x_2)\\) , We need to show that \\(x_1 = x_2\\) . If we fail to prove this, then it is not one-to-one. Alternatively, \\((2)^2 = (-2)^2 = 4\\) , but \\(2 \\neq -2\\) , so the function is not one-to-one. ::: There also exist functions which satisfy both properties discussed. ::: dBox ::: definitionT Definition 2.29 (One-to-one correspondence). A function \\(f: X \\to Y\\) is bijective (or bijective) if it is both one-to-one and onto. ::: center ::: ::: ::: This will aid us in defining a type of function known as the inverse function, which undoes the action of \\(f\\) . It sends each element of \\(Y\\) back to the element of \\(X\\) where it came from. ::: dBox ::: definitionT Definition 2.30 (Inverse function). Let \\(f: X \\to Y\\) be a one-to-one correspondence. The inverse function of \\(f\\) is denoted by \\(f^{-1}\\) , where \\(f^{-1}: Y \\to X\\) . ::: center ::: ::: ::: Obtaining the inverse function should be something you are all familiar with, it just been described in a different setting, which is by solving for \\(x\\) . ::: exampleT Example 2.10 . Is \\(y = 4x-1\\) a bijection (or a one-to-one correspondence) from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) ? If so, find the inverse \\(f^{-1}(x)\\) . To check if it is one-to-one, show that \\(f(x_1) = f(x_2) \\Longrightarrow x_1 = x_2\\) . The function is one-to-one, since \\(4x_1 - 1 = 4x_2 - 1 \\Longrightarrow x_1 = x_2\\) . To check if it is onto, we must show that for every \\(y \\in \\mathbb{R}\\) , there is an \\(x \\in \\mathbb{R}\\) , such that \\(y = f(x)\\) . We can prove this, by solving for \\(x\\) . ( \\(x = \\frac{y+1}{4}\\) \\) There exists some \\(x \\in \\mathbb{R}\\) , such that \\(f(x) = y\\) , which makes the function is onto. To get the inverse function, we can use the equation we obtained in the previous one and interchange \\(x\\) and \\(y\\) : ( \\(y = f^{-1}(x) = \\frac{x+1}{4}\\) \\) ::: Number Theory and Combinatorics \u00b6 -4ex -1ex -.4ex 1ex .2ex Elementary Number Theory The underlying content of this section consists of properties of integers, rational numbers, and real numbers. -3ex -0.1ex -.4ex 0.5ex .2ex Rational Number Sums, differences, and products of integers are integers, but most quotients of integers are not integers, rather known as: ::: dBox ::: definitionT Definition 3.1 (Rational Number). A real number \\(r\\) is rational, if and only if, it can be expressed as a quotient of two integers with a nonzero denominator. ( \\(r \\text{ is rational } \\Leftrightarrow \\exists \\text{ integers } a \\text{ and } b \\text{ such that } r = \\frac{a}{b} \\text{ and } b \\neq 0\\) \\) ::: ::: Some examples of rational numbers are \\(0\\) , \\(1\\) , and \\(1/3\\) . While a real number that is not rational is called an irrational numbers, like \\(\\pi\\) , \\(e\\) , and \\(\\sqrt{2}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Parity Number The parity of an integer focuses on the property of a number being even or odd. ::: dBox ::: definitionT Definition 3.2 (Parity). An integer \\(x\\) is even if \\(x = 2a\\) , for some integer \\(a\\) . An integer \\(x\\) is odd if \\(x = 2b + 1\\) , for some integer \\(b\\) . ::: ::: ::: tBox ::: theoremeT Theorem 3.1 (Properties of parity). Let \\(x\\) and \\(y\\) be integers: If \\(x\\) and \\(y\\) are both, then so are \\(x + y\\) and \\(xy\\) . If \\(x\\) is even and \\(y\\) is odd, then \\(x + y\\) is odd. If \\(x\\) is odd and \\(y\\) is odd, then \\(x + y\\) is even. We have that \\(x\\) is even if and only if \\(x^2\\) is even. ::: ::: Alternatively, the arithmetic on the even and odd numbers can depicted as: Even \\(+\\) Even \\(\\to\\) Even Even \\(+\\) Odd \\(\\to\\) Odd Odd \\(+\\) Odd \\(\\to\\) Even Even \\(\\times\\) Even \\(\\to\\) Even Even \\(\\times\\) Odd \\(\\to\\) Even Odd \\(\\times\\) Odd \\(\\to\\) Odd -4ex -1ex -.4ex 1ex .2ex Divisors Divisors play a central role in number theory, as they help us define prime numbers and the Euclidean algorithm, which will discuss after. ::: dBox ::: definitionT Definition 3.3 (Divisibility). The notation \\(a \\mid b\\) is read \\\" \\(a\\) divides \\(b\\) \\\", if \\(b = ak\\) , for some integer \\(k\\) . We can say that ::: description \\(b\\) is a multiple of \\(a\\) , or \\(b\\) is divisible by \\(a\\) , or \\(a\\) is a factor of \\(b\\) , or \\(a\\) is a divisor of \\(b\\) ::: ::: ::: One useful trick for checking divisibility when it comes to large numbers is by checking if the sum of its individual digit is divisible by instead. Refer to the example below. ::: exampleT Example 3.1 . Is \\(94\\;417\\;898\\;732\\) divisible by \\(9\\) ? Let's start calculating the sum of its digits ( \\(9+4+4+1+7+8+9+8+7+3+2 = 62\\) \\) Since there exist no integers \\(k\\) which satisfy the following equation, \\(62 = 9k\\) , it is not divisible by \\(9\\) . ::: Following this, we can now define what the greatest common divisor of two integers is. ::: dBox ::: definitionT Definition 3.4 (Greatest common divisor). The greatest common divisor of nonzero integers \\(a\\) and \\(b\\) , denoted \\(\\gcd(a,b)\\) , is the largest integer that divides both \\(a\\) and \\(b\\) . ::: ::: Note that every integer divides \\(0\\) , since \\(0 = a \\times 0\\) where \\(k = 0\\) . ::: exampleT Example 3.2 . Find the \\(\\gcd(72,63)\\) : The divisor of \\(72\\) are \\(\\{1,2,3,6,8,9,12,18,24,36,72\\}\\) . The divisor of \\(63\\) are \\(\\{1,3,7,9,21,63\\}\\) . The largest integer that divides both integers is \\(9\\) , such that \\(9 \\mid 72\\) and \\(9 \\mid 63\\) . Hence, \\(\\gcd(72,63) = 9\\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Prime Numbers We can also use divisors as a way to define a prime number. ::: dBox ::: definitionT Definition 3.5 (Prime number). A number \\(p > 1\\) is prime if, and only if, its only positive integer divisors are \\(1\\) and itself. Otherwise, it's composite. ::: ::: The most comprehensive statement about divisibility of integers is contained in the factorization of integers theorem. ::: tBox ::: theoremeT Theorem 3.1 (Fundamental Theorem of Arithmetic). Every integer \\(n>1\\) equals a product of primes, which is unique up to the ordering of factors. ::: ::: For example, \\(72\\) can be written as, \\(2^33^2\\) , where \\(2\\) and \\(3\\) are prime numbers. In a way you can think of each number as made up of building blocks of prime number. -3ex -0.1ex -.4ex 0.5ex .2ex Euclidean Algorithm The Euclidean algorithm provides us a simpler method for deriving the greatest common divisor of two positive integers. It is based on these two key facts: If \\(r\\) is a positive integer, then \\(\\gcd(r,0) = r\\) . If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , and if \\(q\\) and \\(r\\) are integers such that \\(a = bq + r\\) , then \\(\\gcd(a,b) = gcd(b,r)\\) The first fact should be fairly straightforward to understand. The second fact might be harder to understand, so let's use an example. ::: exampleT Example 3.3 . Find the \\(\\gcd(72,63)\\) : Let \\(a = 72\\) and \\(b = 63\\) , then we rewrite it in the form of \\(72 = 63q + r\\) . You can think of \\(q\\) as the quotient and \\(r\\) as the remainder. ( \\(72 = 63(1) + 9\\) \\) Then \\(\\gcd(72,63) = \\gcd(63,9)\\) . Let \\(a = 63\\) and \\(b = 9\\) , where \\(63 = 9q + r\\) . ( \\(63 = 9(7) + 0\\) \\) Then \\(\\gcd(63,9) = \\gcd(9,0)\\) . Using the first key fact, we know \\(\\gcd(9,0) = 9\\) . ::: Alternatively, we can set \\(q = 1\\) , where \\(r = a - b\\) . ::: tBox ::: theoremeT Theorem 3.1 (Reducing gcd). If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , then: ( \\(\\gcd(a,b) = \\gcd(b, a-b)\\) \\) ::: ::: ::: list Keep in mind, \\(\\gcd(a,b) = \\gcd(b,a)\\) , so alternatively can be written as \\(\\gcd(a, b-a)\\) . Also note that \\(\\gcd(a,b) = \\gcd(|a|,|b|)\\) . ::: Using this theorem, we can continue to reduce greatest common divisor, until we either have an integer simple enough to work with or it results in the form of \\(\\gcd(r,0)\\) . -4ex -1ex -.4ex 1ex .2ex Linear Diophantine Equation We focus on equations with integer coefficients and integer solutions. ::: dBox ::: definitionT Definition 3.6 (Linear Diophantine equation (LDE)). An equation of the form \\(ax + by = c\\) , or can also be written as \\(ax = c\\ (\\mathrm{mod}\\ b)\\) , where \\(a\\) , \\(b\\) , \\(c\\) , \\(x\\) , \\(y \\in \\mathbb{Z}\\) . ::: ::: In this section, we try to answer the following problem. ::: problem Problem 3.1 . Given an integer \\(a\\) , \\(b\\) , and \\(c\\) , does a solution \\((x,y)\\) exist and how can you find a solution to an LDE? ::: So how exactly can we prove whether a solution exists? It all relates back to the greatest common divisor, more specifically \\(\\gcd(a,b)\\) . A useful theorem which we'll use in proving this is B\u00e9zout's identity. ::: tBox ::: theoremeT Theorem 3.1 (B\u00e9zout's identity). Let \\(a\\) and \\(b\\) be nonzero integers, and let \\(d = \\gcd(a,b)\\) . Then there exist integers \\(m\\) and \\(n\\) that satisfy: ( \\(ma + nb = d\\) \\) ::: ::: So for an LDE to have a solution, \\(c\\) must be a multiple of \\(d\\) , denoted as \\(d \\mid c\\) , which can be written more formally as: ::: tBox ::: theoremeT Theorem 3.1 (Check if solution exists for LDE). Let \\(d = \\gcd(a,b)\\) . The LDE \\(ax + by = c\\) has a solution if and only if \\(d \\mid c\\) . ::: ::: ::: exampleT Example 3.4 . Does a solution exists to the LDE \\(60x + 33y = 9\\) ? Compute the greatest common divisor of \\(60\\) and \\(33\\) . ( \\(\\gcd(60,33) = \\gcd(33,27) = \\gcd(27,6) \\gcd(6,3) = \\gcd(3,0) = 3\\) \\) Determine whether a solution exists, if \\(\\gcd(12,8) = 4 \\mid 68\\) or there exists an integer \\(k\\) , where ( \\(3 \\mid 9 \\Longleftrightarrow 9 = 3k\\) \\) which is true for \\(k = 3\\) , thus a solution exist. ::: Once it's determined a solution exists for the LDE, we want to way to derive the general solution \\((x,y)\\) , which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Solutions to LDE). Let \\(d = \\gcd(a,b)\\) where \\(a \\neq 0\\) and \\(b \\neq 0\\) . If \\((x,y) = (x_0,y_0)\\) is a solution to the LDE \\(ax + by = c\\) , then all solutions are given by ( \\(x = x_0 + \\frac{b}{d}t \\qquad y = y_0 - \\frac{a}{d}t\\) \\) for all \\(t \\in \\mathbb{Z}\\) . We may write the solution set as ( \\(\\Big\\{\\Big(x_0 + \\frac{b}{d}t\\Big), \\Big(y_0 - \\frac{a}{d}t\\Big) : t \\in \\mathbb{Z}\\Big\\}\\) \\) ::: ::: ::: exampleT Example 3.5 . Solve the following LDE \\(60x + 33y = 9\\) or \\(60x = 9\\ (\\mathrm{mod}\\ 33)\\) . As we proved before, a solution exists where \\(d = \\gcd(60,33) = 3\\) . First step is a finding a solution to B\u00e9zout's identity, where there exists integers \\(m\\) and \\(n\\) that satisfy \\(am + bn = d\\) . ( \\(60m + 33n = 3\\) \\) such that \\(m = 5\\) and \\(n = -9\\) satisfies this equation. Refer to the section after. Then to get \\(x_0\\) and \\(y_0\\) , we multiply \\(m\\) and \\(n\\) by \\(3\\) to get the original LDE equation. ( \\(3\\big[60n + 33n = 3\\big] = 60(3n) + 33(3n) = 9\\) \\) such that \\(x_0 = 15\\) and \\(y_0 = -27\\) satisfies the equation \\(60x_0 + 33y_0 = 9\\) . Finally, we just apply the theorem to get the general solution for all \\(t \\in \\mathbb{Z}\\) ( \\(x = 15 + \\frac{33}{3}t = 15 + 11t \\qquad y = -27 - \\frac{60}{3}t = -27 - 20t\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Solving for Initial Solution The trickiest part is solving for integers \\(m\\) and \\(n\\) which satisfies B\u00e9zout identity to get the initial solutions \\(x_0\\) and \\(y_0\\) . Let's use the previous example, \\(60m + 33n = 9\\) . The process involves by working backwards through your steps in the Euclidean Algorithm: \\(60 = 33(1) + 27\\) \\(33 = 27(1) + 6\\) \\(27 = 6(4) + 3\\) Reformat the Euclidean Algorithm, such that \\(r = a - bq\\) : \\(27 = 60 - 33(1) \\hfill (3.5)\\) \\(6 = 33 - 27(1) \\hfill (3.6)\\) \\(3 = 27 - 6(4) \\hfill (3.7)\\) Now use substitution. Refer to the text in red: \\( \\(3 = 27 - {\\color{red}6}(4)\\) \\) \\( (3 = 27 - \\big {\\color{red}33 - 27(1)}\\big \\tag*{Substitute \\(6\\) using Eq. \\(3.6\\) }\\) \\) \\( \\(3 = 27 - 33(4) + 27(4) \\tag*{Expand}\\) \\) \\( \\(3 = {\\color{red}27}(5) - 33(4) \\tag*{Combine like terms}\\) \\) \\( (3 = \\big {\\color{red}60 - 33(1)}\\big - 33(4) \\tag*{Substitute \\(27\\) using Eq. 3.5}\\) \\) \\( \\(3 = 60(5) - 33(5) - 33(4) \\tag*{Expand}\\) \\) \\( \\(3 = 60(5) - 33(9) \\tag*{Combine like terms}\\) \\) \\( \\(3 = 60(5) + 33(-9)\\) \\) -4ex -1ex -.4ex 1ex .2ex Congruence In number theory, congruence is nothing more than a statement about divisibility. ::: dBox ::: definitionT Definition 3.7 (Congruence). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) , if \\(a\\) is congruent to \\(b\\) modulo \\(n\\) , then we write ( \\(a \\equiv b\\ (\\mathrm{mod}\\ n)\\) \\) which provides that \\(n \\mid (a - b)\\) . ::: ::: So what information can we take away from \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) ? \\(a\\) and \\(b\\) have the same remainder when divided by \\(n\\) \\(a = kn + b\\) for some integer \\(k\\) \\(n \\mid (a-b)\\) There are also some useful algebraic properties of congruences. ::: tBox ::: theoremeT Theorem 3.1 . If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(c \\equiv d \\ (\\mathrm{mod}\\ n)\\) , then: \\(a + c \\equiv b + d \\ (\\mathrm{mod}\\ n)\\) \\(a - c \\equiv b - d \\ (\\mathrm{mod}\\ n)\\) \\(ac \\equiv bd \\ (\\mathrm{mod}\\ n)\\) ::: ::: The algebra of congruence is sometime referred to as clock arithmetic. For example, we can represent modulo \\(12\\) as a clock (where \\(0\\) represents \\(12\\) ). ::: center ::: The clock demonstrate that every integer is congruent to at least one of \\(0 \\dots 11\\) modulo \\(12\\) (row highlighted in pink). Just like a clock, when we go over \\(12\\) , we start over at \\(1\\) , and so the same thing applies with modulo, where \\(1 \\equiv 13 \\ (\\mathrm{mod}\\ 12)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Congruence Class Refer back to [section 2.1.2]{.underline} for a recap. Congruence is another type of equivalence relationsa relation that satisfies all three: Reflexive: \\(a \\equiv a \\ (\\mathrm{mod}\\ n)\\) Symmetric: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) , then \\(b \\equiv a \\ (\\mathrm{mod}\\ n)\\) Transitive: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(b \\equiv c \\ (\\mathrm{mod}\\ n)\\) , then \\(a \\equiv c \\ (\\mathrm{mod}\\ n)\\) As a result, we can form equivalence classes, or otherwise known as ::: dBox ::: definitionT Definition 3.8 (Congruence class). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . The congruence class of modulo \\(n\\) is ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b \\equiv a \\ (\\mathrm{mod}\\ n)\\}\\) \\) Note that ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b = a + kn \\text{ for } k \\in \\mathbb{Z}\\}\\) \\) ::: ::: For example, in congruence modulo \\(2\\) , we have \\([0]_2 = \\{0, \\pm 2, \\pm 4, \\pm 6, \\cdots\\}\\) \\([1]_2 = \\{\\pm 1, \\pm 3, \\pm 5, \\pm 7, \\cdots\\}\\) The congruence classes of \\(0\\) and \\(1\\) are, respectively, the sets of even and odd integers. ::: tBox ::: theoremeT Theorem 3.1 (Equality of congruence classes). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . We then have that \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) if and only if ( \\(_n = [b]_n\\) \\) ::: ::: Referring back to the clock diagram, in congruence modulo \\(12\\) , we have: \\( \\([0]_{12} = [12]_{12} = [24]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\} = \\{\\cdots -24, -12, 0, 12, 24, \\cdots\\}\\) \\) You may have notice that the distinct congruence classes are \\([0], [1], \\cdots, [11]\\) : \\([0]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\}\\) \\([1]_{12} = \\{1, 1 \\pm 12, 1 \\pm 24, \\cdots\\}\\) \\(\\vdots\\) \\([11]_{12} = \\{11, 11 \\pm 12, 11 \\pm 24, \\cdots\\}\\) In congruence modulo \\(n\\) , we can say for \\(n > 2\\) , the distinct congruence classes are \\([0], [1], \\cdots, [n-1]\\) . -4ex -1ex -.4ex 1ex .2ex Principles of Counting We'll start off with the fundamentals, that is counting. Of course, most people know how to count, but combinatorics applies mathematical operations to count quantities that are much too large to be counted the conventional way. -3ex -0.1ex -.4ex 0.5ex .2ex Sum Rule Combinatorics is often concerned with how things are arrangeda way objects could be grouped. One of the most basic rules regarding arrangements is the rule of sum. ::: tBox ::: theoremeT Theorem 3.1 (Sum rule). Suppose that we are given disjoint sets \\(X\\) and \\(Y\\) . If \\(|X| = m\\) and \\(|Y| = n\\) , then ( \\(|X\\ \\cup\\ Y | = m + n\\) \\) ::: ::: Then we can generalized this theorem for more than two disjoints sets. ::: tBox ::: theoremeT Theorem 3.1 (Generalized sum rule). If we are given pairwise disjoint sets \\(X_i\\) , where \\(1 \\leq i \\leq m\\) , so that \\(|X_i| = m\\) , then ( \\(\\bigg|\\bigcup\\limits_{i=1}^m X_i \\bigg| = \\sum_{i=1}^m |X_i|\\) \\) ::: ::: So what about sets that are not disjoint? For example, we have two sets \\(X = \\{1,2,3\\}\\) and \\(Y = \\{2,3,4\\}\\) . If we use the sum rule, where \\(|X| = 3\\) and \\(|Y| = 3\\) , we should get: \\( \\(|X| + |Y| = 3 + 3 = 6\\) \\) However, \\(X\\ \\cup\\ Y = \\{1,2,3,4\\}\\) , where \\(|X \\cup Y| = 4 \\neq 6\\) , such that \\( \\(|X\\ \\cup\\ Y| < |X| + |Y|\\) \\) which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Boole's inequality). For any sets \\(A_i\\) , we have that ( \\(\\bigg|\\bigcup\\limits_{i=1}^m A_i \\bigg| \\leq \\sum_{i=1}^m |A_i|\\) \\) ::: ::: Equivalently, we can rewrite the sum rule as \\(|A\\ \\cup\\ B| = |A| + |B| - |A\\ \\cap\\ B|\\) , where we subtract the cardinality of elements that are common. For disjoint sets that is \\(\\varnothing\\) , compared to joint sets, resulting in \\(\\leq\\) . And so this brings the final theorem which will cover in this section. ::: tBox ::: theoremeT Theorem 3.1 (Principle of Inclusion-Exclusion). Let \\(X_1, X_2, \\dots, X_n\\) be finite sets. We then have that ( \\(\\bigg|\\bigcup\\limits_{1\\leq1\\leq n} X_i \\bigg| = |X_1| + |X_2| + \\dots + |X_n|\\) \\) ( \\(\\qquad\\qquad \\:-\\;|X_1\\ \\cap\\ X_2| - |X_1\\ \\cap\\ X_3| - \\dots - |X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;|X_1\\ \\cap\\ X_2\\ \\cap\\ X_3| + |X_1\\ \\cap\\ X_2\\ \\cap\\ X_4| + \\dots + |X_{n-2}\\ \\cap\\ X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;(-1)^{n-1}|X_1\\ \\cap\\ X_2\\ \\cap\\ \\cdots\\ \\cap\\ X_n|\\) \\) ::: ::: We can break this principle line-by-line: Take the sum of the cardinalities of the sets, as you would in a disjoint union. Subtract off the cardinalities of the pairwise intersections of the sets Add the cardinalities of the triple intersections and so on. The signs \\((-1)^{n-1}\\) depend on the parity of the number of sets intersected. For example, if there is three set in the intersection ::: center ::: \\( \\(|A\\ \\cup\\ B\\ \\cup\\ C| = |A| + |B| + |C| - |A\\ \\cap\\ B| - |A\\ \\cap\\ C| - |B\\ \\cap\\ C| + |A\\ \\cap\\ B\\ \\cap C|\\) \\) You can also refer to the visualization below, if you have trouble understanding the reason behind it ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Product Rule Another basic rules regarding arrangements is the rule of product. ::: tBox ::: theoremeT Theorem 3.1 (Product rule). If a task \\(X\\) can be performed in \\(m\\) ways and a task \\(Y\\) can be performed in \\(n\\) ways, then we have that \\(X\\) and \\(Y\\) can be performed together in \\(mn\\) ways. ::: ::: One example is with cards. How many cards are in a standard deck of cards? ::: center ::: Equivalently, you can think of the suit of the card and the rank of the card as two tasksthere are \\(4\\) suits and \\(13\\) . The product rule, \\(4 \\times 13\\) , tells us there are \\(52\\) card. Likewise, we can generalize this theorem for more than two tasks. ::: tBox ::: theoremeT Theorem 3.1 (Generalized product rule). If tasks \\(X_i\\) can be performed in \\(m_i\\) ways where \\(i \\leq i \\leq n\\) , then we have ( \\(m_1m_2 \\cdots m_n\\) \\) way tasks can be performed together. ::: ::: ::: exampleT Example 3.6 . How many ways can you make a license plate with three-digit number (not including zero) and three letters? For starters, let's focus on the three-digit number. For each digit, there can be nine different ways, \\(1 \\dots 9\\) , we can choose a number. ( \\(9 \\times 9 \\times 9 = 9^3\\) \\) Then for the three letters, for each choice, there can be twenty-six different ways, \\(a \\dots z\\) , we can choose a letter. ( \\(26 \\times 26 \\times 26 =26^3\\) \\) In total, there are \\(9^326^3\\) different ways we can make them. ::: -3ex -0.1ex -.4ex 0.5ex .2ex The Pigeonhole Principle The Pigeonhole Principle is a simple, but powerful tool when counting objects. The metaphors used to describe the principle typically vary, but they all follow the same analogy of inserting a finite set into a smaller finite set. You can think of it like this, if \\(n\\) pigeons fly into \\(m\\) pigeonholes and \\(n > m\\) , then at least one hole must contain two or more pigeons. ::: center ::: ::: tBox ::: theoremeT Theorem 3.1 (The Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , then at least one container must contain more than one item. ::: ::: In hindsight, it is pretty is obvious, thus, we move immediately to applicationsranging from the totally obvious to the extremely subtle. ::: exampleT Example 3.7 . If you choose a set of three non-negative integers, must there be at least two who are both even or both odd. Yes, because we have three items and only two container (odd or even), therefore one container must contain more than one item, which could be odd or even. ::: ::: exampleT Example 3.8 . Let \\(A = \\{1,2,3,4,5,6,7,8\\}\\) . If five integers are selected form \\(A\\) , must at least one pair of the integers have a sum of \\(9\\) ? You can think of the five selected integers as the number of items, where \\(a_n\\) represent a distinct number in the set \\(A\\) . ( \\(a_1,\\ a_2,\\ a_3,\\ a_4, \\text{ and }\\ a_5\\) \\) Then, all the disjoint subsets that have a sum of \\(9\\) is our container. ( \\(\\{1,8\\},\\ \\{2,7\\},\\ \\{3,6\\}, \\text{ and}\\ \\{4,5\\}\\) \\) Applying the pigeonhole principle, because there are more items, \\(n = 5\\) , than there are containers, \\(m = 4\\) . Then at least one container must contain more than one item. In other words, at least one of the disjoint subsets will contain two distinct integers, which will have a sum of \\(9\\) . ::: A generalization of the pigeonhole principle states: ::: tBox ::: theoremeT Theorem 3.1 (Generalized Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , there is at least one container with \\(\\lceil n/m \\rceil\\) items. ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Permutation Before going over combinations, let's talk about permutation, where order matters. You can think of it as an ordered combinations. For example, the set of elements \\(a\\) , \\(b\\) , and \\(c\\) has six permutations: \\( \\(abc \\hspace{1cm} acb \\hspace{1cm} bac \\hspace{1cm} bca \\hspace{1cm} cab \\hspace{1cm} cba\\) \\) The number of permutations can be derived using the product rule. Suppose we have a set of \\(n\\) elements: For our first choice (or task), we have \\(n\\) ways of picking an element. For our second choice, we now have \\(n-1\\) ways of picking an element \\(\\vdots\\) For our \\(n\\) th choice, there's only one element left, so we only have \\(1\\) way of choosing an element. So by the product rule, there are \\( \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 = n!\\) \\) ways to perform the entire operation with no repetitions. In other words, there are \\(n!\\) permutations of a set of \\(n\\) elements. Suppose in the previous example, we want to know how many permutation there one only using two elements, instead of all three. We can define an ordered arrangement of \\(r\\) elements taken from the set of \\(n\\) elements as an \\(r\\) -permutation. ::: dBox ::: definitionT Definition 3.9 (Permutation). The number of \\(r\\) -permutations of a set of \\(n\\) elements is denoted \\(P(n,r)\\) . If \\(0 \\leq r \\leq n\\) , then ( \\(P(n,r) = n \\times (n-1) \\times \\cdots \\times (n - r + 1) = \\frac{n!}{(n-r)!}\\) \\) ::: ::: So now can calculate the \\(2\\) -permutation of \\(\\{a,b,c\\}\\) , resulting in \\( \\(P(3,2) = \\frac{3!}{(3-2)!} = \\frac{3!}{1!} = \\frac{6}{1} = 6\\) \\) which are \\( \\(ab \\hspace{1cm} ac \\hspace{1cm} ba \\hspace{1cm} bc \\hspace{1cm} ca \\hspace{1cm} cb\\) \\) -4ex -1ex -.4ex 1ex .2ex Combination We can now define an unordered arrangement of \\(r\\) elements of a set as an \\(r\\) -combination. ::: dBox ::: definitionT Definition 3.10 (Combination). Let \\(n\\) and \\(r\\) be non-negative integers, with \\(r \\leq n\\) . The symbol \\(\\binom{n}{r}\\) , read \\\" \\(n\\) chooses \\(r\\) \\\", denotes the number of subsets of size \\(r\\) that can be formed from a set of \\(n\\) elements. ::: ::: ::: list There's two ways to denote an \\(r\\) -combination, which is by \\(\\binom{n}{r}\\) or \\(C(n,r)\\) ::: Using the relation between permutation \\(P(n.r)\\) and combination, gives us an important formula: \\( \\(C(n.r) = \\binom{n}{r} = \\frac{n!}{(n-r)!r!}\\) \\) ::: exampleT Example 3.9 . Given a set of four people: Ann, Bob, Cyd, and Dan. List all the combinations that can be made with only three people. Note that order doesn't matter, so a subset consisting of \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) is the same as the subset consisting of \\(\\{\\) Dan, Cyd, Bob \\(\\}\\) . Following this fact, then the \\(3\\) -combination can be obtained by leaving one out of the elements of the set: \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Cyd \\(\\}\\) or alternatively ( \\(C(4,3) = \\binom{4}{3} = \\frac{4!}{(4-3)!3!} = \\frac{24}{6} = 4\\) \\) ::: As a follow up, there are special cases of combinations using this equation. ::: tBox ::: theoremeT Theorem 3.1 (Basic properties of combination). Let \\(n\\) be an integer: \\(\\displaystyle\\binom{n}{0} = \\binom{n}{n} = 1\\) \\(\\displaystyle\\binom{n}{1} = \\binom{n}{n-1} = n\\) \\(\\displaystyle\\binom{n}{2} = \\frac{n(n-1)}{2}\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Combinations There also some useful of identities that you can form using \\(C(n,r)\\) . They seem mysterious at first, but there's usually a good reason for them. Combinations have a recursive quality that is captured in the following theorem. ::: tBox ::: theoremeT Theorem 3.1 (Recursive property). For integers \\(n \\geq 1\\) and \\(r \\geq 1\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n - 1}{r - 1} + \\binom{n - 1}{r}\\) \\) ::: ::: You will often see this depicted as Pascal's formula. As an example, we can calculate \\(\\binom{6}{2}\\) using: \\( \\(\\binom{6}{2} = \\binom{5}{1} + \\binom{5}{2} = 5 + 10 = 15\\) \\) Another important property of combinations is their symmetry. ::: tBox ::: theoremeT Theorem 3.1 (Symmetry property). For integers \\(n \\geq 0\\) and \\(r \\geq 0\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n}{n - r}\\) \\) ::: ::: Using the previous example, we know that: \\( \\(\\binom{6}{2} = \\binom{6}{4}\\) \\) At first, it might not make sense, but it will prove to be useful in the next section, when we go over Pascal's trianglewhich is an arrangement of the combinations that makes them simple to remember. Lastly, we have this identity. ::: tBox ::: theoremeT Theorem 3.1 (Sum of squares combinations). For \\(n \\leq 0\\) , we have that ( \\(\\sum_{r=0}^n\\binom{n}{r}^2 = \\binom{2n}{n}\\) \\) ::: ::: As such, we can express the sum of squares as a single combination, shown below: \\( \\(\\sum_{r=0}^{25} \\binom{25}{r}^2 = \\binom{2 \\cdot 25}{25} = \\binom{50}{25}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Pascal's Triangle The rows of Pascal's triangle are indexed by non-negative integers: \\(0\\) , \\(1\\) , \\(2\\) , and etc. In the \\(n\\) th row, we have \\(\\binom{n}{r}\\) , where \\(1 \\leq r \\leq n\\) and the values of \\(r\\) increase from left to right. The following are the first seven rows of Pascal's triangle. Then, lastly we have this identity. ::: center ::: Though, they are more commonly represented as their integer counterpart. ::: center ::: You can see many patterns of how combinations are related in the triangle, such symmetry in a given row and the recursive property of combinations, which we discussed prior to this. One important use of combinations is in expanding polynomial expressions, such as \\((x + y)^n\\) . The binomial theorem generalizes this formula. ::: tBox ::: theoremeT Theorem 3.1 (Binomial theorem). Let \\(n\\) be non-negative integers and let \\(x\\) , \\(y\\) be variables. ( \\((x + y)^n = \\sum_{r = 0}^n\\binom{n}{r}x^{n-r}y^r\\) \\) ( \\(\\qquad\\quad\\ \\ = \\binom{n}{0}x^n + \\binom{n}{1}x^{n-1}y + \\binom{n}{2}x^{n-2}y^2 + \\cdots + \\binom{n}{n-1}xy^n + \\binom{n}{n}y^n\\) \\) ::: ::: In other words, the triangular arrangement of numbers gives us the coefficients in the expansion of any binomial expression. ::: exampleT Example 3.10 . Expand the following expression \\((x+y)^n\\) for \\(n=6\\) : Using the binomial theorem: ( \\((x + y)^6 = \\binom{6}{0}x^6 + \\binom{6}{1}x^5y + \\binom{6}{2}x^4y^2 + \\binom{6}{3}x^3y^3 + \\binom{6}{4}x^2y^4 + \\binom{6}{5}xy^5 + \\binom{6}{6}y^6\\) \\) If we refer back to Pascal's triangle, then we can easily substitute the binomial coefficient with its respective integers, as such: ( \\((x + y)^6 = x^6 + 6x^5y + 15x^4y^2 + 20x^3y^3 + 15x^2y^4 + 6xy^5 + y^6\\) \\) :::","title":"MTH314"},{"location":"W2022/MTH314/MTH314/#intro-to-sets-and-logic","text":"","title":"Intro to Sets and Logic"},{"location":"W2022/MTH314/MTH314/#the-language-of-sets","text":"One of the most important fundamentals revolves around sets. Definition 1.1 (Set). A set refers to a collection of objects, written in set-roster notation; using curly brackets \\(\\{ \\}\\) or set-builder notation, which will be discussed later. We use the following notation \\(\\in\\) to represent an element of a set and \\(\\notin\\) when it is not an element of a set. Example 1.1 . *Given \\(A = \\{1,2,3,4,5\\}\\) , we can write it as: \\(1 \\in A\\) \\(3 \\in A\\) \\(6 \\notin A\\) \\(\\pi \\notin A\\) There are certain sets of numbers referred to frequently, so they are given common set notations: Symbol Set of ... Example \\(\\mathbb{N}\\) Non-negative integers or natural numbers \\(\\{0,1,2,3, \\cdots\\}\\) \\(\\mathbb{Z}\\) Integers \\(\\{\\cdots, -2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q}\\) Rational numbers \\(\\{\\frac{p}{q} \\mid p,q \\in \\mathbb{Z}, q \\neq 0\\}\\) \\(\\mathbb{R}\\) Real numbers All of the above number sets Addition to a superscript \\(+\\) or \\(-\\) indicates that only positive or negative elements of the sets: \\(\\mathbb{Z}^+ = \\{1,2,\\cdots\\}\\) \\(\\mathbb{Z}^- = \\{-1,-2,\\cdots\\}\\) \\(\\mathbb{R}^+\\) is a set of positive real numbers \\(\\mathbb{R}^-\\) is a set of negative real numbers Another way to describe a set is using a set-builder notation, which characterize all the elements in the set by stating the property or properties they must have to be members. Definition 1.2 (Set-Builder Notation). *Let \\(S\\) denote a set and let \\(P(x)\\) be a property that elements of \\(S\\) may or may not satisfy. \\[\\{x \\in S \\mid P(x)\\}\\] We may define the following to be the set of all elements \\(x\\) in \\(S\\) such that \\(P(x)\\) is true. Example 1.2 . Let \\(A = \\{x \\in \\mathbb{Z} \\mid -2 < x < 5\\}\\) , the following set can be described as the set of all elements \\(x\\) are integers such that \\(-2 < x < 5\\) , where: \\( \\(A = \\{-1,0,1,2,3,4\\}\\) \\) * Definition 1.3 (Cardinality). *The cardinality of set denotes the number of elements of the set, usually denoted with a vertical bar on each side. Example 1.3 . Let \\(B = \\{2,5,7,9,12\\}\\) , then the cardinality of set \\(B\\) is: \\[|B| = 5\\] As defined earlier, a set is a collection of objects and so how do we define a set with no objects? Definition 1.4 (Empty Set). *A special set that contains no elements is called an empty set, which uses the notation \\(\\varnothing\\) . For an empty set, the cardinality would be \\(|\\varnothing| = 0\\) , since it contains no elements; \\(\\varnothing = \\{\\}\\) .","title":"The Language of Sets"},{"location":"W2022/MTH314/MTH314/#subsets","text":"A basic relation between sets is that of subset, which introduces a new notation \\(\\subseteq\\) for subset and \\(\\subsetneq\\) for proper subset. $$A \\subseteq B$$ $$A \\subsetneq B$$ Definition 1.5 (Subset). If \\(A\\) and \\(B\\) are sets, then \\(A\\) is a subset of \\(B\\) , written as \\(A \\subseteq B\\) , if and only if every element of \\(A\\) is also an element of \\(B\\) . Definition 1.6 (Proper subset). *If \\(A\\) is a subset of \\(B\\) , but not equal to \\(B\\) ; where there is at least one element of \\(B\\) not in \\(A\\) , then \\(A\\) is a proper subset of \\(B\\) , written as \\(A \\subsetneq B\\) . Some online sources or textbooks may use the following notation \\(\\subset\\) instead of \\(\\subsetneq\\) , to represent a proper subset. They are the same thing, but to avoid confusion, I'll be using this notation \\(\\subsetneq\\) . Note that every proper subset is a subset, but not every subset is a proper subset. So the diagram shown in (b) can also be used to demonstrate what a subset looks like. Example 1.4 . *Let \\(A = \\{1,2,3,4,5\\}\\) , \\(B = \\{1,3,5\\}\\) , \\(C = \\{2,3,5\\}\\) , and \\(D = \\{2,3,5\\}\\) then: \\(B \\subseteq A\\) and \\(B \\subsetneq A\\) \\(C \\subseteq A\\) and \\(C \\subsetneq A\\) \\(C\\subseteq D\\) and \\(D \\subseteq C\\) Two sets are equal when they share the exact element or in other words, subsets of each other: \\(C \\subseteq D\\) and \\(D \\subseteq C \\Longleftrightarrow C = D\\) .","title":"Subsets"},{"location":"W2022/MTH314/MTH314/#power-sets","text":"Previously, we went over what a subset is and so will use that to define the following set. Definition 1.7 (Power Set). Let \\(S\\) denote a set, the power set of \\(S\\) , denoted \\(\\mathcal{P}(S)\\) , is the set of all subsets of \\(S\\) . In general, for any finite set \\(S\\) , where \\(|S| = n\\) , we have that \\(|\\mathcal{P}(S)| = 2^n\\) . If you recall every proper subset is considered a subset and so we may use that in the following example. Any set is gonna have various subsets. Example 1.5 . Given a set \\(A = \\{1,3,5\\}\\) , we can define the following to be subsets of \\(A\\) : \\(\\{1\\} \\subseteq A\\) \\(\\{1,3\\} \\subseteq A\\) \\(\\{1,5\\} \\subseteq A\\) \\(\\{1,3,5\\} \\subseteq A\\) and so on The power set is basically all the possible subsets of \\(A\\) that can be formed given a set. The empty set \\(\\varnothing\\) and the set \\(A\\) is always included in the \\(\\mathcal{P}(A)\\) . Example 1.6 . Using the set \\(A = \\{1,3,5\\}\\) , then: \\[\\mathcal{P}(A) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,3\\}, \\{1,5\\}, \\{3,5\\}, \\{1,3,5\\}\\}\\] It becomes quite tricky when we use an empty set. The power set of an empty set, \\(\\mathcal{P}(\\varnothing) = \\{\\varnothing\\}\\) with cardinality of \\(1\\) . We can double check this by \\(|\\varnothing| = 0\\) and so \\(|\\mathcal{P}(\\varnothing)| = 2^0 = 1\\) or cardinality of \\(1\\) .","title":"Power Sets"},{"location":"W2022/MTH314/MTH314/#singleton-and-doubleton","text":"When a set only has one or two elements, it can be classified into one of two ways: Definition 1.8 (Singleton). A singleton is a set with a single element, \\(\\{x\\}\\) . Definition 1.9 (Doubleton). A doubleton (or unordered pair) is a set with two elements, \\(\\{x,y\\}\\) . An ordered pair uses round brackets instead of curly brackets to indicate that order matters, \\((x,y)\\) , which will be discussed more in later sections.","title":"Singleton and Doubleton"},{"location":"W2022/MTH314/MTH314/#operations-on-sets","text":"There are four main set operations to be discussed. Let \\(A\\) and \\(B\\) be subsets of a universal set \\(U\\) . Universal set, denoted \\(U\\) , is the collection of all objects that can occur as elements of the sets under consideration. All other sets are subsets of the universal set. Definition 1.10 (Union). The union of \\(A\\) and \\(B\\) , denoted \\(A \\cup B\\) , is the set of all elements that are at least in one of \\(A\\) or \\(B\\) . \\[A\\ \\cup\\ B = \\{x \\in U \\mid x \\in A \\text{ or } x \\in B\\}\\] Definition 1.11 (Intersection). The intersection of \\(A\\) and \\(B\\) , denoted \\(A \\cap B\\) , is the set of all elements that are common to both \\(A\\) and \\(B\\) . \\[A\\ \\cap\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\in B\\}\\] Definition 1.12 (Difference). The difference of \\(A\\) minus \\(B\\) , denoted \\(A \\smallsetminus B\\) , is the set of all elements that are in \\(A\\) and not in \\(B\\) . \\( \\(A\\ \\smallsetminus\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\notin B\\}\\) \\) * Definition 1.13 (Complement). The complement of \\(A\\) , denoted \\(A^c\\) , is the set of all elements in \\(U\\) that are not in \\(A\\) . \\[A^c = \\{x \\in U | x \\notin A\\}\\] There's one other operation not included, which is the symmetric difference, denoted \\(A \\triangle B\\) , is the set of elements which are in either of the sets A and B, but are not common to both \\(A\\) and \\(B\\) .","title":"Operations on Sets"},{"location":"W2022/MTH314/MTH314/#set-properties","text":"The following theorem consists of set identities, some of which you might be familiar with: Theorem 1.1 . *Let \\(A\\) , \\(B\\) , and \\(C\\) be subsets of a universal set \\(U\\) : Commutative law: Associative law: Distributive law: Complement law: Double complement law: De Morgan's law: Identity law: Idempotent law: Set difference law:","title":"Set Properties"},{"location":"W2022/MTH314/MTH314/#indexed-collection-of-sets","text":"The definitions of unions and intersections for more than two sets are very similar to the definitions for two sets, which we may generalize in the following way. Definition 1.14 (Union and Intersection). *Let \\(A_i\\) be a subset of a universal set \\(U\\) where \\(i \\geq 1\\) and given a non-negative integer \\(n\\) . \\[\\bigcup\\limits_{i=1}^n\\ A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } 1 \\leq i \\leq n\\}\\] \\[\\bigcap\\limits_{i=1}^n\\ A_i = A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } 1 \\leq i \\leq n\\}\\] ... and generalize to infinite unions and intersections. \\[\\bigcup\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } i \\geq 1 \\}\\] \\[\\bigcap\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } i \\geq 1\\}\\] Recap of the interval notation, \\((\\ )\\) means the endpoints are excluded and \\([\\ ]\\) means they are included. There are three specific types you'll encounter when solving for infinite intersections of a set: \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg[0, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (n, \\infty)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ [n, \\infty)\\) There might be slight variations of the questions, but it should give you a general idea on how to solve for them. Example 1.7 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : Let's think logically, by drawing out the number lines for the first three sets. If you notice the number line slowly decreases in size. As it reaches infinity, it will eventually reach \\(0\\) , which will be the only thing they all share in common. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = \\{0\\}\\] Example 1.8 . *Solve the following set, \\(\\bigcup\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : 1. We are now interested in all of the possible elements of the set. Using the number line from before, if you notice, \\((-1,1)\\) already contains all the elements in \\(A_2\\) and \\(A_3\\) . So we can say that \\((-1,1)\\) contains all the elements shared in the infinite set. \\[\\bigcup\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = (-1,1)\\] The notation for the open interval \\((a,b)\\) is identical to the notation for ordered pair \\((a,b)\\) , context makes it unlikely that the two will be confused. Example 1.9 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(0, \\frac{1}{i}\\Big)\\) : Let's draw out the number lines again for the first three sets. Likewise, as it reaches infinity, it will eventually reach \\(0\\) , however, note that \\(0\\) is not included, from the round brackets, so we say it's an empty set. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg) = \\varnothing\\] If instead it was \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\Big[0, \\frac{1}{i}\\Big)\\) , then our answer would be \\(\\{0\\}\\) , as it is included in the set. ::: ::: exampleT Example 1.10 . Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty)\\) : Let's draw out the first number line when \\(i = 1\\) and when \\(i = 5\\) . ::: center ::: As \\(i\\) goes from \\(1\\) to \\(5\\) , you notice that \\(\\{1,2,3,4\\}\\) is no longer common for all sets. As \\(i \\to \\infty\\) , logically there exists no real number which are common for all sets. We can conclude that the intersection of the infinite set is an empty set. ( \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty) = \\varnothing\\) \\) ::: ::: list Note that the answer doesn't change even if we have \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (i,\\infty)\\) , our answer is still \\(\\varnothing\\) . ::: If you are interested in the mathematical proof, click on the following link: [Archimedean property]{.underline} . -3ex -0.1ex -.4ex 0.5ex .2ex Cartesian Products on Sets In this section, we'll first focus on the notion of ordered pairs and how they work. ::: dBox ::: definitionT Definition 1.15 (Ordered pair). The symbol \\((a,b)\\) denotes the ordered pair with the specification that \\(a\\) is the first element and \\(b\\) is the second element of the pair. ( \\((a,b) = (c,d) \\longrightarrow a = c \\text{ and } b = d\\) \\) ::: ::: ::: dBox ::: definitionT Definition 1.16 (Cartesian product). For sets \\(A\\) and \\(B\\) , the Cartesian product of \\(A\\) and \\(B\\) , denoted \\(A \\times B\\) , is the set of all ordered pairs \\((a,b)\\) . ( \\(A \\times B = \\{(a,b) \\mid a \\in A \\text{ and } b \\in B\\}\\) \\) ::: ::: ::: exampleT Example 1.11 . Let \\(A = \\{x,y\\}\\) and \\(B = \\{1,2,3\\}\\) , find \\(A \\times B\\) and \\(B \\times A\\) : \\(A \\times B = \\{(x,1), (y,1), (x,2), (y,2), (x,3), (y,3)\\}\\) \\(B \\times A = \\{(1,x), (1,y), (2,x), (2,y), (3,x), (3,y)\\}\\) ::: ::: list Note how the order matters, such that \\(A \\times B \\neq B \\times A\\) in the following example above. ::: -4ex -1ex -.4ex 1ex .2ex Logic It's important to first establish one thing, which we define as: ::: dBox ::: definitionT Definition 1.17 (Statement). A statement (or proposition) is a sentence that is true or false, but not both. ::: ::: There are different ways to express a statement as shown below. ::: exampleT Example 1.12 . Determine whether the following are statements and if so, are they true or false? \\\" \\(2\\) is greater than \\(5\\) \\\" is a statement and is logically false. \\\" \\(x > 5\\) \\\" is not a statement because of the variable \\(x\\) , it is undetermined. \\\" \\(\\sqrt{9}\\) is an integer\\\" is a statement and is logically true. \\\"There are \\(7\\) days in a week\\\" is a statement and is logically true. ::: We will now introduce five logical connectives, used to build more complicated logical expressions out of simpler ones. Let \\(P\\) and \\(Q\\) be statement variables. ::: dBox ::: definitionT Definition 1.18 (Negation). The statement \\\"not \\(P\\) \\\", denoted by \\(\\lnot\\ P\\) , is true when \\(P\\) is false. ::: ::: ::: dBox ::: definitionT Definition 1.19 (Conjunction). The statement \\\" \\(P\\) and \\(Q\\) \\\", denoted by \\(P\\ \\land\\ Q\\) , is true when, and only when, both \\(P\\) and \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.20 (Disjunction). The statement \\\" \\(P\\) or \\(Q\\) \\\", denoted by \\(P\\ \\lor\\ Q\\) , is true when at least one of \\(P\\) or \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.21 (Conditional). The statement \\\"If \\(P\\) then \\(Q\\) \\\", denoted by \\(P\\ \\rightarrow\\ Q\\) , is false when \\(P\\) is true and \\(Q\\) is false; otherwise it is true. ::: ::: ::: dBox ::: definitionT Definition 1.22 (Biconditional). The statement \\\" \\(P\\) if and only if \\(Q\\) \\\", denoted by \\(P\\ \\leftrightarrow\\ Q\\) , is true exactly when either \\(P\\) and \\(Q\\) are both true, or when \\(P\\) and \\(Q\\) are both false. ::: ::: ::: list The order of operations goes from \\(\\lnot\\) , \\(\\land\\) , \\(\\lor\\) , \\(\\rightarrow\\) then \\(\\leftrightarrow\\) , if no parenthesis are present. ::: The five logical connectives has the following truth table: \\(P\\) \\(\\lnot\\ P\\) T F F T \\(P\\) \\(Q\\) \\(P\\ \\land\\ Q\\) \\(P\\ \\lor\\ Q\\) \\(P\\ \\rightarrow\\ Q\\) \\(P\\ \\leftrightarrow\\ Q\\) T T T T T T T F F T F F F T F T T F F F F F T T However, when statement is always true or false we can define to be the following: ::: dBox ::: definitionT Definition 1.23 (Tautology). A tautology is a statement form that is always true regardless of the truth values of the individual statement substituted for its statement variables ::: ::: ::: dBox ::: definitionT Definition 1.24 (Contradiction). A contradiction is a statement form that is always false regardless of the truth values of the individual statements substituted for its statement variables. ::: ::: ::: exampleT Example 1.13 . Show that \\(P\\ \\lor\\ \\lnot\\ P\\) is a tautology and that \\(P\\ \\land\\ \\lnot\\ P\\) is a contradiction. \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\lor\\ \\lnot\\ P\\) *T* *F* *T* *F* *T* *T* \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\land\\ \\lnot\\ P\\) *T* *F* *F* *F* *T* *F* ::: -3ex -0.1ex -.4ex 0.5ex .2ex Conditional Statements When asked to rewrite the following sentences using \\(\\rightarrow\\) , they would use the phrases \\\"necessary condition\\\" and \\\"sufficient condition\\\", which implies: The statement \\(P\\) is a necessary condition for \\(Q\\) means that \\(Q\\ \\rightarrow\\ P\\) . The statement \\(P\\) is a sufficient condition for \\(Q\\) means that \\(P\\ \\rightarrow\\ Q\\) . For a conditional statement, \\(P\\ \\rightarrow\\ Q\\) , we can form two related statements: ::: dBox ::: definitionT Definition 1.25 (Inverse). The inverse of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ P\\ \\rightarrow\\ \\lnot\\ Q\\) . ::: ::: ::: dBox ::: definitionT Definition 1.26 (Converse). The converse of \\(P\\ \\rightarrow\\ Q\\) is \\(Q\\ \\rightarrow\\ P\\) . ::: ::: ::: dBox ::: definitionT Definition 1.27 (Contrapositive). The contrapositive of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ Q\\ \\rightarrow\\ \\lnot\\ P\\) . ::: ::: ::: exampleT Example 1.14 . Rewrite the following statement: \\\"If \\(n\\) is prime, then \\(n\\) is odd or \\(n\\) is \\(2\\) \\\" using the: Inverse: If \\(n\\) is not prime, then \\(n\\) is not odd and \\(n\\) is not \\(2\\) . Converse: If \\(n\\) is odd or \\(n\\) is \\(2\\) , then \\(n\\) is a prime. Contrapositive: If \\(n\\) is not odd and \\(n\\) is not \\(2\\) , then \\(n\\) is not a prime. ::: ::: list When writing the negation, watch out for other logical connectives present in the sentences, like \\(\\land\\) and \\(\\lor\\) and make sure to apply De Morgan's laws properly. ::: -3ex -0.1ex -.4ex 0.5ex .2ex Logical Equivalences Two statements are logically equivalent, denoted by \\(\\equiv\\) , if they share the same truth table. ::: tBox ::: theoremeT Theorem 1.1 . Let \\(P\\) and \\(Q\\) be statement variables: Commutative law: Associative law: Distributive law: Double negation law: De Morgan's law: Idempotent law: Implication law: ::: ::: You might notice some similarities to the previous theorem for unions and intersections of a set. ::: tabu *2X[c] Logical Equivalences & Set Properties \\ ::: flushleft For all statements variables \\(P\\) , \\(Q\\) , and \\(R\\) : ::: & ::: flushleft For all sets \\(A\\) , \\(B\\) , and \\(C\\) : ::: \\ (a) \\(P\\ \\lor\\ Q \\equiv Q\\ \\lor\\ P\\) (b) \\(P\\ \\land\\ Q \\equiv Q\\ \\land\\ P\\) & (a) \\(A\\ \\cup\\ B = B\\ \\cup\\ A\\) (b) \\(A\\ \\cap\\ B = B\\ \\cap\\ A\\) \\ (a) \\(P\\ \\land\\ (Q \\land R) \\equiv (P\\ \\land Q)\\ \\land\\ R\\) (b) \\(P\\ \\lor\\ (Q \\lor R) \\equiv (P\\ \\lor Q)\\ \\lor\\ R\\) & (a) \\(A\\ \\cap\\ (B\\ \\cap\\ C) = (A\\ \\cap\\ B)\\ \\cap\\ C\\) (b) \\(A\\ \\cup\\ (B\\ \\cup\\ C) = (A\\ \\cup\\ B)\\ \\cup\\ C\\) \\ (a) \\(P\\ \\land\\ (Q\\ \\lor R) \\equiv (P\\ \\land Q)\\ \\lor\\ (P\\ \\land\\ R)\\) (b) \\(P\\ \\lor\\ (Q\\ \\land R) \\equiv (P\\ \\lor Q)\\ \\land\\ (P\\ \\lor\\ R)\\) & (a) \\(A\\ \\cap\\ (B\\ \\cup\\ C) = (A\\ \\cap\\ B)\\ \\cup\\ (A\\ \\cap\\ C)\\) (b) \\(A\\ \\cup\\ (B\\ \\cap\\ C) = (A\\ \\cup\\ B)\\ \\cap\\ (A\\ \\cup\\ C)\\) \\ (a) \\(\\lnot(\\lnot\\ P) \\equiv P\\) & (a) \\((A^c)^c = A\\) \\ (a) \\(P\\ \\lor\\ P \\equiv P\\) (b) \\(P\\ \\land\\ P \\equiv P\\) & (a) \\(A\\ \\cup\\ A = A\\) (b) \\(A\\ \\cap\\ A = A\\) \\ (a) \\(\\lnot(P\\ \\lor\\ Q) \\equiv \\lnot\\ P\\ \\land\\ \\lnot\\ Q\\) (b) \\(\\lnot(P\\ \\land\\ Q) \\equiv \\lnot\\ P\\ \\lor\\ \\lnot\\ Q\\) & (a) \\((A\\ \\cup\\ B)^c = A^c\\ \\cap\\ B^c\\) (b) \\((A\\ \\cap\\ B)^c = A^c\\ \\cup\\ B^c\\) \\ ::: ::: tabu *2X[c] (a) \\(P\\ \\lor\\ (P\\ \\land\\ Q) \\equiv P\\) (b) \\(P\\ \\land\\ (P\\ \\lor\\ Q) \\equiv P\\) & (a) \\(A\\ \\cup\\ (A\\ \\cap\\ B) = A\\) (b) \\(A\\ \\cap\\ (A\\ \\cup\\ B) = A\\) \\ ::: -3ex -0.1ex -.4ex 0.5ex .2ex Predicates and Quantified Statements We initially discussed that a logical statement is either true or false. So something like \\\" \\(x > 5\\) \\\" is not a statement, but what we define to be a predicate. ::: dBox ::: definitionT Definition 1.28 (Predicate). A predicate \\(P(x)\\) is a sentence that contains a finite number of variables and becomes a statement when specific values are substituted for variables. ::: ::: ::: dBox ::: definitionT Definition 1.29 (Domain). The domain \\(D\\) of a predicate variable is the set of all values that may be substituted in place of variable. ::: ::: A way to obtain statements from predicates is to add quantifiers. Let \\(P(x)\\) be a predicate and \\(D\\) the domain of \\(x\\) . ::: dBox ::: definitionT Definition 1.30 (Universal quantifier). The symbol \\(\\forall\\) is read as \\\"for every\\\" or \\\"for all.\\\" A universal statement is a statement of the form, \\(\\forall x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for every \\(x\\) in \\(D\\) . It is defined to be false if, and only if, \\(P(x)\\) is false for at least one \\(x\\) in \\(D\\) . ::: ::: ::: dBox ::: definitionT Definition 1.31 (Existential quantifier). The symbol \\(\\exists\\) is read as \\\"there exists\\\" or \\\"there is.\\\" An existential statement is a statement of the form, \\(\\exists x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for at least one \\(x\\) in \\(D\\) . It is false if, and only if, \\(P(x)\\) is false for all \\(x\\) in \\(D\\) . ::: ::: ::: exampleT Example 1.15 . Rewrite the following sentences using quantifiers. \\\"Every real number has a non-negative square\\\" rewritten as \\(\\forall x \\in \\mathbb{R},\\ x^2 \\geq 0\\) \\\"There is a positive integer whose square is equal to itself\\\" rewritten as \\(\\exists y \\in \\mathbb{Z}^+,\\ y^2 = y\\) ::: One final topic to discuss is the negation of universal and existential quantifiers. ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Universal statement). The negation of a universal statement is logically equivalent to an existential statement. Symbolically, ( \\(\\lnot(\\forall x \\in D,\\ P(x)) \\equiv \\exists x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Existential statement). The negation of a existential statement is logically equivalent to an universal statement. Symbolically, ( \\(\\lnot(\\exists x \\in D,\\ P(x)) \\equiv \\forall x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: exampleT Example 1.16 . Negate the following statements from the previous exercise. \\\"There exists a real number that has a negative square\\\" or \\(\\exists x \\in \\mathbb{R},\\ x^2 < 0\\) \\\"Every positive integer whose square is not equal to itself\\\" or \\(\\forall x \\in \\mathbb{Z}^+,\\ y^2 \\neq y\\) ::: ::: list In example 1.11, the first statement is true, while, in example 1.12, the first statement is false, since squaring a number will always be positive. ::: There are some cases where certain statements contain multiple quantifiers. ::: exampleT Example 1.17 . Negate the following statement: \\(\\forall x \\in \\mathbb{Z},\\ \\exists y \\in \\mathbb{Z},\\ y > x\\) . We can simply think of \\\" \\(\\exists y \\in \\mathbb{Z},\\ y > x\\) \\\" as the predicate \\(P(x)\\) . ( \\(\\lnot(\\forall x \\in \\mathbb{Z},\\ P(x)) \\equiv \\exists x \\in \\mathbb{Z},\\ \\lnot\\ P(x)\\) \\) Then we take the negation of \\(P(x)\\) . ( \\(\\lnot\\ P(x) \\equiv \\lnot(\\exists y \\in \\mathbb{Z},\\ y > x) \\equiv \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) And we put it all together. ( \\(\\exists x \\in \\mathbb{Z},\\ \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Order of Quantifiers In a statement containing both \\(\\forall\\) and \\(\\exists\\) , changing the order of the quantifiers can significantly change the meaning of the statementyou read from left to right. For example, the following statements are equivalent: \\( \\(\\forall x,\\ \\forall y,\\ P(x) \\equiv \\forall y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\exists y,\\ P(x) \\equiv \\exists y,\\ \\exists x,\\ P(x)\\) \\) However, now consider mixed quantifier and they are no longer equivalent: \\( \\(\\forall x,\\ \\exists y,\\ P(x) \\not\\equiv \\exists y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\forall y,\\ P(x) \\not\\equiv \\forall y,\\ \\exists x,\\ P(x)\\) \\) For a more dramatic context. let's use the following analogy: \\(\\forall x,\\ \\exists y,\\ x \\text{ loves } y\\) and \\(\\exists y,\\ \\forall x,\\ x \\text{ loves } y\\) Note that both statements looks identical, except the order of quantifiers. However, the first statement means everybody loves somebodywhom that somebody could be a different person for each \\(x\\) , ... whereas the second statement means there is one individual who is loved by all people. We can also visualize this using a directed graph, as a shown below:","title":"Indexed Collection of Sets"},{"location":"W2022/MTH314/MTH314/#relations-and-functions","text":"-4ex -1ex -.4ex 1ex .2ex Binary Relations A relations is something that involves two different sets. A special kind of binary relation is a function. Suppose there are some elements inside \\(X\\) and \\(Y\\) , we can visualize an arrow diagram for our relation. The graph corresponds to the following ordered pairs: \\(\\{(x_1,y_1), (x_2, y_2), (x_3, y_3)\\}\\) ::: dBox ::: definitionT Definition 2.1 (Binary Relation). For sets \\(X\\) and \\(Y\\) , a binary relation \\(R\\) from \\(X\\) to \\(Y\\) is a subset of \\(X \\times Y\\) . Hence, \\(R\\) is a set of ordered pairs \\((x,y)\\) with \\(x \\in X\\) and \\(y \\in Y\\) . We write \\(x \\mathrel{R}y\\) if \\((x,y) \\in R\\) . We say that \\(R\\) is a binary relation on \\(X\\) if \\(X = Y\\) ; that is, \\(R\\subseteq X \\times X\\) . ::: ::: A relation can also be drawn as a directed graph which will prove to be more useful when explaining the properties of relation. Using the same set of ordered pairs from before, it can drawn as such: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Relation There exists various properties that some, but not all, relations satisfy. Let \\(R\\) be a binary relation on a set \\(A\\) : ::: dBox ::: definitionT Definition 2.2 (Reflexive). The relation \\(R\\) is reflexive, if for all \\(x \\in A\\) , \\(x \\mathrel{R}x\\) . \\(R\\) is reflexive \\(\\Leftrightarrow\\) for every \\(x\\) in \\(A\\) , \\((x,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.3 (Symmetric). The relation \\(R\\) is symmetric, if for all \\(x,y \\in A\\) , if \\(x \\mathrel{R}y\\) , then \\(y \\mathrel{R}x\\) . \\(R\\) is symmetric \\(\\Leftrightarrow\\) for every \\(x\\) and \\(y\\) in \\(A\\) , if \\((x,y) \\in R\\) then \\((y,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.4 (Transitive). The relation \\(R\\) is transitive, if for all \\(x,y,z \\in A\\) , if \\(x \\mathrel{R}z\\) and \\(y \\mathrel{R}z\\) , then \\(x \\mathrel{R}z\\) . \\(R\\) is transitive \\(\\Leftrightarrow\\) for every \\(x\\) , \\(y\\) , and \\(z\\) in \\(A\\) , if \\((x,y) \\in R\\) and \\((y,z) \\in R\\) then \\((x,z) \\in R\\) ::: center ::: ::: ::: ::: exampleT Example 2.1 . Suppose we have a set \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0), (0,1), (0,3), (1,0), (1,1), (2,2), (3,0), (3,3)\\}\\) \\) Let's draw out the directed graph. ::: center ::: \\(R\\) is reflexive because there is a loop for each element, as shown for \\((0,0), (1,1), (2,2), (3,3)\\) . \\(R\\) is symmetric because there is an arrow going from one point then back to the other, as shown for \\((0,1), (1,0)\\) and \\((0,3), (3,0)\\) \\(R\\) is not transitive because there's no arrow from \\((1,3)\\) or \\((3,1)\\) which would otherwise make it transitive. ::: Sometimes the set of relation is not given, instead the definition is provided. For example, suppose we have set a \\(A = \\{1,2,3,4\\}\\) and the relation \\(R\\) defined as follows: Properties of Div: \\((x,y) \\in R \\text{ if } x \\mid y\\) The line \\\" \\(\\mid\\) \\\" means \\(x\\) divisible by \\(y\\) . Equivalently, you can think of it as \\(y = xk\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,2),(1,3),(1,4),(2,2),(2,4),(3,3),(4,4)\\}\\) \\) Properties of Congruence Modulo n: \\((x,y) \\in R \\text{ if } n \\mid (x-y)\\) Sometimes, it referred to as \\(x \\equiv y \\ (\\mathrm{mod}\\ n)\\) . Let use \\(n = 2\\) , as an example. We can use the expression before, \\(x-y = 2k\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,3),(2,2),(2,4),(3,1),(3,3),(4,2),(4,4)\\}\\) \\) Properties of \\\"Greater Than\\\": \\((x,y) \\in R \\text{ if } x > y\\) In this case, we are only interested where \\(x\\) is greater than \\(y\\) . The set of relation would be: \\( \\(R = \\{(2,1),(3,1),(3,2),(4,1),(4,2),(4,3)\\}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Equivalence Relation There's are cases when the relation has all three properties discussed. ::: dBox ::: definitionT Definition 2.5 (Equivalence relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is an equivalence relation if it is reflexive, symmetric and transitive, \\((A, R)\\) . ::: ::: A notable example would be the equality sign, which might not make senses at first, but let's break it down. For all real numbers \\(x\\) and \\(y\\) , consider the relation \\(R\\) defined as follows \\( \\((x,y) \\in R \\text{ if } x = y\\) \\) Is \\(R\\) reflexive? Yes, because it is implying the statement \\(x = x\\) , which is true; every real number is equal to itself. Is \\(R\\) symmetric? Yes, because if \\(x = y\\) , then \\(y = x\\) is also true; if one number is equal to another, then the second is equal to the first. Is \\(R\\) transitive? Yes, because if \\(x = y\\) and \\(y = z\\) , then \\(x = z\\) is true; if one real number equals a second and the second equals a third, then the first must also equal the third. Let's introduce this new idea called the equivalence class, as an extension to equivalence relation. Suppose \\(A\\) is a set and \\(R\\) is an equivalence relation on \\(A\\) . ::: dBox ::: definitionT Definition 2.6 (Equivalence class). For each element \\(a\\) in \\(A\\) , the equivalence class of \\(a\\) , denoted \\([a]\\) is the set of all elements \\(x\\) in \\(A\\) such that \\(x\\) is related to \\(a\\) by \\(R\\) . ( \\(= \\{x \\in A \\mid x \\mathrel{R}a \\}\\) \\) ::: ::: ::: list Some textbooks may define it as \\([a] = \\{x \\in A \\mid a \\mathrel{R}x \\}\\) instead. Either one works, as you may recall, it's transitive. ::: ::: exampleT Example 2.2 . Let \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0),(0,4),(1,1),(1,3),(2,2),(3,1),(3,3),(4,0),(4,4)\\}\\) \\) Determine all the elements related to \\(0\\) or \\(x \\mathrel{R}0\\) , where \\(x \\in A\\) . ( \\(0 \\mathrel{R}0,\\ 4 \\mathrel{R}0\\) \\) The elements \\(0\\) and \\(4\\) are essentially what makes up the equivalence class of \\(0\\) , which more formally can be written as ( \\([0] = \\{x \\in A \\mid x \\mathrel{R}0\\} = \\{0,4\\}\\) \\) Thus, find the equivalence class for the rest of the elements of \\(A\\) . ( \\([1] = \\{x \\in A \\mid x \\mathrel{R}1\\} = \\{1,3\\}\\) \\) ( \\([2] = \\{x \\in A \\mid x \\mathrel{R}2\\} = \\{2\\}\\) \\) ( \\([3] = \\{x \\in A \\mid x \\mathrel{R}3\\} = \\{1,3\\}\\) \\) ( \\([4] = \\{x \\in A \\mid x \\mathrel{R}4\\} = \\{0,4\\}\\) \\) Note that \\([4] = [0]\\) and \\([1] = [3]\\) , so the distinct equivalent classes are ( \\(\\{0,4\\}, \\{1,3\\}, \\text{ and } \\{2\\}\\) \\) ::: If you notice, the distinct equivalence classes of an equivalence relation \\(R\\) on a set \\(A\\) form a partition of \\(A\\) . Likewise, the converse is also true, which will discuss in the next section. ::: tBox ::: theoremeT Theorem 2.1 . Let \\(R\\) be an equivalence relation on a set \\(A\\) , where we assume \\(A \\neq \\varnothing\\) . For all \\(x \\in A\\) , \\([x] \\neq \\varnothing\\) . If \\(x \\mathrel{R}y\\) , then \\([x] = [y]\\) . If \\((x,y) \\notin R\\) , then \\([x]\\ \\cap\\ [y] = \\varnothing\\) . ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Relation Induced by Partition As a recap, a partition of a set \\(A\\) is a finite or infinite collection of nonempty, mutually disjoint subsets whose union is \\(A\\) or illustratively, ::: center ::: where \\(A_i\\ \\cap\\ A_j = \\varnothing\\) whenever \\(i \\neq j\\) and \\(A_1\\ \\cup\\ \\cdots\\ \\cup\\ A_6 = A\\) . ::: dBox ::: definitionT Definition 2.7 . Let \\(P\\) be a partition of a set \\(A\\) and define the binary relation \\(R_P\\) , so that \\(x \\mathrel{R}y\\) if \\(x\\) and \\(y\\) are in the same part of the partition. ::: ::: Again, this definition might sound confusing, but let's use the set \\(A\\) from the previous example to get a better understanding. ::: exampleT Example 2.3 . Let \\(A = \\{0,1,2,3,4\\}\\) and consider the following be a partition \\(P\\) of \\(A\\) : ( \\(P = \\{\\{0,1\\},\\{2,3\\},\\{4\\}\\} % A_1 = \\{0,1\\} \\hspace{1cm} A_2 = \\{2,3\\} \\hspace{1cm} A_3 = \\{4\\}\\) \\) Let's consider the first subset of the partition, \\(A_1 = \\{0,1\\}\\) . Since both \\(0\\) and \\(1\\) are in the subset, we can form the following relation. ( \\(0 \\mathrel{R}1,\\ 1\\mathrel{R}0\\) \\) We can also do the following, since they are still in the same part of the partition. ( \\(0 \\mathrel{R}0,\\ 1 \\mathrel{R}1\\) \\) Thus, for the second subset, \\(A_2 = \\{2,3\\}\\) , we can form a similar set of relations. ( \\(2 \\mathrel{R}3,\\ 3 \\mathrel{R}2,\\ 2 \\mathrel{R}2,\\ 3 \\mathrel{R}3\\) \\) Finally for the third subset of the partition, \\(A_3 = \\{4\\}\\) , which only has one. ( \\(4 \\mathrel{R}4\\) \\) Hence, combining all of them makes up the binary relation \\(R_P\\) : ( \\(R_P = \\{\\underbrace{(0,1),(1,0),(0,0),(1,1)}_{\\{0,1\\}},\\underbrace{(2,3),(3,2),(2,2),(3,3)}_{\\{2,3\\}},\\underbrace{(4,4)}_{\\{4\\}}\\}\\) \\) ::: The fact is that a relation induced by a partition of a set satisfies all three properties, or in other words, \\(R_P\\) is an equivalence relation. -3ex -0.1ex -.4ex 0.5ex .2ex Partial Orders Partial orders provide one way of ranking objects. To define them, we define another property of relation. ::: dBox ::: definitionT Definition 2.8 (Antisymmetric). Let \\(R\\) be a binary relation on a set \\(A\\) . We say \\(R\\) is antisymmetric, if and only if, for every \\(a\\) and \\(b\\) in \\(A\\) , if \\(a \\mathrel{R}b\\) and \\(b \\mathrel{R}a\\) , where \\(a = b\\) . ::: ::: Equivalently, we are saying that the relation should not have the following: ::: center ::: For example, the relation on the left is not antisymmetric, whereas the relation on the right is antisymmetric. ::: center ::: ::: dBox ::: definitionT Definition 2.9 (Partial order relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is a partial order relation if it is reflexive, antisymmetric, and transitive. ::: ::: We call \\((A,R)\\) a partial ordered set or poset. We use \\(\\preceq\\) to represent the relation \\(R\\) . Two elements \\(x\\) and \\(y\\) are comparable if either \\(x \\mathrel{R}y\\) or \\(y \\mathrel{R}x\\) . Otherwise, the elements are incomparable. A set of pairwise incomparable is called an antichain . If every pair of element is comparable, then \\(R\\) is a total order , linear order , or chain . Back to previous example, the relation on the left has two elements, \\(0\\) and \\(1\\) , that are incomparable. The relation on the right is a linear order with three elements. ::: dBox ::: definitionT Definition 2.10 (Hasse diagram). A diagram used to represent partial order relations with sufficient information and an implied upward orientation. ::: ::: To obtain a Hasse diagram, proceed as follows: Construct a digraph (or directed graph) of the poset \\((A,R)\\) , so that all arrows point upward, except the loops. Eliminate all loops. Eliminate all directed edges that are redundant because of transitivity. Eliminate the arrows on the directed edge. Suppose we have a set \\(A = \\{1,2,3,9,18\\}\\) with the div relation, \\(x \\mid y\\) . The digraph of this poset has the following appearance on the left and the Hasse diagram on the right: ::: center ::: We can reference some extremal elements of posets using the following definitions. ::: dBox ::: definitionT Definition 2.11 . Let \\(R\\) be a partial order of on a set \\(A\\) : An element \\(u\\) is a least element if \\(\\forall x \\in A\\) , \\(u \\mathrel{R}x\\) . An element \\(v\\) is a greatest element if \\(\\forall x \\in A\\) , \\(x \\mathrel{R}v\\) . An element \\(u\\) is a minimal element if there does not exist an element \\(x \\in A \\smallsetminus \\{u\\}\\) , such that \\(x \\mathrel{R}u\\) . Alternatively, there exists no element \\\"below\\\" it. An element \\(u\\) is a maximal element if there does not exist an element \\(x \\in A \\smallsetminus \\{v\\}\\) , such that \\(v \\mathrel{R}x\\) . Alternatively, there exists no element \\\"above\\\" it. ::: ::: Note the difference between least and minimal element, likewise, with greatest and maximal element. A least element is a minimal, but a minimal element need not to be a least element. Similarly, a greatest element is a maximal, but a maximal element need not to be a greatest element. A poset can have at most one least and greatest element, but it may have more than one minimal or maximal element. Look at the following digraph of each poset: ::: center ::: -4ex -1ex -.4ex 1ex .2ex Introduction to Graph Theory Graph theory one of the most important topics in discrete mathematics. You may have heard of the famous bridge puzzle, known as \\\"The Seven Bridges of K\u00f6nigsberg\\\", which consists of the following problem. ::: problem Problem 2.1 . Is it possible to find a route through the graph that starts and ends at some vertex, one of \\(A\\) , \\(B\\) , \\(C\\) , or \\(D\\) , and traverses each edge exactly once? ::: center ::: ::: We can further simply this to the following graph. If you compare the two diagrams, they are equivalently the same. ::: center ::: If you aren't already aware of it, this problem is impossible to solve and it all relates back to graph theory. Let's start off by defining what a graph is. ::: dBox ::: definitionT Definition 2.12 (Graph). A graph \\(G\\) is a pair consisting of a vertex set \\(V(G)\\) and an edge set \\(E(G)\\) containing pairs of distinct vertices, such that \\(G = (V,E)\\) ::: ::: ::: list The bridge graph is an undirected graphthe order of the two connected vertices is not important, oppose to a directed graph. ::: The order of a graph \\(G\\) is \\(|V(G)|\\) and its size is \\(|E(G)|\\) . We can use the bridge example, to define our vertex and edge set, \\( \\(V(G) = \\{A,B,C,D\\} \\hspace{1cm} E(G) = \\{\\{A,B\\},\\{A,B\\},\\{B,D\\},\\{B,D\\},\\{A,C\\},\\{B,C\\},\\{C,D\\}\\}\\) \\) where \\(|V(G)| = 4\\) and \\(|E(G)| = 7\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Degrees There's no solution to the bridge problem because suppose we start and end at vertex \\(A\\) , then the degree of the other three vertices \\(B\\) , \\(C\\) , and \\(D\\) must be even. ::: dBox ::: definitionT Definition 2.13 (Degree). Given a graph with vertex \\(v\\) , the degree of \\(v\\) , denoted by \\(\\deg_G(v)\\) is the number of edges incident to \\(v\\) . ::: ::: From the graph earlier, we can define the degree of each vertex: \\(\\deg(A) = 3\\) , \\(\\deg(B) = 5\\) , \\(\\deg(C) = 3\\) , and \\(\\deg(D) = 3\\) . ::: dBox ::: definitionT Definition 2.14 (Neighbor set). The neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(\\{w \\in V(G) \\mid v,w \\in E(G)\\}\\) ; any \\(w \\in V(G)\\) is called a neighbor of \\(v\\) , equivalently \\(\\deg_G(v) = |N_G(v)|\\) . The closed neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(N_G(v)\\ \\cup\\ \\{v\\}\\) . ::: ::: ::: list As you'll see later on, the subscript \\(G\\) is often removed, if the graph is clear from context. ::: The definition may sound more confusing that what it is suppose to mean, but it is essentially the set of all vertices which are adjacent to \\(v\\) . Refer to the example below. Another set of terms will discuss in this section the minimum and maximum degree of a graph \\(G\\) . ::: dBox ::: definitionT Definition 2.15 (Minimum and Maximum Degree). The integer \\(\\delta(G)\\) is the minimum degree of \\(G\\) and the integer \\(\\Delta(G)\\) is the maximum degree of \\(G\\) . ::: ::: ::: exampleT Example 2.4 . Suppose we have the following graph \\(G\\) is defined as: ::: center ::: For each vertex: \\(\\deg(1) = 1\\) , \\(\\deg(2) = 2\\) , \\(\\deg(3) = 3\\) , \\(\\deg(4) = 2\\) , and \\(\\deg(5) = 2\\) . The vertex \\(3\\) has three vertices adjacent to it, which are \\(2\\) , \\(4\\) , and \\(5\\) , thus \\(N(3) = \\{2,4,5\\}\\) and \\(|N(3)| = \\deg(3) = 3\\) . The closed neighbor set includes \\(3\\) , since \\(N[3] = \\{2,4,5\\}\\ \\cup\\ \\{3\\} = \\{2,3,4,5\\}\\) . From 1. we can see the minimum degree is \\(1\\) and the maximum degree is \\(3\\) , thus \\(\\delta(G) = 1\\) and \\(\\Delta(G) = 3\\) . ::: The following theorem is pretty simple, but helpful in drawing conclusions when analyzing graphs. ::: tBox ::: theoremeT Theorem 2.1 (Handshake Theorem). Let \\(G\\) be a graph, then ( \\(\\sum_{u \\in V(G)} = \\deg_G(u) = 2|E(G)|\\) \\) ::: ::: The equivalent definition of this in text means, the sum of degree of all the vertices is twice the number of edges contained in it. Using the example from before, there's five vertices and a total degree of \\(10\\) . In general, any undirected graph has an even number of vertices of odd degree. -3ex -0.1ex -.4ex 0.5ex .2ex Walks, Paths, and Cycles Travel in a graph is accomplished by moving from one vertex to another along a sequence of adjacent edges. There various definitions used to describe the movement in a graph. ::: dBox ::: definitionT Definition 2.16 . Let \\(G\\) be a graph and let \\(u\\) and \\(v\\) be vertices in \\(G\\) . Walk: A walk from \\(u\\) to \\(v\\) is a finite sequence of adjacent vertices of \\(G\\) . Thus a walk has the form, \\(W = (v_0,v_1,v_2, \\cdots, v_n)\\) if \\(\\{v_i,v_{i+1}\\} \\in E(G)\\) for \\(0 \\leq i < n\\) , where \\(v_0 = u\\) and \\(v_n = v\\) . The length of a walk is the number of edges in \\(W\\) . Closed Walk: A closed walk is a walk that starts and ends at the same vertex, where \\(v_0 = v_n\\) . Path: A path is a walk without repeated vertices. The path of order \\(n \\geq 1\\) is denoted by \\(P_n\\) . Cycle: A cycle is a closed walk of at least \\(3\\) or more vertices. The cycle of order \\(n \\geq 3\\) is denoted by \\(C_n\\) . ::: ::: A path, \\(P_n\\) , where \\(n \\geq 1\\) , consist of \\(n\\) vertices and \\(n-1\\) edges. ::: center ::: A cycle, \\(C_n\\) , where \\(n \\geq 3\\) , consists of \\(n\\) vertices and \\(n\\) edges. ::: center ::: ::: exampleT Example 2.5 . Suppose we have the graph below, define the following walks: ::: center ::: ::: multicols 2 \\((v_0,v_1,v_2)\\) is a path, \\(P_3\\) , as neither vertices nor edges are repeated. ::: center ::: \\((v_2,v_6,v_4,v_5,v_1,v_2)\\) is a cycle, \\(C_5\\) , as we do not repeat a vertex nor edge, but started and ended at the same vertex. ::: center ::: ::: ::: We can now define what we mean by the diameter of a graph \\(G\\) . Note this will get a bit confusing. First, let's start with the distance between two vertices \\(u\\) and \\(v\\) . ::: dBox ::: definitionT Definition 2.17 (Graph distance). The distance between \\(u\\) and \\(v\\) is the minimum length of the paths in \\(G\\) connecting them, denoted by \\(d_G(u,v)\\) or \\(d(u,v)\\) , if \\(G\\) is clear from context. ::: ::: ::: list The distance between \\(u\\) and \\(v\\) is the same regardless of the start position, such that \\(d(u,v) = d(v,u)\\) . You can think of the distance as the number of edges traversed. ::: So in theory, what does this exactly mean? Let's use a simple graph for now. ::: center ::: The distance between \\(u\\) and \\(v\\) is \\(d(u,v) = 2\\) , as that's the only path to traverse. Another example, let's try and use the graph from before to see if you fully understand the definition. ::: center ::: As you can see, there are many possible paths from \\(u\\) to \\(v\\) , shown in red. Some examples are: ::: center ::: However, remember that we are only interested in the shortest path, which in this case is \\(d(u,v) = 3\\) . It's really important that you understand how to define the shortest path in \\(G\\) given two vertices \\(u\\) and \\(v\\) , as it will help in understand the next definition. ::: dBox ::: definitionT Definition 2.18 (Graph diameter). The diameter of a graph \\(G\\) is defined as ( \\(\\mathop{\\mathrm{diam}}(G) = \\max\\{d_G(v,w) \\mid v,w \\in V(G)\\}\\) \\) Equivalently, the largest number of vertices which must be traversed in order to travel from one vertex to another. ::: ::: ::: exampleT Example 2.6 . Consider the following graph \\(G\\) , determine the diameter of the graph: ::: center ::: We can start off by focusing on vertex \\(a\\) : ::: center ::: Then on vertex \\(b\\) : ::: center ::: Repeat for the rest of the vertices, for all \\(v,w \\in V(G)\\) . If you did it properly, you will see that \\(d(a,d)\\) and \\(d(f,e)\\) have the maximum distance. ( \\(\\mathop{\\mathrm{diam}}(G) = 3\\) \\) ::: You can also refer to this [video]{.underline} if you want a visual explanation of this example. -3ex -0.1ex -.4ex 0.5ex .2ex Subgraphs and Induced Subgraphs As the name suggests, the prefix \\\"sub\\\" refers to it being subsets of another graph. ::: dBox ::: definitionT Definition 2.19 (Subgraph). A graph \\(H\\) is said to be a subgraph of a graph \\(G\\) , written \\(H \\subseteq G\\) , if, and only if, \\(V(H) \\subseteq V(G)\\) and \\(E(H) \\subseteq E(G)\\) . The graph \\(H\\) is a spanning subgraph of \\(G\\) if \\(V(H) = V(G)\\) . ::: ::: It may be easier to understand visually. Suppose we have the following graph \\(G\\) : ::: center ::: As an example, each of the graphs are variations of graph \\(H\\) , which are subgraphs of graph \\(G\\) : ::: center ::: We only consider it to be spanning subgraph if and only if \\(V(H) = V(G)\\) , which in this case, if \\(V(H) = \\{a,b,c,d,e,f,g,h,i,j\\}\\) , which none of them are. On the other hand, these graphs are spanning subgraphs of \\(G\\) : ::: center ::: A final concept is induced subgraphs, which consists of the following property. ::: dBox ::: definitionT Definition 2.20 (Induced subgraph). If \\(S \\subseteq V(G)\\) , then the subgraph of \\(G\\) induced by \\(S\\) , denoted by \\(G[S]\\) , has vertices \\(S\\) and edges are those of \\(G\\) with endpoints in \\(S\\) . ::: ::: Note that none of the subgraphs shown so far are considered induced subgraphs of \\(G\\) . Though, we can modify it slightly to make it an induced subgraph, indicated by the red line. ::: center ::: Just focus on the induced subgraph in the far left. Notice how every possible edge that exists in graph \\(G\\) between the vertices, \\(\\{b,d,e,f,g,h,i,j\\}\\) , exists in this subgraph, thus making it an induced subgraph. Likewise, how the edge \\(a,b\\) is not here because \\(a\\) is not in the subset of vertices. -3ex -0.1ex -.4ex 0.5ex .2ex Special Graphs We consider some important examples of graphs. One important class of graphs consists of those that do not have any loops or parallel edges. ::: dBox ::: definitionT Definition 2.21 (Simple graph). A simple graph is a graph that does not have any loops or parallel edges. In a simple graph, an edge with endpoints \\(v\\) and \\(w\\) is denoted \\(\\{v,w\\}\\) . ::: ::: The following graphs can be depicted as simple graphs: ::: center ::: Another important class of graphs consists of those that are \"complete\" in the sense. ::: dBox ::: definitionT Definition 2.22 (Complete graph). A complete graph on \\(n\\) vertices, denoted \\(K_n\\) , is a simple graph with \\(n\\) vertices and exactly one edge connecting each pair of distinct vertices. ::: ::: This may sound confusing at first, but look at the following graphs below to get a general idea of how it works. ::: center ::: Then another class of graphs we can consider are complete bipartite graphs, which are as follows. ::: dBox ::: definitionT Definition 2.23 (Complete bipartite graph). A complete bipartite graph, denoted \\(K_{m.n}\\) , is a simple graph that has its vertex set partitioned into two subsets of \\(m\\) and \\(n\\) vertices. ::: ::: Note the difference between \\(K_5\\) and \\(K_{3,2}\\) , where there are no edges connected between \\(v_1\\) , \\(v_2\\) and \\(v_3\\) or similarly with \\(w_1\\) and \\(w_2\\) , which would otherwise just make it a complete graph. ::: center ::: The dashed-lines highlights the partition of two subsets. If there are some edges not present between the parts, then the graph is just a bipartite graph. -4ex -1ex -.4ex 1ex .2ex Functions If you think about it, functions are a special kind of binary relation. By definition: ::: dBox ::: definitionT Definition 2.24 (Function). A function \\(f\\) is a binary relation from sets \\(X\\) and \\(Y\\) . For each \\(x \\in X\\) , there is a unique \\(y \\in Y\\) , so that \\(x \\mathrel{f}y\\) . We write \\(f(x) = y\\) for \\(x \\mathrel{f}y\\) and say \\\" \\(f\\) of \\(x\\) equals \\(y\\) .\\\" Denoted as \\(f: X \\to Y\\) , it is a relation from \\(X\\) , the domain of \\(f\\) , to \\(Y\\) , the co-domain of \\(f\\) . ::: ::: Using an arrow diagrams, we can define a function by the following: Every element of \\(X\\) has an arrow that points to an element in \\(Y\\) . No element of \\(X\\) has two arrows that points to two different elements in \\(Y\\) . We can also define the range of a function \\(f\\) , which is a subset of the co-domain. ::: dBox ::: definitionT Definition 2.25 (Range). Let \\(f: X \\to Y\\) be a function. The range of \\(f\\) is: ( \\(\\{y \\in Y \\mid \\text{for some } x \\in X, f(x) = y\\}\\) \\) ::: ::: ::: exampleT Example 2.7 . Suppose a function \\(f\\) is defined from \\(X\\) to \\(Y\\) by the following arrow diagram: ::: center ::: The domain of \\(f = \\{a,b,c\\}\\) and co-domain of \\(f = \\{1,2,3,4\\}\\) . The range of \\(f\\) equals \\(\\{2,4\\}\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Functions Acting on Sets You can consider the set of images in \\(Y\\) of all the elements in a subset of \\(X\\) and the set of inverse images in \\(X\\) of all the elements in a subset of \\(Y\\) . ::: dBox ::: definitionT Definition 2.26 (Image and Inverse Image). Let \\(f: X \\to Y\\) be a function and \\(A \\subseteq X\\) and \\(C \\subseteq Y\\) . The image of \\(A\\) , denoted by \\(f(A)\\) , is ( \\(f(A) = \\{y \\in Y \\mid \\text{for some } x \\in A, f(x) = y\\}\\) \\) The inverse image of \\(C\\) , denoted by \\(f^{-1}(C)\\) , is ( \\(f^{-1}(C) = \\{x \\in X \\mid f(x) \\in C\\}\\) \\) ::: ::: Using the example from before, it might be easier to understand what these definition represent: \\(f(a) = 2\\) , \\(f(b) = 4\\) , and \\(f(c) = 2\\) \\(f^{-1}(1) = \\varnothing\\) , \\(f^{-1}(2) = \\{a,c\\}\\) , \\(f^{-1}(3) = \\varnothing\\) , and \\(f^{-1}(4) = \\{b\\}\\) Note that \\(C\\) is a subset of \\(Y\\) , so you maybe asked to find the inverse image of more than one element. For example, let \\(C = \\{2,4\\}\\) , then \\(f^{-1}(C) = \\{a,b,c\\}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex One-to-One, Onto and Inverse Functions We'll now discuss two important properties that functions may satisfy: the property of being one-to-one and the property of being onto. ::: dBox ::: definitionT Definition 2.27 (One-to-one). A function \\(f: X \\to Y\\) is one-to-one (or injective ) if for all \\(x_1,x_2 \\in X\\) , such that \\(x_1 \\neq x_2\\) , we have \\(f(x_1) \\neq f(x_2)\\) . If any two distinct elements of \\(X\\) are sent to two distinct elements of \\(Y\\) , then it is one-to-one. ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.28 (Onto). A function \\(f: X \\to Y\\) is one-to-one (or surjective ) if for all \\(y \\in Y\\) , there exists \\(x \\in X\\) , such that \\(f(x) = y\\) . If each elements of \\(Y\\) equals \\(f(x)\\) for at least one \\(x\\) in \\(X\\) , then it is onto. ::: center ::: ::: ::: For finite sets, it is pretty easy to determine whether a function is one-to-one or onto, just from the arrow diagram above. The tricky part comes when analyzing a function for an infinite set. ::: exampleT Example 2.8 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = 4x - 1\\) . Is \\(f\\) is onto? In order to prove something is onto, we need show that there exists a real number \\(x\\) , such that \\(y = 4x-1\\) . We can do so, by solving for \\(x\\) in this case, where \\(x = (y - 1)/4\\) . If you notice, \\(y\\) is not restricted to anything, meaning that \\(y\\) can be any \\(\\mathbb{R}\\) . Equivalently, we are saying \\\"There exists an \\(x\\) (which we determined from 3.), which is being mapped \\(\\forall y \\in \\mathbb{R}\\) .\\\" ::: ::: exampleT Example 2.9 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = x^2\\) . Is \\(f\\) one-to-one? Consider \\(f(x_1) = f(x_2)\\) , We need to show that \\(x_1 = x_2\\) . If we fail to prove this, then it is not one-to-one. Alternatively, \\((2)^2 = (-2)^2 = 4\\) , but \\(2 \\neq -2\\) , so the function is not one-to-one. ::: There also exist functions which satisfy both properties discussed. ::: dBox ::: definitionT Definition 2.29 (One-to-one correspondence). A function \\(f: X \\to Y\\) is bijective (or bijective) if it is both one-to-one and onto. ::: center ::: ::: ::: This will aid us in defining a type of function known as the inverse function, which undoes the action of \\(f\\) . It sends each element of \\(Y\\) back to the element of \\(X\\) where it came from. ::: dBox ::: definitionT Definition 2.30 (Inverse function). Let \\(f: X \\to Y\\) be a one-to-one correspondence. The inverse function of \\(f\\) is denoted by \\(f^{-1}\\) , where \\(f^{-1}: Y \\to X\\) . ::: center ::: ::: ::: Obtaining the inverse function should be something you are all familiar with, it just been described in a different setting, which is by solving for \\(x\\) . ::: exampleT Example 2.10 . Is \\(y = 4x-1\\) a bijection (or a one-to-one correspondence) from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) ? If so, find the inverse \\(f^{-1}(x)\\) . To check if it is one-to-one, show that \\(f(x_1) = f(x_2) \\Longrightarrow x_1 = x_2\\) . The function is one-to-one, since \\(4x_1 - 1 = 4x_2 - 1 \\Longrightarrow x_1 = x_2\\) . To check if it is onto, we must show that for every \\(y \\in \\mathbb{R}\\) , there is an \\(x \\in \\mathbb{R}\\) , such that \\(y = f(x)\\) . We can prove this, by solving for \\(x\\) . ( \\(x = \\frac{y+1}{4}\\) \\) There exists some \\(x \\in \\mathbb{R}\\) , such that \\(f(x) = y\\) , which makes the function is onto. To get the inverse function, we can use the equation we obtained in the previous one and interchange \\(x\\) and \\(y\\) : ( \\(y = f^{-1}(x) = \\frac{x+1}{4}\\) \\) :::","title":"Relations and Functions"},{"location":"W2022/MTH314/MTH314/#number-theory-and-combinatorics","text":"-4ex -1ex -.4ex 1ex .2ex Elementary Number Theory The underlying content of this section consists of properties of integers, rational numbers, and real numbers. -3ex -0.1ex -.4ex 0.5ex .2ex Rational Number Sums, differences, and products of integers are integers, but most quotients of integers are not integers, rather known as: ::: dBox ::: definitionT Definition 3.1 (Rational Number). A real number \\(r\\) is rational, if and only if, it can be expressed as a quotient of two integers with a nonzero denominator. ( \\(r \\text{ is rational } \\Leftrightarrow \\exists \\text{ integers } a \\text{ and } b \\text{ such that } r = \\frac{a}{b} \\text{ and } b \\neq 0\\) \\) ::: ::: Some examples of rational numbers are \\(0\\) , \\(1\\) , and \\(1/3\\) . While a real number that is not rational is called an irrational numbers, like \\(\\pi\\) , \\(e\\) , and \\(\\sqrt{2}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Parity Number The parity of an integer focuses on the property of a number being even or odd. ::: dBox ::: definitionT Definition 3.2 (Parity). An integer \\(x\\) is even if \\(x = 2a\\) , for some integer \\(a\\) . An integer \\(x\\) is odd if \\(x = 2b + 1\\) , for some integer \\(b\\) . ::: ::: ::: tBox ::: theoremeT Theorem 3.1 (Properties of parity). Let \\(x\\) and \\(y\\) be integers: If \\(x\\) and \\(y\\) are both, then so are \\(x + y\\) and \\(xy\\) . If \\(x\\) is even and \\(y\\) is odd, then \\(x + y\\) is odd. If \\(x\\) is odd and \\(y\\) is odd, then \\(x + y\\) is even. We have that \\(x\\) is even if and only if \\(x^2\\) is even. ::: ::: Alternatively, the arithmetic on the even and odd numbers can depicted as: Even \\(+\\) Even \\(\\to\\) Even Even \\(+\\) Odd \\(\\to\\) Odd Odd \\(+\\) Odd \\(\\to\\) Even Even \\(\\times\\) Even \\(\\to\\) Even Even \\(\\times\\) Odd \\(\\to\\) Even Odd \\(\\times\\) Odd \\(\\to\\) Odd -4ex -1ex -.4ex 1ex .2ex Divisors Divisors play a central role in number theory, as they help us define prime numbers and the Euclidean algorithm, which will discuss after. ::: dBox ::: definitionT Definition 3.3 (Divisibility). The notation \\(a \\mid b\\) is read \\\" \\(a\\) divides \\(b\\) \\\", if \\(b = ak\\) , for some integer \\(k\\) . We can say that ::: description \\(b\\) is a multiple of \\(a\\) , or \\(b\\) is divisible by \\(a\\) , or \\(a\\) is a factor of \\(b\\) , or \\(a\\) is a divisor of \\(b\\) ::: ::: ::: One useful trick for checking divisibility when it comes to large numbers is by checking if the sum of its individual digit is divisible by instead. Refer to the example below. ::: exampleT Example 3.1 . Is \\(94\\;417\\;898\\;732\\) divisible by \\(9\\) ? Let's start calculating the sum of its digits ( \\(9+4+4+1+7+8+9+8+7+3+2 = 62\\) \\) Since there exist no integers \\(k\\) which satisfy the following equation, \\(62 = 9k\\) , it is not divisible by \\(9\\) . ::: Following this, we can now define what the greatest common divisor of two integers is. ::: dBox ::: definitionT Definition 3.4 (Greatest common divisor). The greatest common divisor of nonzero integers \\(a\\) and \\(b\\) , denoted \\(\\gcd(a,b)\\) , is the largest integer that divides both \\(a\\) and \\(b\\) . ::: ::: Note that every integer divides \\(0\\) , since \\(0 = a \\times 0\\) where \\(k = 0\\) . ::: exampleT Example 3.2 . Find the \\(\\gcd(72,63)\\) : The divisor of \\(72\\) are \\(\\{1,2,3,6,8,9,12,18,24,36,72\\}\\) . The divisor of \\(63\\) are \\(\\{1,3,7,9,21,63\\}\\) . The largest integer that divides both integers is \\(9\\) , such that \\(9 \\mid 72\\) and \\(9 \\mid 63\\) . Hence, \\(\\gcd(72,63) = 9\\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Prime Numbers We can also use divisors as a way to define a prime number. ::: dBox ::: definitionT Definition 3.5 (Prime number). A number \\(p > 1\\) is prime if, and only if, its only positive integer divisors are \\(1\\) and itself. Otherwise, it's composite. ::: ::: The most comprehensive statement about divisibility of integers is contained in the factorization of integers theorem. ::: tBox ::: theoremeT Theorem 3.1 (Fundamental Theorem of Arithmetic). Every integer \\(n>1\\) equals a product of primes, which is unique up to the ordering of factors. ::: ::: For example, \\(72\\) can be written as, \\(2^33^2\\) , where \\(2\\) and \\(3\\) are prime numbers. In a way you can think of each number as made up of building blocks of prime number. -3ex -0.1ex -.4ex 0.5ex .2ex Euclidean Algorithm The Euclidean algorithm provides us a simpler method for deriving the greatest common divisor of two positive integers. It is based on these two key facts: If \\(r\\) is a positive integer, then \\(\\gcd(r,0) = r\\) . If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , and if \\(q\\) and \\(r\\) are integers such that \\(a = bq + r\\) , then \\(\\gcd(a,b) = gcd(b,r)\\) The first fact should be fairly straightforward to understand. The second fact might be harder to understand, so let's use an example. ::: exampleT Example 3.3 . Find the \\(\\gcd(72,63)\\) : Let \\(a = 72\\) and \\(b = 63\\) , then we rewrite it in the form of \\(72 = 63q + r\\) . You can think of \\(q\\) as the quotient and \\(r\\) as the remainder. ( \\(72 = 63(1) + 9\\) \\) Then \\(\\gcd(72,63) = \\gcd(63,9)\\) . Let \\(a = 63\\) and \\(b = 9\\) , where \\(63 = 9q + r\\) . ( \\(63 = 9(7) + 0\\) \\) Then \\(\\gcd(63,9) = \\gcd(9,0)\\) . Using the first key fact, we know \\(\\gcd(9,0) = 9\\) . ::: Alternatively, we can set \\(q = 1\\) , where \\(r = a - b\\) . ::: tBox ::: theoremeT Theorem 3.1 (Reducing gcd). If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , then: ( \\(\\gcd(a,b) = \\gcd(b, a-b)\\) \\) ::: ::: ::: list Keep in mind, \\(\\gcd(a,b) = \\gcd(b,a)\\) , so alternatively can be written as \\(\\gcd(a, b-a)\\) . Also note that \\(\\gcd(a,b) = \\gcd(|a|,|b|)\\) . ::: Using this theorem, we can continue to reduce greatest common divisor, until we either have an integer simple enough to work with or it results in the form of \\(\\gcd(r,0)\\) . -4ex -1ex -.4ex 1ex .2ex Linear Diophantine Equation We focus on equations with integer coefficients and integer solutions. ::: dBox ::: definitionT Definition 3.6 (Linear Diophantine equation (LDE)). An equation of the form \\(ax + by = c\\) , or can also be written as \\(ax = c\\ (\\mathrm{mod}\\ b)\\) , where \\(a\\) , \\(b\\) , \\(c\\) , \\(x\\) , \\(y \\in \\mathbb{Z}\\) . ::: ::: In this section, we try to answer the following problem. ::: problem Problem 3.1 . Given an integer \\(a\\) , \\(b\\) , and \\(c\\) , does a solution \\((x,y)\\) exist and how can you find a solution to an LDE? ::: So how exactly can we prove whether a solution exists? It all relates back to the greatest common divisor, more specifically \\(\\gcd(a,b)\\) . A useful theorem which we'll use in proving this is B\u00e9zout's identity. ::: tBox ::: theoremeT Theorem 3.1 (B\u00e9zout's identity). Let \\(a\\) and \\(b\\) be nonzero integers, and let \\(d = \\gcd(a,b)\\) . Then there exist integers \\(m\\) and \\(n\\) that satisfy: ( \\(ma + nb = d\\) \\) ::: ::: So for an LDE to have a solution, \\(c\\) must be a multiple of \\(d\\) , denoted as \\(d \\mid c\\) , which can be written more formally as: ::: tBox ::: theoremeT Theorem 3.1 (Check if solution exists for LDE). Let \\(d = \\gcd(a,b)\\) . The LDE \\(ax + by = c\\) has a solution if and only if \\(d \\mid c\\) . ::: ::: ::: exampleT Example 3.4 . Does a solution exists to the LDE \\(60x + 33y = 9\\) ? Compute the greatest common divisor of \\(60\\) and \\(33\\) . ( \\(\\gcd(60,33) = \\gcd(33,27) = \\gcd(27,6) \\gcd(6,3) = \\gcd(3,0) = 3\\) \\) Determine whether a solution exists, if \\(\\gcd(12,8) = 4 \\mid 68\\) or there exists an integer \\(k\\) , where ( \\(3 \\mid 9 \\Longleftrightarrow 9 = 3k\\) \\) which is true for \\(k = 3\\) , thus a solution exist. ::: Once it's determined a solution exists for the LDE, we want to way to derive the general solution \\((x,y)\\) , which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Solutions to LDE). Let \\(d = \\gcd(a,b)\\) where \\(a \\neq 0\\) and \\(b \\neq 0\\) . If \\((x,y) = (x_0,y_0)\\) is a solution to the LDE \\(ax + by = c\\) , then all solutions are given by ( \\(x = x_0 + \\frac{b}{d}t \\qquad y = y_0 - \\frac{a}{d}t\\) \\) for all \\(t \\in \\mathbb{Z}\\) . We may write the solution set as ( \\(\\Big\\{\\Big(x_0 + \\frac{b}{d}t\\Big), \\Big(y_0 - \\frac{a}{d}t\\Big) : t \\in \\mathbb{Z}\\Big\\}\\) \\) ::: ::: ::: exampleT Example 3.5 . Solve the following LDE \\(60x + 33y = 9\\) or \\(60x = 9\\ (\\mathrm{mod}\\ 33)\\) . As we proved before, a solution exists where \\(d = \\gcd(60,33) = 3\\) . First step is a finding a solution to B\u00e9zout's identity, where there exists integers \\(m\\) and \\(n\\) that satisfy \\(am + bn = d\\) . ( \\(60m + 33n = 3\\) \\) such that \\(m = 5\\) and \\(n = -9\\) satisfies this equation. Refer to the section after. Then to get \\(x_0\\) and \\(y_0\\) , we multiply \\(m\\) and \\(n\\) by \\(3\\) to get the original LDE equation. ( \\(3\\big[60n + 33n = 3\\big] = 60(3n) + 33(3n) = 9\\) \\) such that \\(x_0 = 15\\) and \\(y_0 = -27\\) satisfies the equation \\(60x_0 + 33y_0 = 9\\) . Finally, we just apply the theorem to get the general solution for all \\(t \\in \\mathbb{Z}\\) ( \\(x = 15 + \\frac{33}{3}t = 15 + 11t \\qquad y = -27 - \\frac{60}{3}t = -27 - 20t\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Solving for Initial Solution The trickiest part is solving for integers \\(m\\) and \\(n\\) which satisfies B\u00e9zout identity to get the initial solutions \\(x_0\\) and \\(y_0\\) . Let's use the previous example, \\(60m + 33n = 9\\) . The process involves by working backwards through your steps in the Euclidean Algorithm: \\(60 = 33(1) + 27\\) \\(33 = 27(1) + 6\\) \\(27 = 6(4) + 3\\) Reformat the Euclidean Algorithm, such that \\(r = a - bq\\) : \\(27 = 60 - 33(1) \\hfill (3.5)\\) \\(6 = 33 - 27(1) \\hfill (3.6)\\) \\(3 = 27 - 6(4) \\hfill (3.7)\\) Now use substitution. Refer to the text in red: \\( \\(3 = 27 - {\\color{red}6}(4)\\) \\) \\( (3 = 27 - \\big {\\color{red}33 - 27(1)}\\big \\tag*{Substitute \\(6\\) using Eq. \\(3.6\\) }\\) \\) \\( \\(3 = 27 - 33(4) + 27(4) \\tag*{Expand}\\) \\) \\( \\(3 = {\\color{red}27}(5) - 33(4) \\tag*{Combine like terms}\\) \\) \\( (3 = \\big {\\color{red}60 - 33(1)}\\big - 33(4) \\tag*{Substitute \\(27\\) using Eq. 3.5}\\) \\) \\( \\(3 = 60(5) - 33(5) - 33(4) \\tag*{Expand}\\) \\) \\( \\(3 = 60(5) - 33(9) \\tag*{Combine like terms}\\) \\) \\( \\(3 = 60(5) + 33(-9)\\) \\) -4ex -1ex -.4ex 1ex .2ex Congruence In number theory, congruence is nothing more than a statement about divisibility. ::: dBox ::: definitionT Definition 3.7 (Congruence). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) , if \\(a\\) is congruent to \\(b\\) modulo \\(n\\) , then we write ( \\(a \\equiv b\\ (\\mathrm{mod}\\ n)\\) \\) which provides that \\(n \\mid (a - b)\\) . ::: ::: So what information can we take away from \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) ? \\(a\\) and \\(b\\) have the same remainder when divided by \\(n\\) \\(a = kn + b\\) for some integer \\(k\\) \\(n \\mid (a-b)\\) There are also some useful algebraic properties of congruences. ::: tBox ::: theoremeT Theorem 3.1 . If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(c \\equiv d \\ (\\mathrm{mod}\\ n)\\) , then: \\(a + c \\equiv b + d \\ (\\mathrm{mod}\\ n)\\) \\(a - c \\equiv b - d \\ (\\mathrm{mod}\\ n)\\) \\(ac \\equiv bd \\ (\\mathrm{mod}\\ n)\\) ::: ::: The algebra of congruence is sometime referred to as clock arithmetic. For example, we can represent modulo \\(12\\) as a clock (where \\(0\\) represents \\(12\\) ). ::: center ::: The clock demonstrate that every integer is congruent to at least one of \\(0 \\dots 11\\) modulo \\(12\\) (row highlighted in pink). Just like a clock, when we go over \\(12\\) , we start over at \\(1\\) , and so the same thing applies with modulo, where \\(1 \\equiv 13 \\ (\\mathrm{mod}\\ 12)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Congruence Class Refer back to [section 2.1.2]{.underline} for a recap. Congruence is another type of equivalence relationsa relation that satisfies all three: Reflexive: \\(a \\equiv a \\ (\\mathrm{mod}\\ n)\\) Symmetric: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) , then \\(b \\equiv a \\ (\\mathrm{mod}\\ n)\\) Transitive: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(b \\equiv c \\ (\\mathrm{mod}\\ n)\\) , then \\(a \\equiv c \\ (\\mathrm{mod}\\ n)\\) As a result, we can form equivalence classes, or otherwise known as ::: dBox ::: definitionT Definition 3.8 (Congruence class). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . The congruence class of modulo \\(n\\) is ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b \\equiv a \\ (\\mathrm{mod}\\ n)\\}\\) \\) Note that ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b = a + kn \\text{ for } k \\in \\mathbb{Z}\\}\\) \\) ::: ::: For example, in congruence modulo \\(2\\) , we have \\([0]_2 = \\{0, \\pm 2, \\pm 4, \\pm 6, \\cdots\\}\\) \\([1]_2 = \\{\\pm 1, \\pm 3, \\pm 5, \\pm 7, \\cdots\\}\\) The congruence classes of \\(0\\) and \\(1\\) are, respectively, the sets of even and odd integers. ::: tBox ::: theoremeT Theorem 3.1 (Equality of congruence classes). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . We then have that \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) if and only if ( \\(_n = [b]_n\\) \\) ::: ::: Referring back to the clock diagram, in congruence modulo \\(12\\) , we have: \\( \\([0]_{12} = [12]_{12} = [24]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\} = \\{\\cdots -24, -12, 0, 12, 24, \\cdots\\}\\) \\) You may have notice that the distinct congruence classes are \\([0], [1], \\cdots, [11]\\) : \\([0]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\}\\) \\([1]_{12} = \\{1, 1 \\pm 12, 1 \\pm 24, \\cdots\\}\\) \\(\\vdots\\) \\([11]_{12} = \\{11, 11 \\pm 12, 11 \\pm 24, \\cdots\\}\\) In congruence modulo \\(n\\) , we can say for \\(n > 2\\) , the distinct congruence classes are \\([0], [1], \\cdots, [n-1]\\) . -4ex -1ex -.4ex 1ex .2ex Principles of Counting We'll start off with the fundamentals, that is counting. Of course, most people know how to count, but combinatorics applies mathematical operations to count quantities that are much too large to be counted the conventional way. -3ex -0.1ex -.4ex 0.5ex .2ex Sum Rule Combinatorics is often concerned with how things are arrangeda way objects could be grouped. One of the most basic rules regarding arrangements is the rule of sum. ::: tBox ::: theoremeT Theorem 3.1 (Sum rule). Suppose that we are given disjoint sets \\(X\\) and \\(Y\\) . If \\(|X| = m\\) and \\(|Y| = n\\) , then ( \\(|X\\ \\cup\\ Y | = m + n\\) \\) ::: ::: Then we can generalized this theorem for more than two disjoints sets. ::: tBox ::: theoremeT Theorem 3.1 (Generalized sum rule). If we are given pairwise disjoint sets \\(X_i\\) , where \\(1 \\leq i \\leq m\\) , so that \\(|X_i| = m\\) , then ( \\(\\bigg|\\bigcup\\limits_{i=1}^m X_i \\bigg| = \\sum_{i=1}^m |X_i|\\) \\) ::: ::: So what about sets that are not disjoint? For example, we have two sets \\(X = \\{1,2,3\\}\\) and \\(Y = \\{2,3,4\\}\\) . If we use the sum rule, where \\(|X| = 3\\) and \\(|Y| = 3\\) , we should get: \\( \\(|X| + |Y| = 3 + 3 = 6\\) \\) However, \\(X\\ \\cup\\ Y = \\{1,2,3,4\\}\\) , where \\(|X \\cup Y| = 4 \\neq 6\\) , such that \\( \\(|X\\ \\cup\\ Y| < |X| + |Y|\\) \\) which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Boole's inequality). For any sets \\(A_i\\) , we have that ( \\(\\bigg|\\bigcup\\limits_{i=1}^m A_i \\bigg| \\leq \\sum_{i=1}^m |A_i|\\) \\) ::: ::: Equivalently, we can rewrite the sum rule as \\(|A\\ \\cup\\ B| = |A| + |B| - |A\\ \\cap\\ B|\\) , where we subtract the cardinality of elements that are common. For disjoint sets that is \\(\\varnothing\\) , compared to joint sets, resulting in \\(\\leq\\) . And so this brings the final theorem which will cover in this section. ::: tBox ::: theoremeT Theorem 3.1 (Principle of Inclusion-Exclusion). Let \\(X_1, X_2, \\dots, X_n\\) be finite sets. We then have that ( \\(\\bigg|\\bigcup\\limits_{1\\leq1\\leq n} X_i \\bigg| = |X_1| + |X_2| + \\dots + |X_n|\\) \\) ( \\(\\qquad\\qquad \\:-\\;|X_1\\ \\cap\\ X_2| - |X_1\\ \\cap\\ X_3| - \\dots - |X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;|X_1\\ \\cap\\ X_2\\ \\cap\\ X_3| + |X_1\\ \\cap\\ X_2\\ \\cap\\ X_4| + \\dots + |X_{n-2}\\ \\cap\\ X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;(-1)^{n-1}|X_1\\ \\cap\\ X_2\\ \\cap\\ \\cdots\\ \\cap\\ X_n|\\) \\) ::: ::: We can break this principle line-by-line: Take the sum of the cardinalities of the sets, as you would in a disjoint union. Subtract off the cardinalities of the pairwise intersections of the sets Add the cardinalities of the triple intersections and so on. The signs \\((-1)^{n-1}\\) depend on the parity of the number of sets intersected. For example, if there is three set in the intersection ::: center ::: \\( \\(|A\\ \\cup\\ B\\ \\cup\\ C| = |A| + |B| + |C| - |A\\ \\cap\\ B| - |A\\ \\cap\\ C| - |B\\ \\cap\\ C| + |A\\ \\cap\\ B\\ \\cap C|\\) \\) You can also refer to the visualization below, if you have trouble understanding the reason behind it ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Product Rule Another basic rules regarding arrangements is the rule of product. ::: tBox ::: theoremeT Theorem 3.1 (Product rule). If a task \\(X\\) can be performed in \\(m\\) ways and a task \\(Y\\) can be performed in \\(n\\) ways, then we have that \\(X\\) and \\(Y\\) can be performed together in \\(mn\\) ways. ::: ::: One example is with cards. How many cards are in a standard deck of cards? ::: center ::: Equivalently, you can think of the suit of the card and the rank of the card as two tasksthere are \\(4\\) suits and \\(13\\) . The product rule, \\(4 \\times 13\\) , tells us there are \\(52\\) card. Likewise, we can generalize this theorem for more than two tasks. ::: tBox ::: theoremeT Theorem 3.1 (Generalized product rule). If tasks \\(X_i\\) can be performed in \\(m_i\\) ways where \\(i \\leq i \\leq n\\) , then we have ( \\(m_1m_2 \\cdots m_n\\) \\) way tasks can be performed together. ::: ::: ::: exampleT Example 3.6 . How many ways can you make a license plate with three-digit number (not including zero) and three letters? For starters, let's focus on the three-digit number. For each digit, there can be nine different ways, \\(1 \\dots 9\\) , we can choose a number. ( \\(9 \\times 9 \\times 9 = 9^3\\) \\) Then for the three letters, for each choice, there can be twenty-six different ways, \\(a \\dots z\\) , we can choose a letter. ( \\(26 \\times 26 \\times 26 =26^3\\) \\) In total, there are \\(9^326^3\\) different ways we can make them. ::: -3ex -0.1ex -.4ex 0.5ex .2ex The Pigeonhole Principle The Pigeonhole Principle is a simple, but powerful tool when counting objects. The metaphors used to describe the principle typically vary, but they all follow the same analogy of inserting a finite set into a smaller finite set. You can think of it like this, if \\(n\\) pigeons fly into \\(m\\) pigeonholes and \\(n > m\\) , then at least one hole must contain two or more pigeons. ::: center ::: ::: tBox ::: theoremeT Theorem 3.1 (The Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , then at least one container must contain more than one item. ::: ::: In hindsight, it is pretty is obvious, thus, we move immediately to applicationsranging from the totally obvious to the extremely subtle. ::: exampleT Example 3.7 . If you choose a set of three non-negative integers, must there be at least two who are both even or both odd. Yes, because we have three items and only two container (odd or even), therefore one container must contain more than one item, which could be odd or even. ::: ::: exampleT Example 3.8 . Let \\(A = \\{1,2,3,4,5,6,7,8\\}\\) . If five integers are selected form \\(A\\) , must at least one pair of the integers have a sum of \\(9\\) ? You can think of the five selected integers as the number of items, where \\(a_n\\) represent a distinct number in the set \\(A\\) . ( \\(a_1,\\ a_2,\\ a_3,\\ a_4, \\text{ and }\\ a_5\\) \\) Then, all the disjoint subsets that have a sum of \\(9\\) is our container. ( \\(\\{1,8\\},\\ \\{2,7\\},\\ \\{3,6\\}, \\text{ and}\\ \\{4,5\\}\\) \\) Applying the pigeonhole principle, because there are more items, \\(n = 5\\) , than there are containers, \\(m = 4\\) . Then at least one container must contain more than one item. In other words, at least one of the disjoint subsets will contain two distinct integers, which will have a sum of \\(9\\) . ::: A generalization of the pigeonhole principle states: ::: tBox ::: theoremeT Theorem 3.1 (Generalized Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , there is at least one container with \\(\\lceil n/m \\rceil\\) items. ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Permutation Before going over combinations, let's talk about permutation, where order matters. You can think of it as an ordered combinations. For example, the set of elements \\(a\\) , \\(b\\) , and \\(c\\) has six permutations: \\( \\(abc \\hspace{1cm} acb \\hspace{1cm} bac \\hspace{1cm} bca \\hspace{1cm} cab \\hspace{1cm} cba\\) \\) The number of permutations can be derived using the product rule. Suppose we have a set of \\(n\\) elements: For our first choice (or task), we have \\(n\\) ways of picking an element. For our second choice, we now have \\(n-1\\) ways of picking an element \\(\\vdots\\) For our \\(n\\) th choice, there's only one element left, so we only have \\(1\\) way of choosing an element. So by the product rule, there are \\( \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 = n!\\) \\) ways to perform the entire operation with no repetitions. In other words, there are \\(n!\\) permutations of a set of \\(n\\) elements. Suppose in the previous example, we want to know how many permutation there one only using two elements, instead of all three. We can define an ordered arrangement of \\(r\\) elements taken from the set of \\(n\\) elements as an \\(r\\) -permutation. ::: dBox ::: definitionT Definition 3.9 (Permutation). The number of \\(r\\) -permutations of a set of \\(n\\) elements is denoted \\(P(n,r)\\) . If \\(0 \\leq r \\leq n\\) , then ( \\(P(n,r) = n \\times (n-1) \\times \\cdots \\times (n - r + 1) = \\frac{n!}{(n-r)!}\\) \\) ::: ::: So now can calculate the \\(2\\) -permutation of \\(\\{a,b,c\\}\\) , resulting in \\( \\(P(3,2) = \\frac{3!}{(3-2)!} = \\frac{3!}{1!} = \\frac{6}{1} = 6\\) \\) which are \\( \\(ab \\hspace{1cm} ac \\hspace{1cm} ba \\hspace{1cm} bc \\hspace{1cm} ca \\hspace{1cm} cb\\) \\) -4ex -1ex -.4ex 1ex .2ex Combination We can now define an unordered arrangement of \\(r\\) elements of a set as an \\(r\\) -combination. ::: dBox ::: definitionT Definition 3.10 (Combination). Let \\(n\\) and \\(r\\) be non-negative integers, with \\(r \\leq n\\) . The symbol \\(\\binom{n}{r}\\) , read \\\" \\(n\\) chooses \\(r\\) \\\", denotes the number of subsets of size \\(r\\) that can be formed from a set of \\(n\\) elements. ::: ::: ::: list There's two ways to denote an \\(r\\) -combination, which is by \\(\\binom{n}{r}\\) or \\(C(n,r)\\) ::: Using the relation between permutation \\(P(n.r)\\) and combination, gives us an important formula: \\( \\(C(n.r) = \\binom{n}{r} = \\frac{n!}{(n-r)!r!}\\) \\) ::: exampleT Example 3.9 . Given a set of four people: Ann, Bob, Cyd, and Dan. List all the combinations that can be made with only three people. Note that order doesn't matter, so a subset consisting of \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) is the same as the subset consisting of \\(\\{\\) Dan, Cyd, Bob \\(\\}\\) . Following this fact, then the \\(3\\) -combination can be obtained by leaving one out of the elements of the set: \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Cyd \\(\\}\\) or alternatively ( \\(C(4,3) = \\binom{4}{3} = \\frac{4!}{(4-3)!3!} = \\frac{24}{6} = 4\\) \\) ::: As a follow up, there are special cases of combinations using this equation. ::: tBox ::: theoremeT Theorem 3.1 (Basic properties of combination). Let \\(n\\) be an integer: \\(\\displaystyle\\binom{n}{0} = \\binom{n}{n} = 1\\) \\(\\displaystyle\\binom{n}{1} = \\binom{n}{n-1} = n\\) \\(\\displaystyle\\binom{n}{2} = \\frac{n(n-1)}{2}\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Combinations There also some useful of identities that you can form using \\(C(n,r)\\) . They seem mysterious at first, but there's usually a good reason for them. Combinations have a recursive quality that is captured in the following theorem. ::: tBox ::: theoremeT Theorem 3.1 (Recursive property). For integers \\(n \\geq 1\\) and \\(r \\geq 1\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n - 1}{r - 1} + \\binom{n - 1}{r}\\) \\) ::: ::: You will often see this depicted as Pascal's formula. As an example, we can calculate \\(\\binom{6}{2}\\) using: \\( \\(\\binom{6}{2} = \\binom{5}{1} + \\binom{5}{2} = 5 + 10 = 15\\) \\) Another important property of combinations is their symmetry. ::: tBox ::: theoremeT Theorem 3.1 (Symmetry property). For integers \\(n \\geq 0\\) and \\(r \\geq 0\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n}{n - r}\\) \\) ::: ::: Using the previous example, we know that: \\( \\(\\binom{6}{2} = \\binom{6}{4}\\) \\) At first, it might not make sense, but it will prove to be useful in the next section, when we go over Pascal's trianglewhich is an arrangement of the combinations that makes them simple to remember. Lastly, we have this identity. ::: tBox ::: theoremeT Theorem 3.1 (Sum of squares combinations). For \\(n \\leq 0\\) , we have that ( \\(\\sum_{r=0}^n\\binom{n}{r}^2 = \\binom{2n}{n}\\) \\) ::: ::: As such, we can express the sum of squares as a single combination, shown below: \\( \\(\\sum_{r=0}^{25} \\binom{25}{r}^2 = \\binom{2 \\cdot 25}{25} = \\binom{50}{25}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Pascal's Triangle The rows of Pascal's triangle are indexed by non-negative integers: \\(0\\) , \\(1\\) , \\(2\\) , and etc. In the \\(n\\) th row, we have \\(\\binom{n}{r}\\) , where \\(1 \\leq r \\leq n\\) and the values of \\(r\\) increase from left to right. The following are the first seven rows of Pascal's triangle. Then, lastly we have this identity. ::: center ::: Though, they are more commonly represented as their integer counterpart. ::: center ::: You can see many patterns of how combinations are related in the triangle, such symmetry in a given row and the recursive property of combinations, which we discussed prior to this. One important use of combinations is in expanding polynomial expressions, such as \\((x + y)^n\\) . The binomial theorem generalizes this formula. ::: tBox ::: theoremeT Theorem 3.1 (Binomial theorem). Let \\(n\\) be non-negative integers and let \\(x\\) , \\(y\\) be variables. ( \\((x + y)^n = \\sum_{r = 0}^n\\binom{n}{r}x^{n-r}y^r\\) \\) ( \\(\\qquad\\quad\\ \\ = \\binom{n}{0}x^n + \\binom{n}{1}x^{n-1}y + \\binom{n}{2}x^{n-2}y^2 + \\cdots + \\binom{n}{n-1}xy^n + \\binom{n}{n}y^n\\) \\) ::: ::: In other words, the triangular arrangement of numbers gives us the coefficients in the expansion of any binomial expression. ::: exampleT Example 3.10 . Expand the following expression \\((x+y)^n\\) for \\(n=6\\) : Using the binomial theorem: ( \\((x + y)^6 = \\binom{6}{0}x^6 + \\binom{6}{1}x^5y + \\binom{6}{2}x^4y^2 + \\binom{6}{3}x^3y^3 + \\binom{6}{4}x^2y^4 + \\binom{6}{5}xy^5 + \\binom{6}{6}y^6\\) \\) If we refer back to Pascal's triangle, then we can easily substitute the binomial coefficient with its respective integers, as such: ( \\((x + y)^6 = x^6 + 6x^5y + 15x^4y^2 + 20x^3y^3 + 15x^2y^4 + 6xy^5 + y^6\\) \\) :::","title":"Number Theory and Combinatorics"}]}