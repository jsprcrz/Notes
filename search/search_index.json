{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Listed below are all the collections of notes taken for the courses I've took.","title":"Preface"},{"location":"#introduction","text":"Listed below are all the collections of notes taken for the courses I've took.","title":"Introduction"},{"location":"Term/F2021/","text":"MTH312 Differential Equations and Vector Calculus ELE302 Electronic Networks PCS224 Solid State Physics COE328 Engineering Algorithms and Data Structures","title":"Fall 2021"},{"location":"Term/W2021/","text":"ECN801 Principle of Engineering Economics MTH240 Calculus 2 ELE202 Electric Circuit Analysis PCS125 Physics: Waves and Fields","title":"Winter 2021"},{"location":"Term/W2022/","text":"MTH314 Discrete Mathematics for Engineers ELE404 Electronic Circuits I COE428 Engineering Algorithms and Data Structures COE528 Object Oriented Eng Analysis and Design","title":"Winter 2022"},{"location":"W2022/COE428/COE428/","text":"COE428 \u00b6 This will cover various topics for the course COE428: Algorithms and Data Structures, using the textbook, Introduction to Algorithms , by T. Cormen, C. Leiserson, R. Rive, and lectures notes provided by the professor, Dr. Reza Samavi. Other resources used: Abdul Bari - Algorithms Last Updated: 2022-04-16 Introduction to Algorithms \u00b6 Algorithm Analysis \u00b6 An algorithm is a sequence of computational step that transform the input into the output. You will typically see them displayed as pseudocode shown below: It is very convenient to classify algorithms based on the relative amount of time or relative amount of space they require and specify as a function of the input size. Thus, we have the notions of: - Time Complexity: Running time of the program as a function of the size of input. - Space Complexity: Amount of computer memory required during the program execution, as a function of the input size. - Running Time The time complexity of an algorithm can be measured by characterizing running time as a function, \\(\\color{ocre2}T(n)\\) , of the input size, \\(\\color{ocre2}n\\) . We count the number of primitive operations that are executed: Assigning a value to a variable Calling a method Performing an arithmetic operation Comparing two values Indexing into an array Returning from a method ::: algorithm ::: algorithmic \\(temp = a\\) ; \\(a = b\\) ; \\(b = temp\\) ; ::: ::: For this example, we say the running time is \\(\\color{ocre2}T(n) = 3\\) , since there's three primitive operations executed by an algorithm. When it comes to loops, it becomes a bit more complex. ::: algorithm ::: algorithmic \\(x = s[0]\\) \\(x = s[i]\\) ::: ::: We define the size of the input array to be \\(\\color{ocre2}n\\) : In line 1, its indexing an array, \\(\\color{ocre2}s[0]\\) , then assigning it to \\(\\color{ocre2}x\\) In line 2, inside the for loop: It first assigns \\(\\color{ocre2}i = 1\\) On each iteration, it makes a comparison, \\(\\color{ocre2}<\\) , for \\(\\color{ocre2}n\\) times In line 3, the if statement repeats for : Its indexing an array, \\(\\color{ocre2}s[i]\\) , then makes a comparison, \\(\\color{ocre2}>\\) In line 4, there's two possibilities that could occur: If true, then its indexing an array, \\(\\color{ocre2}s[i]\\) , then assigning it to \\(\\color{ocre2}x\\) If false, then it won't execute At the end, \\(\\color{ocre2}i++\\) , is incremented, then assigned back to \\(\\color{ocre2}i\\) The last line returns \\(\\color{ocre2}x\\) The number of primitive operations executed by algorithm can be characterized in two ways: Best-Case Analysis: \\(\\color{ocre2}T(n) = 2 + 1 + n + (n-1)(2 + 0 + 2) + 1 = 5n\\) Worst-Case Analysis: \\(\\color{ocre2}T(n) = 2 + 1 + n + (n-1)(2 + 2 + 2) + 1 = 7n - 2\\) ... to which it is bounded by two linear functions, \\(\\color{ocre2}5n \\leq T(n) \\leq 7n - 2\\) -4ex -1ex -.4ex 1ex .2ex Classes of Functions Let's first go over some common functions that characterize the running time of an algorithm: Constant function: \\(\\color{ocre2}f(n) = c\\) Linear function: \\(\\color{ocre2}f(n) = n\\) n-log-n function: \\(\\color{ocre2}f(n) = n\\log{n}\\) Quadratic function: \\(\\color{ocre2}f(n) = n^2\\) Cubic function: \\(\\color{ocre2}f(n) = n^3\\) Exponents: \\(\\color{ocre2}f(n) = b^n\\) Logarithms: \\(\\color{ocre2}f(n) = \\log_2{n}\\) A review from MTH is the summation formula, which will be useful when analyzing the time complexity of algorithms: Arithmetic series: \\(\\displaystyle\\color{ocre2}\\sum_{k = 0}^{n} k = 1 + 2 + \\cdots + n = \\frac{n(n+1)}{2}\\) Geometric series: \\(\\displaystyle\\color{ocre2}\\sum_{k = 0}^{n} x^k = 1 + x + x^2 + \\cdots + x^n = \\frac{1-x^{n+1}}{1-x}\\) ... and the following terms which you might recall from coding: \\(\\color{ocre2}\\lfloor x \\rfloor\\) or floor(x) is largest integer less than to equal to \\(x\\) . \\(\\color{ocre2}\\lceil x \\rceil\\) or ceiling(x) is least integer greater than to equal to \\(x\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Growth Rate of Running Time When choosing between algorithms, we care most about asymptotic performance; how the algorithm increases with the input size. ::: center ::: If you notice, as the input size increases, certain class of functions grows much more rapidly than others. ::: center ::: If we let the value of the growth-rate function represent the units of time, an algorithm with the function \\(\\color{ocre2}f(n) = \\log_2{n}\\) would be much more efficient than an algorithm with the function \\(\\color{ocre2}f(n) = 2^n\\) . In general, the order-of-growth can be classified to be: \\( \\(\\color{ocre2}1 < \\log{n} < \\sqrt{n} < n < n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n\\) \\) Analyzing and Designing Algorithms \u00b6 In this lecture, we will go over three different types of sorting algorithm: insertion sort, merge sort, and selection sort. The purpose of sorting algorithms is to solve the following problem: Input: A sequence of \\(\\color{ocre2}n\\) numbers \\(\\color{ocre2}\\langle a_1, a_2, \\dots, a_n \\rangle\\) Output: A permutation (reordering) \\(\\color{ocre2}\\langle a'_1, a'_2, \\dots, a'_n \\rangle\\) of the input sequence such that \\(\\color{ocre2}a'_1 \\leq a'_2 \\leq \\dots \\leq a'_n\\) . ::: dBox ::: definitionT Definition 2.1 (Key). The sequence are typically stored in array. We also refer to the number as keys. ::: ::: -4ex -1ex -.4ex 1ex .2ex Insertion Sort Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. ::: algorithm [Insertion-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: Iterate from [ A[1] ]{style=\"background-color: light-gray\"} to [ A[n] ]{style=\"background-color: light-gray\"} over the array. Compare the current element [ key = A[j] ]{style=\"background-color: light-gray\"} to its predecessor. If the element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element. ::: list Note that iterations starts at [ A[1] ]{style=\"background-color: light-gray\"}, not [ A[0] ]{style=\"background-color: light-gray\"}. For the sake of convenience, we assume a fictitious record [ A[0] ]{style=\"background-color: light-gray\"} as the sentinel value with key of \\(\\color{ocre2}-\\infty\\) . ::: It maybe easier to visualize this using images to better understand the psuedocode written. Suppose we start out with the following array with 6 elements. ::: center ::: If we apply the insertion sort algorithm, the following array would be sorted as shown below. Let's denote the [ key ]{style=\"background-color: light-gray\"} in green. ::: center ::: Since we care most about the asymptotic performance, we are interested on finding the running time \\(\\color{ocre2}T(n)\\) . ::: algorithm ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: The lectures note and textbook goes in-depth deriving the following running time of insertion sort: \\( \\(\\color{ocre2}T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^n t_j + c_6\\sum_{j = 2}^n (t_j - 1) + c_7\\sum_{j = 2}^n (t_j - 1) + c_8(n - 1)\\) \\) where \\(\\color{ocre2}t_j\\) is the number of times the while loop is executed for that value of \\(\\color{ocre2}j\\) . The main takeaway is knowing how it sorts and the function for best and worst-case running time of the following algorithm is. In the next lecture, we will go more in-depth on analyzing the time complexity using asymptotic notations, which simplifies all of this stuff. -3ex -0.1ex -.4ex 0.5ex .2ex Best-Case Complexity The best-case scenario is when the array is already sorted. ::: center ::: In this example, the while loop does the comparison but never enters the loop, since it always find that [ A[i] ]{style=\"background-color: light-gray\"} is always less than or equal to [ key ]{style=\"background-color: light-gray\"}. ::: center ::: Thus, \\(\\color{ocre2}t_j = 1\\) , we can derive the number of comparisons for every outer loop iteration: \\( \\(\\color{ocre2}\\sum_{j=2}^n t_j \\to \\sum_{j=2}^n 1 = (n - 2) + 1 = n - 1\\) \\) Substituting this in the equation to the running time simplifies \\(\\color{ocre2}T(n)\\) to \\( \\({\\color{ocre2} \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5(n - 1) + c_8(n - 1) \\\\ &= (c_1 + c_2 + c_4 + c_5 + c_8)n - (c_2 + c_4 + 5 + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(\\color{ocre2}c_n\\) simplify to some constants \\(\\color{ocre2}a\\) and \\(\\color{ocre2}b\\) then \\( \\(\\color{ocre2}T(n) = an - b\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Worst-Case Complexity The worst-case scenario is when the array is sorted in reversefrom largest to smallest. ::: center ::: In this example, \\(\\color{ocre2}t_j\\) has to compare with all elements to the left \\(\\color{ocre2}j\\) -th positioncompare with \\(\\color{ocre2}j - 1\\) elements. Thus, \\(\\color{ocre2}t_j = j\\) , we can derive the number of comparisons for every outer loop iteration \\( \\(\\color{ocre2}\\sum_{j = 2}^{n} t_j \\to \\sum_{j = 2}^{n} j = 2 + 3 + 4 + \\dots + n = \\bigg[\\sum_{j=1}^n j\\bigg] - 1 = \\frac{n(n+1)}{2} - 1\\) \\) and as well the number of moves inside the while loop: \\( \\(\\color{ocre2}\\sum_{j = 2}^{n} (t_j - 1) \\to \\sum_{j = 2}^{n} (j - 1) = 1 + 2 + 3 + \\dots + n - 1 = \\frac{n(n-1)}{2}\\) \\) Substituting this in the equation to the running time simplifies \\(\\color{ocre2}T(n)\\) to \\( \\({\\color{ocre2} \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + (c_6 + c_7)\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_8(n - 1) \\\\ &= \\bigg[\\frac{c_5}{2} + \\frac{c_6}{2} + \\frac{c_7}{2}\\bigg]n^2 + (c_1 + \\dots + c_8)n - (c_2 + \\dots + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(\\color{ocre2}c_n\\) simplify to some constants \\(\\color{ocre2}a\\) , \\(\\color{ocre2}b\\) and \\(\\color{ocre2}c\\) then \\( \\(\\color{ocre2}T(n) = an^2 + bn - c\\) \\) -4ex -1ex -.4ex 1ex .2ex Merge Sort Merge sort closely follows the divide-and-conquer paradigm. Intuitively, it operates as follows. Divide: Divide the \\(\\color{ocre2}n\\) -element sequence to be sorted into two subsequences of \\(\\color{ocre2}n = 2\\) elements each. Conquer: Sort the two subsequences recursively using merge sort. Combine: Merge the two sorted subsequences to produce the sorted answer. ::: algorithm [Merge-Sort]{.smallcaps} \\((A,p,r) \\to A[p \\dots r]\\) ::: algorithmic \\(q = \\lfloor (p + r)/2 \\rfloor\\) [Merge-Sort( \\(A,p,q\\) )]{.smallcaps} [Merge-Sort( \\(A,q + 1,r\\) )]{.smallcaps} [Merge( \\(A,p,q,r\\) )]{.smallcaps} ::: ::: Split the deck into two piles, until these become simple enoughan array of size \\(\\color{ocre2}1\\) . Sort the left pile and sort the right pile using [ Merge-Sort() ]{style=\"background-color: light-gray\"}. Merge both piles into the final pile. ::: list In the [ Merge-Sort(A,p,r) ]{style=\"background-color: light-gray\"}, the floor function is used to determine [ q ]{style=\"background-color: light-gray\"}, so in the case there's a decimalit will result in an integer, ex. \\(\\color{ocre2}\\lfloor 7.5\\rfloor = 7\\) . ::: It may also be helpful to use a diagram like before to fully understand what's happening. The number in red denotes the order in which steps are processed. ::: center ::: If you prefer are more concrete example, look at the code I wrote on the left-side, which demonstrate how recursion works in this sorting algorithm. Each indent indicates the recursion depth. Step 1 to 3: Calls [ Merge-Sort(A,p,q) ]{style=\"background-color: light-gray\"} to split the left children with different values of \\(\\color{ocre2}q\\) and \\(\\color{ocre2}r\\) (passing parameter by value). Step 4: Since left child can no longer split, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on right child. Step 5: If both left and right child are already split, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Step 6: Trace back to tree structure and find the node that does not complete the splitting, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on the right children. Step 7 to 8: The same process is done as for Step 3 and 4. Step 9: Like in Step 5, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Most of the steps are just repeated for the other half of the array, until all children have complete the splitting, then they are merged together. -3ex -0.1ex -.4ex 0.5ex .2ex Merge Algorithm The key operation of the merge sort algorithm is the merging of two sorted sequences, after divide and conquer. ::: algorithm [Merge]{.smallcaps} \\((A,p,q,r) \\to A[p \\dots q]\\) and \\(A[q + 1 \\dots r]\\) where \\(p \\leq q \\leq r\\) ::: algorithmic \\(n_1 = q - p + 1\\) \\(n_2 = r - q\\) Let L[1 ... \\(n_1 + 1\\) ] and L[1 ... \\(n_2 + 1\\) ] be new arrays \\(L[i] = A[p + i - 1]\\) \\(R[j] = A[q + j]\\) \\(L[n_1 + 1] = \\infty\\) \\(R[n_2 + 1] = \\infty\\) \\(i = 1\\) \\(j = 1\\) \\(A[k] = L[i]\\) \\(i = i + 1\\) \\(j = j + 1\\) ::: ::: It may look like a lot, but it's pretty simple. Most of the code are explained in the comments listed in the right. The main focus here is the [ for ]{style=\"background-color: light-gray\"} loop in Line 12 to 17. ::: center ::: The heavily shaded elements in [ A ]{style=\"background-color: light-gray\"} contain values that will be copied over, and heavily shaded elements in [ L[] ]{style=\"background-color: light-gray\"} and [ R[] ]{style=\"background-color: light-gray\"} contain values that have already been copied back into [ A[] ]{style=\"background-color: light-gray\"}. The lightly shaded elements in [ A[] ]{style=\"background-color: light-gray\"} indicate their final value. -3ex -0.1ex -.4ex 0.5ex .2ex Time Complexity Let's discuss the time complexity of the following algorithm, which we can break down to the divide-and-conquer paradigm. The time to split deck takes can be denoted by \\(\\color{ocre2}c_1\\) , as it takes constant timedoes not depend on any input. The time to sort left pile and sort right pile can be denoted by \\(\\color{ocre2}2T(n/2)\\) , due to recursion, where the size is now divided by two, \\(\\color{ocre2}n/2\\) . The time to merge piles can be denoted by \\(\\color{ocre2}c_2n + c_3\\) , as it takes linear timeonly the [ for ]{style=\"background-color: light-gray\"} loop depends on input size, while the rest take constant time, thus simplified to that. The time complexity results to \\( \\(\\color{ocre2}T(n) = c_1 + T(n/2) + T(n/2) + c_2n + c_3\\) \\) Our goal is to determine the most rapidly growing term in \\(\\color{ocre2}T(n)\\) and so we can set a few rules. We set constants \\(\\color{ocre2}c_n\\) to either: \\(\\color{ocre2}0\\) , if they will not be significant in the most rapidly growing term or ... \\(\\color{ocre2}1\\) , if they will be For \\(\\color{ocre2}T(n)\\) , when \\(\\color{ocre2}n > 1\\) , we can set \\(\\color{ocre2}c_1\\) and \\(\\color{ocre2}c_3\\) to \\(\\color{ocre2}0\\) and \\(\\color{ocre2}c_2\\) to \\(\\color{ocre2}1\\) , which simplifies to: \\( \\(\\color{ocre2}T(n) = \\begin{cases}c_1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) ::: list Note that when \\(\\color{ocre2}n = 1\\) , the code inside [ if ... else ]{style=\"background-color: light-gray\"} wont run, as there's only one element \\(\\color{ocre2}p \\nless r\\) or \\(\\color{ocre2}1 \\nless 1\\) , so we write it as a constant. ::: In order to solve the recurrence, when \\(\\color{ocre2}n > 1\\) , we need a base case, so for simplicity, \\(\\color{ocre2}T(1) = 0\\) and we can make a deduction from it. ::: tabu c | c c | c \\(\\color{ocre2}n\\) & \\(\\color{ocre2}T(n/2)\\) & \\(\\color{ocre2}2T(n/2) + n\\) & \\(\\color{ocre2}n\\log_2{n}\\) \\ \\(\\color{ocre2}2\\) & \\(\\color{ocre2}0\\) & \\(\\color{ocre2}2(0) + 2 = 2\\) & \\(\\color{ocre2}2\\) \\ \\(\\color{ocre2}4\\) & \\(\\color{ocre2}2\\) & \\(\\color{ocre2}2(2) + 4 = 8\\) & \\(\\color{ocre2}8\\) \\ \\(\\color{ocre2}8\\) & \\(\\color{ocre2}8\\) & \\(\\color{ocre2}2(8) + 8 = 24\\) & \\(\\color{ocre2}24\\) \\ \\(\\color{ocre2}16\\) & \\(\\color{ocre2}24\\) & \\(\\color{ocre2}2(24) + 16 = 64\\) & \\(\\color{ocre2}64\\) \\ \\(\\color{ocre2}32\\) & \\(\\color{ocre2}64\\) & \\(\\color{ocre2}2(64) + 32 = 160\\) & \\(\\color{ocre2}160\\) \\ ::: Examining the numbers allows us to form an educated guess it is growing by a function of \\(\\color{ocre2}n\\log_2{n}\\) , which can also be deducted by drawing a recursion tree. We start by representing \\(\\color{ocre2}T(n) = 2T(n/2) + n\\) as a graph where we put the non-recursive part ( \\(\\color{ocre2}n\\) in this case) on the top row and put each recursive part on a row below. ::: center ::: Then expand downwards for the next level. ::: center ::: Repeat the same process. Eventually, it will reach a certain height which it reaches the base case and stop. ::: center ::: ::: {#height_avl} If you notice the sum of the non-recursive elements for each level is \\(\\color{ocre2}n\\) . Let's denote the depth or height of the tree as \\(\\color{ocre2}h\\) and so we can say the time complexity is ::: \\( \\(\\color{ocre2}T(n) = n \\times h\\) \\) It will eventually reach the base case which we set to some constant when \\(\\color{ocre2}n=1\\) , where \\(\\color{ocre2}T(1) = 1\\) . We can rewrite the fraction in terms of the depth, \\(\\color{ocre2}h\\) , where \\( \\(\\color{ocre2}\\frac{n}{2^h} = 1 \\to n = 2^h \\to h = \\log_2{n}\\) \\) Thus, the time complexity is \\( \\(\\color{ocre2}T(n) = n\\log_2{n}\\) \\) If you recall the order-of-growth from Lecture 2, we know that \\(\\color{ocre2}n\\log{n} < n^2\\) , and so merge sorting beats insertion sort in the worst-case scenario, as it grows much more slowly. -4ex -1ex -.4ex 1ex .2ex Selection Sort The final sorting algorithm will cover is selection sort. ::: algorithm [Selection-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Select the first element as [ min ]{style=\"background-color: light-gray\"} Compare [ min ]{style=\"background-color: light-gray\"} with the second element. If the second element is smaller than minimum, assign the second element as [ min ]{style=\"background-color: light-gray\"}. Repeat until last element. After each iteration, [ min ]{style=\"background-color: light-gray\"} is placed in the front of the unsorted list. For each iteration, indexing starts from the first unsorted element. The steps are repeated until sorted. Let's use the example as we did for insertion sort, which is the following array with 6 elements. ::: center ::: The first iteration would like something like this. Let's denote the [ min ]{style=\"background-color: light-gray\"} in green and the line to the element it's being compared to. The arrow indicates a swap to be made. ::: center ::: In the next iteration, the first unsorted element is [ A[2] ]{style=\"background-color: light-gray\"}, so it starts at \\(\\color{ocre2}3\\) . ::: center ::: Then it is repeated until all the elements are placed at their correct positions. -3ex -0.1ex -.4ex 0.5ex .2ex Time Complexity As we have covered for the other algorithm, let's analyze the time complexity of selection sort. ::: algorithm ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Combining each one of them, we get the following running time of selection sort: \\( \\(\\color{ocre2}T(n) = c_1n + c_2(n-1) + c_3\\sum_{j=2}^n j + c_4\\sum_{j = 2}^n (j - 1) + c_5(n-1)\\) \\) As we have previously done with insertion sort, we can simplify the summation using the arithmetic series \\( \\({\\color{ocre2} \\begin{split} T(n) &= c_1n + c_2(n-1) + c_3\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + c_4\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_5(n-1) \\\\ &= \\bigg[\\frac{c_3}{2}+\\frac{c_4}{2}\\bigg]n^2 + (c_1 + \\dots + c_5)n - (c_2 + \\dots + c_5) \\end{split}}\\) \\) or equivalently, if we let \\(\\color{ocre2}c_n\\) simplify to some constants \\(\\color{ocre2}a\\) , \\(\\color{ocre2}b\\) and \\(\\color{ocre2}c\\) then \\( \\(\\color{ocre2}T(n) = an^2 + bn - c\\) \\) Comparing it to the other two algorithms discussed, selection sort is on par with insertion sort in the worst-case scenario and so merge sorting is better than selection sort as well. Complexity Analysis \u00b6 -4ex -1ex -.4ex 1ex .2ex Asymptotic Notations As covered briefly in the growth rate of running time, it's hard to determine which algorithm is better with no prior knowledge of the input size, so we consider the asymptotic behavior of the two functions for very large input size \\(\\color{ocre2}n\\) . We use specific notations called asymptotic notations to express mathematical properties of asymptotic efficiency. ::: dBox ::: definitionT Definition 3.1 (Asymptotic efficiency). The study of how the running time of an algorithm increases as the size of the input increases without bound. ::: ::: There are three asymptotic notations, which will go over in this lecture: Big-Oh notation, \\(\\color{ocre2}\\text{O}()\\) , for the upper bound or worst-case complexity Big-Omega notation, \\(\\color{ocre2}\\Omega()\\) , for the lower bound or best-case complexity Theta notation, \\(\\color{ocre2}\\Theta()\\) , for the average bound or average-case complexity We can apply these to the previous lecture, which we covered three different sorting algorithms with varying time complexity: Algorithm Time Complexity Best Worst Insertion Sort \\(\\color{ocre2}\\Omega(n)\\) \\(\\color{ocre2}\\text{O}(n^2)\\) Merge Sort \\(\\color{ocre2}\\Omega(n\\log{n})\\) \\(\\color{ocre2}\\text{O}(n\\log{n})\\) Selection Sort \\(\\color{ocre2}\\Omega(n^2)\\) \\(\\color{ocre2}\\text{O}(n^2)\\) -3ex -0.1ex -.4ex 0.5ex .2ex Big-Oh Notation (O-notation) The notation represents the upper bound of the running time of an algorithm. Thus, it gives the worst-case complexity of an algorithm. \\( \\({\\color{ocre2} \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Some tips for determining \\(\\color{ocre2}\\text{O}()\\) complexity: Ignore the constants: \\( \\(\\color{ocre2}5n \\to n\\) \\) Certain terms dominate other, which we ignore lower order terms: \\( \\(\\color{ocre2}\\text{O}(1) < \\text{O}(\\log{n}) < \\text{O}(n) < \\text{O}(n\\log{n}) < \\text{O}(n^2) < \\cdots < \\text{O}(2^n) < \\cdots < \\text{O}(n!) < \\text{O}(n^n)\\) \\) It might be easier to understand if we have examples to determine the big-Oh notation. ::: exampleT Example 3.1 . Determine the upper bound \\(\\color{ocre2}\\text{O}()\\) for \\(\\color{ocre2}f(n)\\) : \\(\\color{ocre2}f_A(n) = an^2 + bn + c\\) is \\(\\color{ocre2}\\text{O}(n^2)\\) \\(\\color{ocre2}f_B(n) = 2n + 3\\) is \\(\\color{ocre2}\\text{O}(n)\\) \\(\\color{ocre2}f_C(n) = 5 + (15 \\cdot 20)\\) is \\(\\color{ocre2}\\text{O}(1)\\) \\(\\color{ocre2}f_D(n) = n^2\\log{n} + n\\) is \\(\\color{ocre2}\\text{O}(n^2\\log{n})\\) ::: ::: list When writing the big-Oh notation, try to write the closest function to the running time. While the function \\(\\color{ocre2}\\text{O}(n^2)\\) is true for \\(\\color{ocre2}f_B(n)\\) , the function \\(\\color{ocre2}\\text{O}(n)\\) is the closest to \\(\\color{ocre2}f_B(n)\\) . ::: The rules for determining the \\(\\color{ocre2}\\text{O}()\\) complexity are as listed: If \\(\\color{ocre2}g(n) = \\text{O}(G(n))\\) and \\(\\color{ocre2}f(n) = \\text{O}(F(n))\\) , then: \\( \\(\\color{ocre2}f(n) + g(n) = \\text{O}(F(n)) + \\text{O}(G(n)) = \\text{O}(\\text{max}[F(n), G(n)])\\) \\) \\( \\(\\color{ocre2}f(n) \\cdot g(n) = \\text{O}(F(n)) \\cdot \\text{O}(G(n)) = \\text{O}(F(n) \\cdot G(n))\\) \\) If \\(\\color{ocre2}g(n) = \\text{O}(kG(n))\\) , where \\(\\color{ocre2}k\\) is a constant, then \\(\\color{ocre2}g(n) = \\text{O}(G(n))\\) . If \\(\\color{ocre2}f(n)\\) is a polynomial of degree \\(\\color{ocre2}d\\ (P(n) = \\sum_{i=0}^d a_in^i\\) where \\(\\color{ocre2}a_d \\neq 0)\\) , then \\(\\color{ocre2}f(n)\\) is \\(\\color{ocre2}\\text{O}(n^d)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Big-Omega Notation ( \\(\\Omega\\) -notation) The notation represents the lower bound of the running time of an algorithm. Thus, it provides the best-case complexity of an algorithm. \\( \\({\\color{ocre2} \\begin{aligned} \\Omega(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq cg(n) \\leq f(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: The rules for determining \\(\\color{ocre2}\\text{O}()\\) complexity is also true for determining the \\(\\color{ocre2}\\Omega()\\) complexity. Let's use an example from before. ::: exampleT Example 3.2 . Determine the lower bound \\(\\color{ocre2}\\Omega()\\) for \\(\\color{ocre2}f(n) = 2n + 3\\) : If we look at the order-of-growth for functions, \\(\\color{ocre2}T(n)\\) belongs to the linear function, \\(\\color{ocre2}n\\) and if we define our lower and upper bounds as such ( \\({\\color{ocre2}\\overunderbraces{&\\br{2}{\\text{Lower bound}}}% {&1 < \\log{n} < \\sqrt{n} <& n &< n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n&} {& &\\br{2}{\\text{Upper bound}}}}\\) \\) So the lower bound can be defined by any of the following: ( \\(\\color{ocre2}\\Omega(1) < \\Omega(\\log{n}) < \\Omega(n)\\) \\) Similar to the upper bound, we want the function closest to \\(\\color{ocre2}f(n)\\) and so \\(\\color{ocre2}f(n) = 2n + 3\\) is \\(\\color{ocre2}\\Omega(n)\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Theta Notation ( \\(\\Theta\\) -notation) The next notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm, it is used for analyzing the average-case complexity of an algorithm. \\( \\({\\color{ocre2} \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c_1,\\ c_2, \\text{ and } n_0 \\text{ such that } \\\\ &\\ 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Equivalently, \\(\\color{ocre2}f(n)\\) is \\(\\color{ocre2}\\Theta(g(n))\\) if and only if \\(\\color{ocre2}f(n)\\) is both \\(\\color{ocre2}\\text{O}(g(n))\\) and \\(\\color{ocre2}\\Omega(g(n))\\) . One notable example which we used previously is the function \\(\\color{ocre2}f(n) = 2n + 3\\) , which as we demonstrated in previous notations are \\(\\color{ocre2}\\text{O}(n)\\) and \\(\\color{ocre2}\\Omega(n)\\) , thus \\(\\color{ocre2}f(n)\\) is \\(\\color{ocre2}\\Theta(n)\\) . -4ex -1ex -.4ex 1ex .2ex Complexity of Code Structures Loops are considered as dynamic if they depend on input size, otherwise they are static statements, everything within a loop is considered as static statementtakes a constant amount of time, \\(\\color{ocre2}\\text{O}(1)\\) . The complexity is determined by: number of iterations in the loops \\(\\color{ocre2}\\times\\) number of static statement -3ex -0.1ex -.4ex 0.5ex .2ex For Loop The following example is a simple for loop: for (int i = 0; i < n; i++) { // statement } The for loop is a dynamic statement, as it depends on the size of \\(\\color{ocre2}n\\) . We are interested in the amount of times [ statement ]{style=\"background-color: light-gray\"} runs, which determines the time complexity of the following loop. Suppose \\(\\color{ocre2}n = 3\\) then let's determine how many iterations: ::: tabu c c c Iteration & \\(i\\) &\\ & i = 0 &\\ 2 & i = 1 &\\ 3 & i = 2 &\\ 4 & i = 3 &\\ ::: You can see that the loop executes \\(\\color{ocre2}3\\) times or in general, we can say \\(\\color{ocre2}n\\) times. Thus, the time complexity is: \\( \\(\\color{ocre2}n \\cdot 1 = \\text{O}(n)\\) \\) Note that there might be few variations of the for loop. Suppose there are also consecutive statements: for (int i = 0; i < n; i++) { // statement } for (int i = 1; i <= n; i++) { // statement } In both examples, the loop executes for \\(\\color{ocre2}n\\) times. When we have consecutive statements, we would just add them together. If you recall, we ignore any constants of lower order terms. Thus, the time complexity is: \\( \\(\\color{ocre2}\\underbrace{n \\cdot 1}_{\\substack{\\text{The first} \\\\ \\text{for loop}}} + \\underbrace{n \\cdot 1}_{\\substack{\\text{The second} \\\\ \\text{for loop}}} = 2n = \\text{O}(n)\\) \\) Note that this is not always the case for every for loop, as it depends on the initialization, condition test, and update statement. Suppose we have the following for loop to analyze: for (int i = 1; i <= n; i = i * 2) { // statement } Let's list out each iterations of the loop, till \\(\\color{ocre2}k\\) iterations, since we do not know how many times this loop will execute. ::: tabu c c c Iteration & \\(i\\) \\ & i = 1 & \\(2^0\\) \\ 2 & i = 2 & \\(2^1\\) \\ 3 & i = 4 & \\(2^2\\) \\ \u22ee& \u22ee\\ \\(k\\) & i = \\(2^{k - 1}\\) &\\ ::: From the condition, we know that the loop will terminate once [ i > n ]{style=\"background-color: light-gray\"}. So we assume \\(\\color{ocre2}i = n\\) , when it has reach \\(\\color{ocre2}k\\) iterations; our very last iteration. Then we will solve for \\(\\color{ocre2}k\\) : \\( \\({\\color{ocre2} \\begin{split} 2^{k - 1} &= n \\\\ k - 1 &= \\log_2{n} \\\\ k &= \\log_2{n} + 1 \\end{split}}\\) \\) If you recall from earlier, we ignore lower order terms. Thus, the time complexity is: \\( \\(\\color{ocre2}(\\log_2{n} + 1) \\cdot 1 = \\text{O}(\\log_2{n})\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Nested Loop The following example is a nested for loop: for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement } } As we covered earlier, the following for loop executes \\(\\color{ocre2}n\\) times. Suppose now we have an inner loop, which also executes \\(\\color{ocre2}n\\) times, then the statement is run \\(\\color{ocre2}n \\times n\\) times. Thus, the time complexity is: \\( \\(\\color{ocre2}(n \\cdot n) \\cdot 1 = \\text{O}(n^2)\\) \\) The general formula for a nested loop is the time complexity of the outer loop times the inner loops. This also applies if we have a outer while loop with an inner for loop. -3ex -0.1ex -.4ex 0.5ex .2ex If Else Statement The following example is an if else statement: if (n == 0) { // statement 1 } else { for (int i = 0; i < n; i++) { // statement 2 } } If you notice, there's two possibilities that could occur: the if part, where[ statement 1 ]{style=\"background-color: light-gray\"} will run once, \\(\\color{ocre2}\\text{O}(1)\\) or the else part, where [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(\\color{ocre2}n\\) times, \\(\\color{ocre2}\\text{O}(n)\\) . In general, the time complexity of an if else statement is: \\( \\(\\color{ocre2}\\text{O}(if-else) = \\text{O}\\Big(\\text{max}\\Big[\\text{O}(\\text{condition1}), \\text{O}(\\text{condition2}), \\dots, \\text{O}(\\text{branch}1), \\text{O}(\\text{branch2}), \\dots\\Big]\\Big)\\) \\) As we are typically interested in the worst-cases, we only consider the branch with the largest running time. The condition runs once and then we add whichever is larger, which is the else part, thus, the time complexity is: \\( \\(\\color{ocre2}1 + n = \\text{O}(n)\\) \\) or equivalently \\( \\(\\color{ocre2}\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(1), \\text{O}(n)\\Big]\\Big) = \\text{O}(n)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Switch Statement The following example is a switch statement: switch (key) { case 'a': for (int i = 0; i < n; i++) { // statement 1 } case 'b': for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement 2 } } default: // statement 3 break; } Similar to the if else statement, we only consider the case with the largest running time, including the default case. In this example, for [ case \u2019b\u2019 ]{style=\"background-color: light-gray\"}, [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(n^2\\) times, \\(\\color{ocre2}\\text{O}(n^2)\\) . Thus, the time complexity is: \\( \\(\\color{ocre2}1 + n^2 = \\text{O}(n^2)\\) \\) or equivalently \\( \\(\\color{ocre2}\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(n), \\text{O}(n^2), \\text{O}(1)\\Big]\\Big) = \\text{O}(n^2)\\) \\) Recurrence Equations \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction In Lecture 2, we described the worst-case running time \\(\\color{ocre2}T(n)\\) of merge-sort procedure by the recurrence: \\( \\(\\color{ocre2}T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) whose solution we claimed to be \\(\\color{ocre2}T(n) = \\Theta(n\\log n)\\) . Previously, we didn't really have a general method for finding the form of recurrences. Our goal for this lecture is to go in-depth in ways we can analyze recursive algorithms and form a general formula. ::: dBox ::: definitionT Definition 4.1 (Recursive algorithm). An algorithm which calls itself to solve smaller problems. ::: ::: Recurrence can be polymorphic, meaning it can take many forms: A recursive algorithm which divides to two problem with equal sizes. \\( \\(\\color{ocre2}T(n) = 2T(n/2) + \\Theta(n)\\) \\) A recursive algorithm might divide subproblems into unequal sizes. \\( \\(\\color{ocre2}T(n) = T(2n/3) + T(n/3) + \\Theta(n)\\) \\) They are not necessarily constrained to being a constant fraction of the original problem size. \\( \\(\\color{ocre2}T(n) = T(n-1) + \\Theta(1)\\) \\) -4ex -1ex -.4ex 1ex .2ex Finding the Asymptotic Bounds There are three methods for solving recurrencesthat is, for obtaining asymptotic \" \\(\\color{ocre2}\\Theta\\) \" or \" \\(\\color{ocre2}\\text{O}\\) \" bounds on the solution: Substitution Method Recursion-Tree Method Master Method -3ex -0.1ex -.4ex 0.5ex .2ex Substitution Method This method is powerful, but we must be able to guess the form of the answer in order to apply it. It comprises of the following steps: Step 1: Try a few substitutions to find a pattern. Step 2: Guess the recurrence formula after \\(\\color{ocre2}k\\) iterations (in terms of \\(\\color{ocre2}k\\) and \\(\\color{ocre2}n\\) ). Step 3: Set \\(\\color{ocre2}k\\) so we get the base case. Step 4: Put \\(\\color{ocre2}k\\) back into the formula to find a potential closed form. Step 5: Prove the potential closed form using induction. Using the merge-sort algorithm as an example, which has the following recurrence: \\( \\(\\color{ocre2}T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) Let's go step-by-step, as described. The easiest way to find a pattern, is by simply writing out the first few iterations. Let's denote \\(\\color{ocre2}k\\) as our number of iterations starting from \\(\\color{ocre2}1\\) . \\( \\({\\color{ocre2}\\begin{split} k &= 1 & T(n) &= 2T(n/2) + n \\\\ k &= 2 & T(n) &= 2\\Big[2T(n/4) + n/2\\Big] + n = 4 \\cdot T(n/4) + 2n \\\\ k &= 3 & T(n) &= 2\\bigg[2\\Big[2T(n/8) + n/4\\Big] + n/2\\bigg] + n = 8 \\cdot T(n/8) + 3n \\\\ \\end{split}}\\) \\) Our goal is to generalize this for \\(\\color{ocre2}k\\) iterations. In other words, relating each of the constants to \\(\\color{ocre2}k\\) . We can rewrite it as such \\( \\({\\color{ocre2}\\begin{split} k &= 1 & T(n) &= 2^1 \\cdot T(n/2^1) + 1n \\\\ k &= 2 & T(n) &= 2^2 \\cdot T(n/2^2) + 2n \\\\ k &= 3 & T(n) &= 2^3 \\cdot T(n/2^3) + 3n \\\\ \\end{split}}\\) \\) Thus, we can form a general formula, using in terms of \\(\\color{ocre2}k\\) and \\(\\color{ocre2}n\\) \\( \\(\\color{ocre2}T(n) = 2^k \\cdot T(n/2^k) + kn\\) \\) We know the base case is set to \\(\\color{ocre2}T(1) = 1\\) . From our general formula, we can determine how many iterations there are in terms of \\(\\color{ocre2}n\\) to reach the base case, by solving for \\(\\color{ocre2}k\\) . \\( \\({\\color{ocre2}\\begin{split} \\frac{n}{2^k} = 1 \\ \\to\\ n &= 2^k \\\\ k &= \\log_2{n} \\end{split}}\\) \\) Substituting \\(\\color{ocre2}k = \\log_2{n}\\) back into the general formula, we get a potential closed form, as \\(\\color{ocre2}T()\\) is no longer inside our formula. \\( \\({\\color{ocre2}\\begin{split} T(n) &= 2^{\\log_2{n}} \\cdot T(n/2^{\\log_2{n}}) + n\\log_2{n} \\\\ &= n + n\\log_2{n} \\end{split}}\\) \\) We can guess that the solution is \\(\\color{ocre2}T(n) = \\text{O}(n\\log{n})\\) . However, we need a definite proof that this is true, by using mathematical induction for the following statement. \\( \\(\\color{ocre2}0 \\leq T(n) \\leq cn\\log{n} \\hspace{1cm} \\exists c > 0,\\ \\forall n \\geq n_0\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Mathematical Induction In order to prove something is true, we use mathematical induction. We must show that we can choose the constant \\(\\color{ocre2}c\\) large enough so that \\(\\color{ocre2}T(n) \\leq cn\\log{n}\\) is true. Remember, the base case is \\(\\color{ocre2}T(1) = 1\\) . Then for \\(\\color{ocre2}n = 1\\) , it yields \\(\\color{ocre2}T(1) \\nleq c(1)\\log{1} = 0\\) . Consequently, the base case fails to hold, so what now? For asymptotic notation we can specify a specific bound, \\(\\color{ocre2}\\forall n \\geq n_0\\) , where \\(\\color{ocre2}n_0\\) is something we can choose. Thus \\(\\color{ocre2}n_0 = 2\\) , removing it from consideration in the induction proof. The induction proof consists of three parts: the base case, inductive hypothesis and inductive step. Let's assume \\(\\color{ocre2}n\\) is some power of \\(\\color{ocre2}2\\) or \\(\\color{ocre2}n = 2^k\\) , for sake of convenience. Base Case: Let \\(\\color{ocre2}k = 1\\) or \\(\\color{ocre2}n = 2\\) then: \\( \\(\\color{ocre2}T(2) = 2T(1) + 2 = 2 + 2 = 4 \\leq c(2)\\log{2}\\) \\) We can see that the inequality holds true for the base case, such that there \\(\\color{ocre2}c \\geq 2\\) . Inductive hypothesis: We will now assume that our proposition, \\(\\color{ocre2}T(n) = \\text{O}(n\\log{n})\\) , holds true for \\(\\color{ocre2}k -1\\) , which equivalently is \\(\\color{ocre2}n/2\\) , therefore: \\( \\(\\color{ocre2}T(n/2) \\leq c(n/2)\\log{(n/2)}\\) \\) To prove the inductive step, one assumes the induction hypothesis for \\(\\color{ocre2}k-1\\) and then uses this assumption to prove that the statement holds for \\(\\color{ocre2}k\\) . If instead, we assume our hypothesis to hold for \\(\\color{ocre2}k\\) , then we must prove it holds for \\(\\color{ocre2}k+1\\) . Inductive step: From our hypothesis, prove the guess of correct for \\(\\color{ocre2}k\\) . Using the following: \\( \\(\\color{ocre2}T(n) = 2T(n/2) + n\\) \\) Since we know \\(\\color{ocre2}T(n/2) \\leq c(n/2)\\log{(n/2)}\\) , then we can rewrite it as such: \\( \\({\\color{ocre2}\\begin{split} T(n) &\\leq 2\\Big[c(n/2)\\log{(n/2)}\\Big] + n \\\\ &\\leq cn\\log{(n/2)} + n = cn\\log{n} - cn\\log{2} + n \\\\ &\\leq cn\\log{n} + (1 - c)n \\\\ &\\leq cn\\log{n}\\qquad (\\forall c \\geq 1) \\end{split}}\\) \\) From the inductive step, we proved that proposition is true as we found that there exists some value of \\(\\color{ocre2}c\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Recursion-Tree Method In a recursion tree, we sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to determine the total cost of all levels of the recursion. Step 1: Start by substituting the parent with non-recursive part of the formula and adding child nodes for each recursive part. Step 2: Expand each node repeating the step above, until you reach the base case. We already covered how to do this using merge-sort algorithm, so let's start off simple, by using the following recurrence: \\( \\(\\color{ocre2}T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ T(n-1) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) As usual, let's go step-by-step. The non-recursive part, \\(\\color{ocre2}n\\) , will be the parent node and the recursive part, \\(\\color{ocre2}T(n-1)\\) , will be the child node. The costs within each level is displayed in the right-hand side. ::: center ::: Expand on \\(\\color{ocre2}T(n-1)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(\\color{ocre2}T(1) = 1\\) . The fully expanded tree has height \\(\\color{ocre2}n\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the arithmetic series, thus: \\( \\(\\color{ocre2}T(n) = 1 + \\cdots + (n-2) + (n-1) + n = \\frac{n(n+2)}{2} = \\Theta(n^2)\\) \\) Suppose you consider something a bit more complex, which divides the subproblems into unequal sizes, for the following recurrence: \\( \\(\\color{ocre2}T(n) = T(n/4) + T(n/2) + n^2\\) \\) The non-recursive part, \\(\\color{ocre2}n^2\\) , will be the parent node and the recursive part, \\(\\color{ocre2}T(n/4)\\) and \\(\\color{ocre2}T(n/2)\\) , will be the child nodes. ::: center ::: Expand on \\(\\color{ocre2}T(n/4)\\) and \\(\\color{ocre2}T(n/2)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(\\color{ocre2}T(1)\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the geometric series, thus: \\( \\(\\color{ocre2}T(n) = n^2\\bigg[1 + \\Big[\\frac{5}{16}\\Big] + \\Big[\\frac{5}{16}\\Big]^2 + \\cdots\\bigg] = \\Theta(n^2)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Master Method The master method provides a \"cookbook\" method for solving recurrences of the form: \\( \\(\\color{ocre2}T(n) = aT(n/b) + f(n)\\) \\) where \\(\\color{ocre2}a \\geq 1\\) , \\(\\color{ocre2}b > 1\\) and \\(\\color{ocre2}f(n)\\) be a function of \\( \\(\\color{ocre2}f(n) = n^k\\log^p{n}\\) \\) Note that there are various variations of the master theorem, but this is definition is what I found the easiest to understand. It consists of memorizing these three cases: Case One: If \\(\\color{ocre2}\\log_b{a} > k\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^{\\log_b{a}})\\) . Case Two: If \\(\\color{ocre2}\\log_b{a} = k\\) and ... (a) \\(\\color{ocre2}p > -1\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k\\log^{p+1}{n})\\) . (b) \\(\\color{ocre2}p = -1\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k\\log({\\log{n}}))\\) . (c) \\(\\color{ocre2}p < -1\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k)\\) . Case Three: If \\(\\color{ocre2}\\log_ba < k\\) and ... (a) \\(\\color{ocre2}p \\geq 0\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k\\log^{p}{n})\\) . (b) \\(\\color{ocre2}p < 0\\) , then \\(\\color{ocre2}T(n) =\\Theta(n^k)\\) . It looks a bit complicated at first glance, but once we get to the examples, it becomes quite easy. ::: exampleT Example 4.1 . Suppose we have the following recurrence: ( \\(\\color{ocre2}T(n) = 2T(n/2) + 1\\) \\) We know \\(\\color{ocre2}a = 2\\) and \\(\\color{ocre2}b = 2\\) , but how do we get \\(\\color{ocre2}k\\) and \\(\\color{ocre2}p\\) ? We can rewrite it in the form of \\(\\color{ocre2}n^k\\log^p{n}\\) : ( \\(\\color{ocre2}f(n) = 1 = n^0\\log^0{n}\\) \\) You can confirm that both equations are identical, thus \\(\\color{ocre2}k = 0\\) and \\(\\color{ocre2}p = 0\\) . Since \\(\\color{ocre2}\\log_2{2} > k\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^{\\log_b{a}})\\) . Substituting in the values for \\(\\color{ocre2}a\\) and \\(\\color{ocre2}b\\) , we get: ( \\(\\color{ocre2}T(n) = \\Theta(n^{\\log_2{2}}) = \\Theta(n)\\) \\) ::: You can refer to this [video]{.underline} for more examples covering the three cases. Elementary Data Structures \u00b6 -4ex -1ex -.4ex 1ex .2ex Stacks Stacks are dynamic sets in which the element removed from the set by the delete operation is prespecified. What defines a stack is that it implements a last-in, first-out (LIFO) principle, so only the top element is accessible. ::: center ::: There are three main methods on a stack: [ push(S,x) ]{style=\"background-color: light-gray\"} - Inserts an object \\(\\colorbox{light-gray}{\\texttt{x}}\\) onto top of Stack [ S ]{style=\"background-color: light-gray\"}. [ pop(S) ]{style=\"background-color: light-gray\"} - Removes the top object of stack [ S ]{style=\"background-color: light-gray\"}; if the stack is empty, an error occurs. [ top(S) ]{style=\"background-color: light-gray\"} - Returns the top object of the stack [ S ]{style=\"background-color: light-gray\"}, without removing it; if the stack is empty, an error occurs. ::: center ::: The following support methods should also be defined: [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in stack [ S ]{style=\"background-color: light-gray\"}. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if stack [ S ]{style=\"background-color: light-gray\"} is empty. We can implement each of the stack operations with just a few lines of code: ::: algorithm [Stack-Empty]{.smallcaps} \\((S)\\) ::: algorithmic \\(\\textsc{True}\\) \\(\\textsc{False}\\) ::: ::: ::: algorithm [Push]{.smallcaps} \\((S,x)\\) ::: algorithmic \\(S\\,.\\,top = S\\,.\\,top + 1\\) \\(S[S\\,.\\,top ] = x\\) ::: ::: ::: algorithm [Pop]{.smallcaps} \\((S)\\) ::: algorithmic \\\"underflow\\\" \\(S\\,.\\,top = S\\,.\\,top - 1\\) \\(S[S\\,.\\,top + 1]\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a stack [ S ]{style=\"background-color: light-gray\"} with \\(\\color{ocre2}7\\) elements. Let \\(\\color{ocre2}S\\,.\\,top\\) be a pointer to keep track of the last element (or top). When \\(\\color{ocre2}S\\,.\\,top = 0\\) , there is no elements and is empty, so stack [ S ]{style=\"background-color: light-gray\"} has \\(\\color{ocre2}0\\) elements. ::: center ::: When we call [ push(S,15) ]{style=\"background-color: light-gray\"}, \\(\\color{ocre2}S\\,.\\,top\\) moves up by \\(\\color{ocre2}1\\) and inserts element \\(\\color{ocre2}15\\) to the stack. ::: center ::: Suppose we call the following: [ push(S,6) ]{style=\"background-color: light-gray\"}, [ push(S,2) ]{style=\"background-color: light-gray\"} and [ push(S,3) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ pop(S) ]{style=\"background-color: light-gray\"}, \\(\\color{ocre2}S\\,.\\,top\\) moves down by \\(\\color{ocre2}1\\) and returns the element that was removed, which is element \\(\\color{ocre2}3\\) . ::: center ::: Although element \\(\\color{ocre2}3\\) still appears in the array, it is no longer in the stack. When we call [ push(S,9) ]{style=\"background-color: light-gray\"}, \\(\\color{ocre2}S\\,.\\,top\\) moves up by \\(\\color{ocre2}1\\) and overwrites element \\(\\color{ocre2}3\\) with \\(\\color{ocre2}9\\) . ::: center ::: If you notice, when pushing an element or popping an element off the stack, it takes a constant amount of time. Let \\(\\color{ocre2}n\\) be the numbers of elements in the stack. Each operation runs in time \\(\\color{ocre2}\\text{O}(1)\\) . The space used is \\(\\color{ocre2}\\text{O}(n)\\) . There are a few limitations we must consider: The maximum size of the stack must be defined priority and cannot be changed. When pushing a new element into a full stack, it causes an implementation error. -4ex -1ex -.4ex 1ex .2ex Queue Queue are another type of dynamic sets, which implements first-in, first-out (FIFO) principle, so queue items are removed in exactly the same order as they were added to the queue. ::: center ::: There are exist the following operations on a queue: [ enqueue(Q,x) ]{style=\"background-color: light-gray\"} - Inserts an element \\(\\colorbox{light-gray}{\\texttt{x}}\\) at the rear of the queue [ Q ]{style=\"background-color: light-gray\"}. [ dequeue(Q) ]{style=\"background-color: light-gray\"} - Removes the element at the front of queue [ Q ]{style=\"background-color: light-gray\"}. [ front() ]{style=\"background-color: light-gray\"} - Returns the front element of the queue without removing it. [ new() ]{style=\"background-color: light-gray\"} - Creates an empty queue. [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in queue. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if queue is empty. ::: center ::: Assume \\(\\color{ocre2}n = Q\\,.\\,length\\) . The pseudocode for enqueue and dequeue is shown below: ::: algorithm [Enqueue]{.smallcaps} \\((Q,x)\\) ::: algorithmic \\(Q[Q\\,.\\,tail] = x\\) \\(Q\\,.\\,tail = 1\\) ::: ::: ::: algorithm [Dequeue]{.smallcaps} \\((Q)\\) ::: algorithmic \\(x = Q[Q\\,.\\,head]\\) \\(Q\\,.\\,head = 1\\) ::: ::: Note that we didn't account for the error when underflow and overflow occurs. -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a queue [ Q ]{style=\"background-color: light-gray\"} with \\(\\color{ocre2}7\\) elements. Let \\(\\color{ocre2}Q\\,.\\,head\\) be a pointer for the front of the queue and \\(\\color{ocre2}Q\\,.\\,tail\\) be the back of the queue. When \\(\\color{ocre2}Q\\,.\\,head = \\color{ocre2}Q\\,.\\,tail\\) , there is no elements, so queue [ Q ]{style=\"background-color: light-gray\"} has \\(\\color{ocre2}0\\) elements. ::: center ::: When we call [ enqueue(Q,15) ]{style=\"background-color: light-gray\"}, element \\(\\color{ocre2}15\\) is added to the queue then \\(\\color{ocre2}Q\\,.\\,tail\\) moves up by \\(\\color{ocre2}1\\) . ::: center ::: Suppose we call the following: [ enqueue(Q,6) ]{style=\"background-color: light-gray\"}, [ enqueue(Q,2) ]{style=\"background-color: light-gray\"} and [ enqueue(Q,9) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ dequeue(Q) ]{style=\"background-color: light-gray\"}, element \\(\\color{ocre2}15\\) located at the front of queue indicated by \\(\\color{ocre2}Q\\,.\\,head\\) , is removed then \\(\\color{ocre2}Q\\,.\\,head\\) moves up by \\(\\color{ocre2}1\\) to element \\(\\color{ocre2}6\\) . ::: center ::: As the final scenario, suppose we filled the array from \\(\\color{ocre2}Q[2 .. 7]\\) , as shown below. ::: center ::: When we call \\(\\colorbox{light-gray}{\\texttt{enqueue(Q,x)}}\\) or add one more element, \\(\\color{ocre2}Q\\,.\\,tail\\) will have to move up by one where \\(\\color{ocre2}Q\\,.\\,head = \\color{ocre2}Q\\,.\\,tail\\) . ::: center ::: But, if you recall, this means the queue is empty, which is not the case and so the queue overflows. Similar to a stack, when enqueueing or dequeueing an element, it takes a constant amount of time. Let \\(\\color{ocre2}n\\) be the numbers of elements in the queue. Each operation runs in time \\(\\color{ocre2}\\text{O}(1)\\) . There are also a few limitations we must consider which carries over for queue: The maximum size of the stack must be defined priority and cannot be changed. If we attempt to dequeue an element from an empty queue, the queue underflows. If we attempt to enqueue an element from a full queue, the queue overflows and so we can only store \\(\\color{ocre2}n - 1\\) elements. -4ex -1ex -.4ex 1ex .2ex Linked Lists A collection of nodes that together form a linear ordering. Unlike an array, however, in which the linear order is determined by the array indices, the order in a linked list is determined by a pointer in each object. It consists of: A sequence of nodes Each node contains a value and link reference to some other node The last node contains a null link -3ex -0.1ex -.4ex 0.5ex .2ex Singly Linked Lists The most basic of all linked data structures, which are used to implement stacks and queues. Each node has data and a pointer to the next node. ::: center ::: Searching a singly linked list. ::: algorithm [List-Search]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,head\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: To search a list of \\(\\color{ocre2}n\\) elements, the [List-Search]{.smallcaps} procedure takes \\(\\color{ocre2}\\Theta(n)\\) time in the worst-case, since it may have to search the entire listsimilar to insertion sort. Inserting into a singly linked list. The [List-Insert]{.smallcaps} procedure splices the inserted element, [ x ]{style=\"background-color: light-gray\"}, onto the front of the linked list. ::: center ::: The running time for [List-Insert]{.smallcaps} on a list of \\(\\color{ocre2}n\\) elements is \\(\\color{ocre2}\\text{O}(1)\\) . Deleting from a singly linked list. The [List-Delete]{.smallcaps} procedure removes an element, [ x ]{style=\"background-color: light-gray\"}, from a linked list by getting a pointer to \\(\\colorbox{light-gray}{\\texttt{x}}\\) and it splices [ x ]{style=\"background-color: light-gray\"} out of the list by updating pointers. ::: center ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\color{ocre2}\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\color{ocre2}\\text{O}(n)\\) time is required in the worst case. Some applications of singly linked lists are: Implement stacks and queues, as shown below. Dynamic memory allocation, which will cover in the very end. -3ex -0.1ex -.4ex 0.5ex .2ex Doubly Linked Lists We add a pointer to the previous node. Thus, we can go in either direction: forward or backward. ::: center ::: Searching a doubly linked list. A singly and linked list uses the same algorithm for searching. Thus, both take \\(\\color{ocre2}\\Theta(n)\\) times in the worst-case to search through a list of \\(\\color{ocre2}n\\) elements. <!-- --> Inserting into a doubly linked list. The [List-Insert]{.smallcaps} procedure is also similar to the singly, but now we also have to account for the previous pointer. ::: center ::: ::: algorithm [List-Insert]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,head\\) \\(L\\,.\\,head\\,.\\,prev = x\\) \\(L\\,.\\,head = x\\) \\(x\\,.\\,prev =\\) [nil]{.smallcaps} ::: ::: The running time for [List-Insert]{.smallcaps} on a list of \\(\\color{ocre2}n\\) elements is \\(\\color{ocre2}\\text{O}(1)\\) . Deleting from a doubly linked list. Likewise, same thing can be said for the [List-Delete]{.smallcaps} procedure, in which we now have to also assign the previous pointer ::: center ::: ::: algorithm [List-Delete]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(L\\,.\\,head = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\color{ocre2}\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\color{ocre2}\\text{O}(n)\\) time is required in the worst case. Some applications of doubly linked lists are: Browsers to implement backward and forward navigation of visited web pagesthe back and forward button. Various application to implement Undo and Redo functionality. -3ex -0.1ex -.4ex 0.5ex .2ex Circularly Linked Lists A circularly singly linked list is a variation of a linked list in which the last element is linked to the first element. This forms a circular loop. ::: center ::: A circularly doubly linked list, in which in addition to the one above, the first element is linked to the last element. ::: center ::: In a circularly linked list, we used a sentinelrepresented by the dark grey node [ L.nil ]{style=\"background-color: light-gray\"}. ::: center ::: It represents [nil]{.smallcaps} which lies between the head and tail. It functions like any other object in a doubly linked list, which it has a pointer from the previous and next node. Below are the procedures used for circularly doubly linked list with sentinel. ::: algorithm [List-Search/]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,nil\\,.\\,next\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: ::: algorithm [List-Insert']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,nil\\,.\\,next\\) \\(L\\,.\\,nil\\,.\\,next\\,.\\,prev = x\\) \\(L\\,.\\,nil\\,.\\,next = x\\) \\(x\\,.\\,prev = L\\,.\\,nil\\) ::: ::: ::: algorithm [List-Delete']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: Some applications of circularly linked lists are: Useful for implementation of queue. Circular lists are useful in applications to repeatedly go around the list. Circular doubly linked lists are used for implementation of advanced data structures like Fibonacci Heap. -3ex -0.1ex -.4ex 0.5ex .2ex Implementing Pointers and Objects We can implement pointers and objects in languages that do not provide them by synthesizing them from arrays and array indices. For this example, let's use the following doubly linked list: ::: center ::: Single-array representation of objects. Analogous to storing an object in the memory. ::: center ::: Each object is represented by a contiguous sub-array of length \\(\\color{ocre2}3\\) . The three attributes [ key ]{style=\"background-color: light-gray\"}, [ next ]{style=\"background-color: light-gray\"}, and [ prev ]{style=\"background-color: light-gray\"} correspond to the offsets: \\(\\color{ocre2}0\\) , \\(\\color{ocre2}1\\) , and \\(\\color{ocre2}2\\) of the sub-array. Multiple-array representation of objects. We can represent a collection of objects that have the same attributes by using an array for each attribute. ::: center ::: You can think of each column (or vertical slice) as a single object. The pointers resides in the [ next ]{style=\"background-color: light-gray\"} and [ prev ]{style=\"background-color: light-gray\"} array, which point to the index where the next object resides. Allocating and freeing objects. To insert a key into a dynamic set represented by a doubly linked list, we must allocate a pointer to a currently unused object in the linked-list representation. ::: center ::: We keep the free objects in a singly linked list (only [ next ]{style=\"background-color: light-gray\"} pointer), which we call the free list. The free list acts like a stackthe next object allocated is the last one freed. -4ex -1ex -.4ex 1ex .2ex Heaps The (binary) heap data structure is an array of object that we can view as a nearly complete binary tree. Each node of the tree corresponds to an element of the array. ::: center ::: There are two kinds of binary heap. In both kinds, the values in the nodes satisfy a heap property, the specifics of which depend on the kind of heap. Max-heap. The max-heap property is that for every node \\(\\color{ocre2}i\\) other than the root: \\( \\(\\color{ocre2}A[\\textsc{Parent}(i)] \\geq A[i]\\) \\) which means that a child node can't have a greater value than its parent. Min-heap. The min-heap property is the opposite, which for every node \\(\\color{ocre2}i\\) other than the root: \\( \\(\\color{ocre2}A[\\textsc{Parent}(i)] \\leq A[i]\\) \\) which means that a parent node can't have a greater value than its child nodes. If all the nodes satisfy the heap property, then a binary tree is a heap. However, if a node does not have the heap property, the node is swapped with the parent. This operation is called sifting up. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Constructing a Heap A heap can be stored as an array \\(\\color{ocre2}A\\) , where the: Root of tree is \\(\\color{ocre2}A[1]\\) . The cell at index \\(\\color{ocre2}0\\) is not used, thus we start at index \\(\\color{ocre2}1\\) . Parent of \\(\\color{ocre2}A[i]\\) is \\(\\color{ocre2}A[\\lfloor i/2 \\rfloor]\\) . Left child of \\(\\color{ocre2}A[i]\\) is \\(\\color{ocre2}A[2i]\\) . Right child of \\(\\color{ocre2}A[i]\\) is \\(\\color{ocre2}A[2i + 1]\\) . To construct a heap: Start with a single node. Add a node to the right of the rightmost node in the deepest level. If the deepest level is full, start a new level. Each time we add a node, we may destroy heap property of its parent node. To fix this, sift up until either: We reach nodes whose values don't need to be swappedthe parent node is larger than both children. We reach the root. Suppose we have an array \\(\\color{ocre2}A = [8, 10, 5, 12, 14]\\) , we would construct the heap as such: ::: center ::: Our final heap should look like this: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Maintaining Heap Property To implement this: Represent an arbitrary array as a binary tree. Devise a [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm that maintains the heap property of any given node \\(\\color{ocre2}i\\) in the heap with sub-trees \\(\\color{ocre2}l\\) and \\(\\color{ocre2}r\\) rooted at \\(\\color{ocre2}i\\) th children, given to be heaps. Devise a [ Build-Max-Heap() ]{style=\"background-color: light-gray\"} algorithm that uses [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm to construct a heap. ::: algorithm [Max-Heapify]{.smallcaps} \\((A,n)\\) ::: algorithmic ::: ::: ::: algorithm [Build-Max-Heap]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The worst-case time complexity of: [Max-Heapify]{.smallcaps} is \\(\\color{ocre2}\\text{O}(\\log{n})\\) [Build-Max-Heap]{.smallcaps} is \\(\\color{ocre2}\\text{O}(n)\\) The heapsort algorithm is based on the heap data structure, which uses these two main parts: building a max-heap and sorting it, to sort ::: algorithm [Heapsort]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: Thus, heapsort has a worst-case time complexity of \\(\\color{ocre2}\\text{O}(n\\log{n})\\) like merge sort, but heapsort has a space complexity of \\(\\color{ocre2}\\text{O}(1)\\) , since it sorts in-place, taking a constant amount of memory. -3ex -0.1ex -.4ex 0.5ex .2ex Priority Queue One of the most popular implementations of a heap, a priority queue is a data structure for maintaining a set \\(\\color{ocre2}S\\) of elements, each with an associated value called a key. As with heaps, there are two kinds of priority queues: max-priority queue and min-priority queue. ::: center ::: We will focus here on how to implement max-priority queues, which are in turn based on max-heaps. A max-priority queue supports dynamic-set operations: [ Insert(S,x) ]{style=\"background-color: light-gray\"} - Inserts element [ x ]{style=\"background-color: light-gray\"} into set [ S ]{style=\"background-color: light-gray\"}. [ Maximum(S) ]{style=\"background-color: light-gray\"} - Returns an element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Extract-Max(S) ]{style=\"background-color: light-gray\"} - Removes and returns element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Increase-Key(S,x,k) ]{style=\"background-color: light-gray\"} - Increases value of element [ x ]{style=\"background-color: light-gray\"}'s key to [ k ]{style=\"background-color: light-gray\"}. Assume [ k \\geq x ]{style=\"background-color: light-gray\"}'s current key value. The procedure [Heap-Maximum]{.smallcaps} has a running time of \\(\\color{ocre2}\\Theta(1)\\) . ::: algorithm [Heap-Maximum]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Extract-Max]{.smallcaps} has a running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Extract-Max]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Increase-Key]{.smallcaps} has a running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Increase-Key]{.smallcaps} \\((A,i,key)\\) ::: algorithmic ::: ::: The procedure [Max-Heap-Insert]{.smallcaps} has a running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) . ::: algorithm [Max-Heap-Insert]{.smallcaps} \\((A,key)\\) ::: algorithmic ::: ::: In summary, a heap can support any priority-queue operation on a set of size \\(\\color{ocre2}n\\) in \\(\\color{ocre2}\\text{O}(\\log{n})\\) time. Hash Tables \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction Many applications require a dynamic set that supports only the dictionary operations. ::: dBox ::: definitionT Definition 6.1 (Dictionary). A data structure that stores (key, value) pairs and supports the operations [Insert]{.smallcaps}, [Search]{.smallcaps}, and [Delete]{.smallcaps}. ::: ::: So far we have seen a couple ways to implement dictionaries, such as linked lists. Now we will learn how to use a hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex How It Works? A hash tables takes a key (typically a string of characters or numbers) and passes it through a hash function to convert it into an index of the array to store the associated value. ::: center ::: Suppose you need to find a value of the key, you do not need to iterate through all items in the collection, because you can just use the hash function to easily find the index. Using a hash table offers a very fast lookup for a value based on the key, which should be the \\(\\color{ocre2}\\text{O}(1)\\) operation. It is a generalization of an ordinary array. -3ex -0.1ex -.4ex 0.5ex .2ex Sample Problem As an example, you can think of a phone book. In the phone book, a person's name can be considered as a key, by which we can find a phone number. Case One: The simple and straightforward way to lookup number is to check all names in the phone book until we find a matching name. The worst-case search time is \\(\\color{ocre2}\\text{O}(n)\\) . Case Two: Use a hash function that helps us to lookup entries much faster. Suppose we have a person's name \\\"James Davis\\\" with the phone number \\\"416-999-1234\\\". A hash function takes the key and maps it to an integer that is within the size of the array: \\( \\(\\color{ocre2}\\text{String}\\ \\Rightarrow\\ \\boxed{\\text{Hash Function}}\\ \\Rightarrow\\ \\text{Index}\\) \\) Then it stores the value of the phone number to an index of the array. If we continue to add more people, it would map each one to an index of the array. ::: center ::: If we wanna lookup a person's phone number, all we need is the person's name and we can easily find the index it is stored in the array, by passing it through a hash function. Obviously, this is a watered-down explanation and doesn't go in-depthlike the possibility when two or more keys hash to the same slot. But before moving further, let's understand how direct-address table works to see the benefits of using hash tables instead. -3ex -0.1ex -.4ex 0.5ex .2ex Direct Address Table With an ordinary array, we store element whose key is \\(\\color{ocre2}k\\) in position \\(\\color{ocre2}k\\) of the array. ::: center ::: ::: dBox ::: definitionT Definition 6.2 (Direct addressing). Given a key \\(\\color{ocre2}k\\) , we find the element whose key is \\(\\color{ocre2}k\\) by just looking in the \\(\\color{ocre2}k\\) th position of the array. ::: ::: A direct-address table (DAT) uses the keys as indices of the array and stores the values at those bucket locations. ::: center ::: It does facilitate fast searching, fast inserting and fast deletion operations: Inserting or deleting an element in the table, is the same as you would do for an array, hence we can do that in \\(\\color{ocre2}\\text{O}(1)\\) time as we already know the index (via key). Searching an element takes \\(\\color{ocre2}\\text{O}(1)\\) times, as we can easily access an element in an array in linear time if we already know the index of that element. Direct addressing is applicable when we can afford to allocate an array with one position for every possible key, and so it comes at a cost: It cannot handle collisionstwo keys are equal and contain different values. It is not recommended using the direct address table if the key values are very large. It has serious disadvantages, making it not suitable for the practical usage of current world scenarios, which is why we make use of hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Table As a recap, from the introduction, instead of storing an element with key \\(\\color{ocre2}k\\) in index \\(\\color{ocre2}k\\) , we use a hash function \\(\\color{ocre2}h\\) and store the element in index \\(\\color{ocre2}h(k)\\) . ::: center ::: ::: dBox ::: definitionT Definition 6.3 (Hash function). A hash function \\(\\color{ocre2}h\\) maps all possible keys to the slots of an array \\(\\color{ocre2}T[0 \\dots n - 1]\\) . ::: ::: While hash table offer the same time complexity of \\(\\color{ocre2}\\text{O}(1)\\) when we talk about insertion, deletion, or searching an element, the main focus is in its ability to maintains the size constraint. The problem with DAT is if the universe \\(\\color{ocre2}U\\) of keys is large, storing a table of size of \\(\\color{ocre2}|U|\\) may be impractical or impossible. Often, the set of keys \\(\\color{ocre2}K\\) actually stored is small, compared to \\(\\color{ocre2}U\\) . ::: problem Problem 6.1 . Suppose we have a key of \\(\\color{ocre2}7898\\) , which in turn is a large number. ::: Case One: Using a DAT table, we would need a huge array, for the key in index \\(\\color{ocre2}7898\\) to store the value at \\(\\color{ocre2}T[7898]\\) . In turn, we are wasting too much space, as most of the allocated space for the array is wasted. Case Two: But, in the case of a hash table, we can process this key via a hash function. The hash function \\(\\color{ocre2}h(7898)\\) maps it to an index within the hash table \\(\\color{ocre2}T[0 \\dots n - 1]\\) . Regarding the size of the hash table \\(\\color{ocre2}n\\) it typically varies, as it depends in part on choice of the hash function and collision resolution, where a situation might arise when two or more keys hash to the same slot. -4ex -1ex -.4ex 1ex .2ex Hash Function A good hash function should minimizes collision as mush as possible. It is usually specified as the composition of two functions \\(\\color{ocre2}h(k) = h_2\\big(h_1(k)\\big)\\) : Hash code. \\(\\color{ocre2}h_1: \\text{keys}\\ \\to\\ \\text{integers}\\) Compression function. \\(\\color{ocre2}h_2: \\text{integers}\\ \\to\\ [0 \\dots n - 1]\\) ::: center ::: The goal of the hash function is to disperse the keys in an apparently random way. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Code As mentioned previously, keys can be a string of characters. Thus, if the keys are not natural numbers, we find a way to interpret them as natural numbers. Some popular hash code maps are: Summing hash code. By adding up the [ASCII values]{.underline} of each letters in a string, we get an integer in return. For example, if the key is \\\"stop\\\": \\( \\(\\color{ocre2}h_1(\"stop\") = 115 + 116 + 111 + 112\\) \\) However, this is not suitable for strings cause two different strings can have the same set of letters, but have different meaning\\\"post\\\", \\\"tops\\\", and \\\"pots\\\" will have the same hash code. Polynomial hash code. A better hash code takes into account the position of each character. Using the example from before: \\( \\(\\color{ocre2}h_1(\"stop\") = (115 \\times a^0) + (116 \\times a^1) + (111 \\times a^2) + (112 \\times a^3)\\) \\) where \\(\\color{ocre2}a\\) is a non-zero constantcompared to \\\"post\\\", \\\"tops\\\", and \\\"pots\\\", all have unique hash codes, which is ideal. -3ex -0.1ex -.4ex 0.5ex .2ex Compression Function The hash code typically returns a large range of integers and so the compression functions maps it in the range \\(\\color{ocre2}[0 \\dots n - 1]\\) , the indices of the hash table. There's two methods: Division Method. A simple-modulo based compression rule: \\( \\(\\color{ocre2}h_2(k) = k\\ \\text{mod}\\ n\\) \\) The size \\(\\color{ocre2}n\\) of the hash table is usually chosen to be a prime number, to help spread out the distribution of hash values. MAD Method. The Multiply-Add-Divide method still use \\(\\color{ocre2}\\text{mod}\\ n\\) to get the numbers in the range, but a little fancier by spreading the numbers out first: \\( \\(\\color{ocre2}h_2(k) = [(ak + b)\\ \\text{mod}\\ p]\\ \\text{mod}\\ n\\) \\) The values \\(\\color{ocre2}a\\) and \\(\\color{ocre2}b\\) are chosen at random as positive integers and \\(\\color{ocre2}p\\) is a prime number, where \\(\\color{ocre2}p > n\\) . With the addition of \\(\\color{ocre2}(ak + b)\\ \\text{mod}\\ p\\) , it eliminates patterns provided by \\(\\color{ocre2}k\\ \\text{mod}\\ n\\) . Both incorporate the modulo operator, as it guarantees the output to be within the size of the hash table. Suppose we have a key of \\(\\color{ocre2}7898\\) from the previous example and a hash table with \\(\\color{ocre2}23\\) slots: \\( \\(\\color{ocre2}h_2(7898) = 7898\\ \\text{mod}\\ 23 = 9\\) \\) Then the key will be mapped to index \\(\\color{ocre2}9\\) of the hash table. -4ex -1ex -.4ex 1ex .2ex Collision Handling Collision occurs when different elements are mapped to the same index of the arraywhen \\(\\color{ocre2}h(k_1) = h(k_2)\\) , but \\(\\color{ocre2}k_1 \\neq k_2\\) . ::: center ::: Avoiding collision is ideal, nonetheless, it is impossible, so we use closed or open addressing to overcome this problem. Each of them have their pros and cons. -3ex -0.1ex -.4ex 0.5ex .2ex Closed Addressing Closed addressing (or open hashing) is also known as separate chaining. When collision occurs, the index keeps a reference to a linked list or dynamic array that stores all items with the same index. Let \\(\\color{ocre2}e_1\\) and \\(\\color{ocre2}e_2\\) represent the values attached to \\(\\color{ocre2}k_1\\) and \\(\\color{ocre2}k_2\\) respectively. ::: center ::: Separate chaining is fairly simple to implement and faster than open addressing in general. However, it is memory inefficient as it requires a secondary data structure and longs chains can result in \\(\\color{ocre2}\\text{O}(n)\\) times. -3ex -0.1ex -.4ex 0.5ex .2ex Open Addressing Instead of referencing to a list or an array, open addressing (or closed hashing) resolves collision by searching for another empty bucket. ::: center ::: There's three types of open addressing: Linear Probing. When collision occurs, we linearly probe for the next bucket by increasing the index linearly until it finds an empty bucket: \\( \\(\\color{ocre2}\\text{Index} = \\big[h(k) + i\\big]\\ \\text{mod}\\ n\\) \\) where \\(\\color{ocre2}i\\) increases by one each iteration, until it finds an empty bucket. Quadratic Probing. Similar to the previous one, but instead we increase the index quadratically until it finds an empty bucket: \\( \\(\\color{ocre2}\\text{Index} = \\big[h(k) + i^2\\big]\\ \\text{mod}\\ n\\) \\) where \\(\\color{ocre2}i\\) increases by one each iteration, until it finds an empty bucket. Double Hashing. Using a secondary hash function \\(\\color{ocre2}h'(k)\\) , it places the colliding item in the first available cell by: \\( \\(\\color{ocre2}\\text{Index} = \\big[h(k) + jh'(k)\\big]\\ \\text{mod}\\ n\\) \\) where \\(\\color{ocre2}j\\) increases by one each iteration, until it finds an empty bucket. The secondary hash function cannot have zero values and is typically written as such: \\( \\(\\color{ocre2}h'(k) = q -(k\\ \\text{mod}\\ q)\\) \\) where \\(\\color{ocre2}q\\) is a prime number, such that \\(\\color{ocre2}q > n\\) . Unlike separate chaining, open addressing is more memory efficient, as it stores element in empty indices. However, it can create cluster: Linear probing can result in primary clustering. Quadratic probing can result in secondary clustering. Compared to the two, double hashing distributes the keys more evenly and produces a uniform distribution of records throughout the hash table. Trees \u00b6 -4ex -1ex -.4ex 1ex .2ex Introduction A tree is a dynamic set of nodes storing elements in a parent-child relationship (edge) with the following properties: It has a special node called root. Each node different from the root has a parent node. There is a single unique path along the edges from the root to any particular nodedoesn't have any cycles. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Tree Terminology In a tree, we often refer to certain parts of tree, which are listed below. For reference: ::: center ::: Root: The top element with no parent ( \\(\\color{ocre2}A\\) ). Siblings: Children of the same parent ( \\(\\color{ocre2}G, H\\) both have the parent \\(\\color{ocre2}C\\) ). External node: Also referred to as leave, ndoes with no children ( \\(\\color{ocre2}E, I, J, K, G, H\\) ). Internal node: nodes with one or more children ( \\(\\color{ocre2}A, B, C, F\\) ). Ancestors: A node that is connected to all lower-level node ( \\(\\color{ocre2}A, B, F\\) are ancestors of \\(\\color{ocre2}I, J, K\\) ). Descendants: The connected lower-level nodes ( \\(\\color{ocre2}I\\) is a descendant of \\(\\color{ocre2}A, B, F\\) ). Depth of a node: Number of ancestors ( \\(\\color{ocre2}I\\) has a depth of \\(\\color{ocre2}3\\) ). Height of a tree: The max node depth (The height of tree is \\(\\color{ocre2}3\\) ). Sub-tree: A tree consisting of a node and all its descendants (Refer to the red triangle above). -3ex -0.1ex -.4ex 0.5ex .2ex Tree Traversals A traversal is defined as a systematic way of accessing or visiting all nodes of a tree. Let's use the following tree as an example: ::: center ::: There's three ways a tree can be traverse, but we'll only go over two of them. The last one will be covered in the next section. Preorder traversal. Root is visited first and then sub-trees rooted at its children are visited recursively ( \\(\\color{ocre2}A \\to B \\to D \\to E \\to C \\to F \\to G\\) ). Postorder traversal. Recursively traverse the sub-trees rooted at children and then visit the root itself ( \\(\\color{ocre2}D \\to E \\to F \\to G \\to B \\to C \\to A\\) ). -4ex -1ex -.4ex 1ex .2ex Binary Search Tree Search trees are designed to support efficient search operations, including [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps}. A binary tree is a tree with the following: Each internal node has at most two children. The children of a node are an ordered pairleft child, right child and left sub-tree, right sub-tree. The keys satisfy the binary-search tree property: \\(\\color{ocre2}u.key \\leq v.key \\leq w.key\\) Node \\(\\color{ocre2}u\\) is a node (any node) in the left sub-tree of node \\(\\color{ocre2}v\\) . Node \\(\\color{ocre2}w\\) is a node (any node) in the right sub-tree of node \\(\\color{ocre2}v\\) . ::: center ::: In other words, the value of the key of the parent should be between the value of the key of the left child and right child. A binary search tree (BST) is organized, as the name suggests, in a binary tree, where [ root[T] ]{style=\"background-color: light-gray\"} points to the root of tree [ T ]{style=\"background-color: light-gray\"} and each node contains the fields: [ key ]{style=\"background-color: light-gray\"} (and possibly other satellite data) [ left ]{style=\"background-color: light-gray\"} which points to left child. [ right ]{style=\"background-color: light-gray\"} which points to right child. [ p ]{style=\"background-color: light-gray\"} which points to parent, where [ p[root[T]] = nil ]{style=\"background-color: light-gray\"} -3ex -0.1ex -.4ex 0.5ex .2ex Inorder Traversal The binary-search tree property allows us to print out all the keys in sorted tree by a simple recursive algorithm, called an inorder tree walk, which can be visualized as such: ::: center ::: \\( \\(\\color{ocre2}D \\to B \\to E \\to A \\to F \\to C \\to G\\) \\) How [Inorder-Tree-Walk]{.smallcaps} works: Check to make sure that [ x ]{style=\"background-color: light-gray\"} is not [ nil ]{style=\"background-color: light-gray\"}. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s left sub-tree. Print [ x ]{style=\"background-color: light-gray\"}'s key. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. ::: algorithm [Inorder-Tree-Walk]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Querying Binary search tree can support such queries as [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} operations. The [Tree-Search]{.smallcaps} procedure starts at the root and traces a simple path downward in the tree. The running time is \\(\\color{ocre2}\\text{O}(h)\\) , where \\(\\color{ocre2}h\\) is the height of the tree. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: The [Iterative-Tree-Search]{.smallcaps} is more efficient in which works by \\\"unrolling\\\" the recursion into a while loop. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: The binary search-tree property guarantees that: the leftmost node is the minimum key of the binary search tree the rightmost node is the maximum key of the binary search tree Thus, the [Tree-Minimum]{.smallcaps} and [Tree-Maximum]{.smallcaps} procedure traverse the appropriate points until [nil]{.smallcaps} is reached. The running time for both is \\(\\color{ocre2}\\text{O}(h)\\) . ::: algorithm [Tree-Minimum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: ::: algorithm [Tree-Maximum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: Before going over the procedure for successor and predecessor, let's define what it means. Assuming all keys are unique, if \\(\\color{ocre2}x\\) has two children: The successor is the minimum value in its right sub-tree. The predecessor is the maximum value in its left sub-tree. Refer to this example using the key value of \\(\\color{ocre2}25\\) : ::: center ::: If you recall from earlier, when we performed inorder traversal, we can find the successor and predecessor based entirely on the tree structure. ::: algorithm [Tree-Successor]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: We can break the code for [Tree-Successor]{.smallcaps} into two cases: If [ x.right ]{style=\"background-color: light-gray\"} is non-empty, then [ x ]{style=\"background-color: light-gray\"}'s successor is the minimum in [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. If [ x.right ]{style=\"background-color: light-gray\"} is empty, then go up the tree until the current node is a left child. If you cannot go up further and you reached root, then [ x ]{style=\"background-color: light-gray\"} is the largest element. For example, if we want to find the successor of the key value of \\(\\color{ocre2}20\\) : The right sub-tree is empty, so we go up the tree to the key value of \\(\\color{ocre2}19\\) . Since \\(\\color{ocre2}20\\) is not a left child or located in the left sub-tree of \\(\\color{ocre2}19\\) , go up the tree to the key value of \\(\\color{ocre2}15\\) . Likewise, it is not a left child of \\(\\color{ocre2}15\\) , so go up the tree to the key value of \\(\\color{ocre2}25\\) . The key value of \\(\\color{ocre2}25\\) has \\(\\color{ocre2}20\\) as a left child, therefore, the successor of \\(\\color{ocre2}20\\) is \\(\\color{ocre2}25\\) . Refer to the diagram below: ::: center ::: The [Tree-Predecessor]{.smallcaps} procedure is symmetric to [Tree-Predecessor]{.smallcaps} procedure, which instead uses [ x\u2006.\u2006left ]{style=\"background-color: light-gray\"}. The running time for both is \\(\\color{ocre2}\\text{O}(h)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion The operations of insertion and deletion cause the dynamic set represented by a binary search tree to change. Thus, the binary-search tree property must hold after this change. The [Tree-Insert]{.smallcaps} procedure works quite similar to [Tree-Search]{.smallcaps} and [Iterative-Tree-Search]{.smallcaps}, which begins at the root of the tree. ::: algorithm [Tree-Insert]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: In the code, we are trying to insert [ z ]{style=\"background-color: light-gray\"} to the tree [ T ]{style=\"background-color: light-gray\"}: The pointer [ x ]{style=\"background-color: light-gray\"} traces a simple path downward looking for a [nil]{.smallcaps} to replace with the input [ z ]{style=\"background-color: light-gray\"}. The trailing pointer [ y ]{style=\"background-color: light-gray\"} maintains the parent of [ x ]{style=\"background-color: light-gray\"}. Suppose we want to insert an item with key \\(\\color{ocre2}9\\) . The while loop in lines 3-8 can be expressed as: ::: center ::: The [nil]{.smallcaps} occupies the position where we wish to place the input item [ z ]{style=\"background-color: light-gray\"}. The lines 10-15 set the pointers that cause [ z ]{style=\"background-color: light-gray\"} to be inserted. Deletion is somewhat more tricky than insertion. The process for deleting node [ z ]{style=\"background-color: light-gray\"} can be broken into three cases: Case One: If [ z ]{style=\"background-color: light-gray\"} has no children, then we simply remove it by modifying it's parent to replace [ z ]{style=\"background-color: light-gray\"} with [nil]{.smallcaps}. ::: center ::: Case Two: If [ z ]{style=\"background-color: light-gray\"} has one child, then delete [ z ]{style=\"background-color: light-gray\"} by making the parent of [ z ]{style=\"background-color: light-gray\"} point to [ z ]{style=\"background-color: light-gray\"}'s child, instead of [ z ]{style=\"background-color: light-gray\"}. ::: center ::: Case Three: If [ z ]{style=\"background-color: light-gray\"} has two children, then delete [ z ]{style=\"background-color: light-gray\"}'s successor, [ y ]{style=\"background-color: light-gray\"}, from the tree (via Case One or Case Two) and replace [ z ]{style=\"background-color: light-gray\"}'s key and satellite data with [ y ]{style=\"background-color: light-gray\"}. ::: center ::: The [Tree-Delete]{.smallcaps} procedure executes the three cases as follows. The running time is \\(\\color{ocre2}\\text{O}(h)\\) . ::: algorithm [Tree-Delete]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: As you may have notice, the running time for these operations: [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps} all take \\(\\color{ocre2}\\text{O}(h)\\) , where \\(\\color{ocre2}h\\) is the height of the tree. These operations are fast if the height of the tree is small. For binary search trees given we have \\(\\color{ocre2}n\\) items, the minimum height of a binary tree can be \\(\\color{ocre2}\\log{n}\\) and the maximum be \\(\\color{ocre2}n\\) . It can be depicted as such: ::: center ::: Ideally we want make sure the height of the binary tree is always \\(\\color{ocre2}\\log{n}\\) , as it provides the worst-case running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) , thus comes the motivation for the next topic. -3ex -0.1ex -.4ex 0.5ex .2ex Balanced Search Tree One way we can ensure our tree is always balanced is by implementing a self-balancing binary search tree. A search-tree data structure for which a height of \\(\\color{ocre2}\\log{n}\\) is guaranteed when implementing dynamic set of \\(\\color{ocre2}n\\) items. AVL Tree Red-Black Tree It ensures the \\(\\color{ocre2}\\text{O}(\\log{n})\\) time complexity at all times, by maintaining the binary-search tree property and height-balance property of the tree, whenever insertion or deletion is performed. -4ex -1ex -.4ex 1ex .2ex Red-Black Trees Red-black trees are one of many search-tree schemes that are \"balanced\" in order to guarantee that basic dynamic-set operations take \\(\\color{ocre2}\\text{O}(\\log{n})\\) time in the worst case. ::: center ::: It is a binary tree that satisfies the following red-black properties: Every node is either red or black. The root and leaves ([nil]{.smallcaps}) are black. If a node is red, then both of its children are black. For each node, all simple paths from the node to ([nil]{.smallcaps}) descendant leaves contain the same number of black nodes. To expand more on property 4, let's find the black-height of the key value of \\(\\color{ocre2}7\\) . These are all the simple paths that can be taken indicated by the grey dashed arrow above: 7, 3, [nil]{.smallcaps} 7, 18, 10, 8, [nil]{.smallcaps} 7, 18, 10, 11, [nil]{.smallcaps} 7, 18, 22, 26, [nil]{.smallcaps} If we don't include the root node, notice how all simple paths consists of the (same number of) \\(\\color{ocre2}2\\) black nodes. Likewise, the same can be said for every node in the tree. The red-black tree is a BST, so we can implement the dynamic-set operations [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} in \\(\\color{ocre2}\\text{O}(\\log{n})\\) time. -3ex -0.1ex -.4ex 0.5ex .2ex Recoloring and Rotation However, it does not directly support the dynamic-set operations [Insert]{.smallcaps} and [Delete]{.smallcaps}. Because they modify the tree, the result may violate the red-black properties. We must change color of some nodes via recoloring Restructure the links of the tree via rotation For starters, let's go over the relationship in a binary tree: ::: center ::: There's two types of procedures called [Left-Rotate]{.smallcaps} and [Right-Rotate]{.smallcaps}. ::: center ::: The letters \\(\\color{ocre2}\\alpha\\) , \\(\\color{ocre2}\\beta\\) , and \\(\\color{ocre2}\\gamma\\) represent an arbitrary sub-treeall of them have the same black-height. First, determine if recoloring needs to be done. Case One: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is red. Proceed with recoloring. Push \\(\\color{ocre2}C\\) 's black onto \\(\\color{ocre2}A\\) and \\(\\color{ocre2}D\\) . Recurse and check for \\(\\color{ocre2}C\\) 's uncle if it exists. ::: center ::: Case Two: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LR or RL imbalance. ::: center ::: There are four restructuring configurations depending on whether the double red nodes ( \\(\\color{ocre2}A\\) and \\(\\color{ocre2}B\\) ) are left or right children. ::: center ::: If there's a LR imbalance, perform [Left-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: If there's a RL imbalance, perform [Right-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: Case Three: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LL or RR imbalance. Preserve the color If there's a LL imbalance, perform [Right-Rotate]{.smallcaps} on top node. ::: center ::: If there's a RR imbalance, perform [Left-Rotate]{.smallcaps} on top node. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion To perform insertion, we insert [ x ]{style=\"background-color: light-gray\"} and color it red. The motivation for using the color red is that only property 2 and 3 might be brokenthese violation are fairly easy to fix. The [RB-Insert]{.smallcaps} procedure performs the following three cases describe in the previous section. The running time is \\(\\color{ocre2}\\text{O}(\\log{n})\\) . Suppose we want to insert \\(\\color{ocre2}15\\) using the red-black tree shown in the beginning, then we would insert as we normally would in a BST and color it red. Refer to the diagram below: ::: center ::: Just like deleting a node in a BST, it's just as complicated to delete a node in a red-black tree. The process for deleting node can be broken into three cases: Case One: If the deleted node is red, perform the deletion as you would in BST. No color changes should occur. ::: center ::: Case Two: If the deleted node is black and has one red child. Reattach the red child in place of the black node we removed, then recolor the red node as black to fix black-height of the tree. ::: center ::: Case Three: If the deleted node is black. Reattach a black child in place of the black node we removed, then recolor as a double black. ::: center ::: The double black is to keep track of where we violated the black depth property. Denoted as [ r ]{style=\"background-color: light-gray\"} and the sibling of [ r ]{style=\"background-color: light-gray\"} as [ y ]{style=\"background-color: light-gray\"}, we'll divide this into three sub-cases based on [ y ]{style=\"background-color: light-gray\"}: ::: list The color of [ x ]{style=\"background-color: light-gray\"}, parent of [ z ]{style=\"background-color: light-gray\"}, displayed can be black or red. These three sub-cases differ only on the color of [ y ]{style=\"background-color: light-gray\"}, sibling of [ r ]{style=\"background-color: light-gray\"}. ::: Case Three (a): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and has a red child [ z ]{style=\"background-color: light-gray\"}. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a left child, perform [Right-Rotate]{.smallcaps} on [ y ]{style=\"background-color: light-gray\"}, then proceed below. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a right child, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ x ]{style=\"background-color: light-gray\"} and [ z ]{style=\"background-color: light-gray\"} black, give [ y ]{style=\"background-color: light-gray\"} the former color of [ x ]{style=\"background-color: light-gray\"}, and color [ r ]{style=\"background-color: light-gray\"} black. ::: center ::: As you can see, we managed to achieve the same configuration as the original tree prior to the deletion of node. We basically converted the red node to be a black node, thus maintaining the red-black tree property. Case Three (b): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and both children of [ y ]{style=\"background-color: light-gray\"} are black. ::: center ::: If [ x ]{style=\"background-color: light-gray\"} is red, we color it black, then we color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: Otherwise, we only color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: In this case, we are essentially removing one black-height from the other sub-tree, to deal with the double black. Case Three (c): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is red. Perform an adjustment operation. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the right child of [ x ]{style=\"background-color: light-gray\"}, perform [Left-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the left child of [ x ]{style=\"background-color: light-gray\"}, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: The sibling of [ r ]{style=\"background-color: light-gray\"} should be black now, thus solve using Case Three (a) or Case Three (b). ::: list In Case Three (a) and Case Three (b), if [ r ]{style=\"background-color: light-gray\"} is in the right-side instead of the left-side, the direction of rotation changese.g. [Right-Rotate]{.smallcaps} instead of [Left-Rotate]{.smallcaps} and vice-versa, as we have done in Case Three (c). ::: -3ex -0.1ex -.4ex 0.5ex .2ex Comparing AVL and Red-Black Trees Since both provide dynamic-set operations in \\(\\color{ocre2}\\text{O}(\\log{n})\\) time, which one to choose? AVL trees provide faster lookups than Red Black Trees because they are more strictly balanced. Red-Black Trees provide faster insertion and removal operations than AVL trees as fewer rotations are done due to relatively relaxed balancing. AVL trees store balance factors or heights with each node, thus requires storage for an integer per node whereas Red Black Tree requires only 1 bit of information per node. Red-Black Trees are used in most of the language libraries like map, multi-map, multi-set in C++ whereas AVL trees are used in databases where faster retrievals are required. Graph \u00b6 -4ex -1ex -.4ex 1ex .2ex Properties of a Graph A graph should consists of the following: Vertices (nodes), which specify some entities we are interested in. Edges (lines), which specify the relationship between entities. Weights (number in lines), which specify the weight the edge represent. The formal definition of a graph is a pair \\(\\color{ocre2}(V,E)\\) where: \\(\\color{ocre2}V\\) is a collection of nodes, called vertices. \\(\\color{ocre2}E\\) is a collection of pairs of vertices, called edges. ::: exampleT Example 8.1 . We can represent the following graph using the given vertices and edges: \\(\\color{ocre2}V = \\{a,b,c,d,e,f\\}\\) \\(\\color{ocre2}E = \\{(a,c),(b,c),(c,f),(b,d),(d,f),(c,d)\\}\\) ::: center ::: ::: A graph can be categorized into one of two types, depending on the edge type: Undirected Graph. Edges do not have a directionundirected edge are unordered pair of vertices, such that \\(\\color{ocre2}(u,v)\\) and \\(\\color{ocre2}(v,u)\\) are the same edge. ::: center ::: Directed Graph. Edges with directiondirected edges are ordered pair of vertices, such that \\(\\color{ocre2}\\langle u,v \\rangle\\) and \\(\\color{ocre2}\\langle v,u \\rangle\\) are two different edges. ::: center ::: To distinguish between the two edge types, we use round brackets \\(\\color{ocre2}( )\\) for unordered pairs and angle brackets \\(\\color{ocre2}\\langle \\rangle\\) for ordered pairs. -3ex -0.1ex -.4ex 0.5ex .2ex Graph Terminology We will go over a few graph terminologies, some of which you should be familiar with. The degree of a vertex is the number of incident edges of this vertex. Below are some examples. Pay close attention to the degree of vertex \\(\\color{ocre2}z\\) . ::: center ::: Let \\(\\color{ocre2}m\\) be the number of edges and \\(\\color{ocre2}\\deg(a)\\) be the degree of vertex \\(\\color{ocre2}a\\) , then \\( \\(\\color{ocre2}\\sum_{a \\in V}\\deg(a) = 2m\\) \\) For undirected graphs, parallel edges are edges that have the same endpoints, whereas for directed graph, they are edges that have the same origin and destination. ::: center ::: Self-loop is an edge whose endpoints coincide, such as the edge \\(\\color{ocre2}(z,z)\\) ::: center ::: In this course, we will deal almost exclusively with simple graphs, which are graphs that do not have a parallel edge or self-loop. Let \\(\\color{ocre2}n\\) be the number of vertices and \\(\\color{ocre2}m\\) the number of edges, then \\( \\(\\color{ocre2}m \\leq \\frac{n(n - 1)}{2}\\) \\) There are various definitions used to describe the movement in a graph: A path is a sequence of vertices, such that consecutive vertices are adjacent. A simple path is path such that all its vertices are distinct. ::: center ::: A cycle is a path on which the first vertex is equal to the last vertex. A simple cycle is a cycle such that all its vertices are distinct, except the first and last one. ::: center ::: Lastly, we'll cover the characteristics of a connected graph and the definition of a subgraph. A connected graph is a graph in which there is a path from any vertex to any other vertex in the graph. ::: center ::: We can say a tree is a connected graph without a cycleany two vertices are connected by exactly one path. ::: center ::: A subgraph of a graph \\(\\color{ocre2}(V,E)\\) is a pair \\(\\color{ocre2}(V', E')\\) where \\(\\color{ocre2}V' \\subseteq V\\) and \\(\\color{ocre2}E' \\subseteq E\\) . Both endpoints of edges in \\(\\color{ocre2}E'\\) are in \\(\\color{ocre2}V'\\) . ::: center ::: Then a spanning tree is a subgraph of a connected graph, which includes all vertices of the connected graph. ::: center ::: -4ex -1ex -.4ex 1ex .2ex Representations of Graphs We can choose between two standard ways to represent a graph \\(\\color{ocre2}G = (V,E)\\) , as a collection of: Adjacency list Adjacency matrix Either way applies to both directed and undirected graph. They are useful in representing dense and sparse graphs: Sparse graphs. A graph with only a few edge. Dense graphs. The number of edges is close to the maximal number of edges. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency List The adjacency-list representation of a graph consists of an array \\(\\color{ocre2}\\textit{Adj}\\) of \\(\\color{ocre2}|V|\\) list, one for each vertex in \\(\\color{ocre2}V\\) . For an undirected graph, the adjacency list \\(\\color{ocre2}\\textit{Adj}[u]\\) contains all the vertices \\(\\color{ocre2}v\\) such that there is an edge \\(\\color{ocre2}(u,v), (v,u) \\in E\\) . ::: center ::: Alternatively, you can think of it as a list of all the vertices adjacent to \\(\\color{ocre2}u\\) . ::: center ::: <!-- --> - For a directed graph, the adjacency list \\(\\color{ocre2}\\textit{Adj}[u]\\) contains all the vertices \\(\\color{ocre2}v\\) such that there is an edge \\(\\color{ocre2}\\langle u,v \\rangle \\in E\\) . ::: center ![image](Figure/8/Adjacency/adjacency_list2.png){height=\"3cm\"} ::: Alternatively, you can think of it as a list of destinations given the origin $\\color{ocre2}u$. ::: center ![image](Figure/8/Adjacency/adjacency_list2a.png){height=\"2.5cm\"} ::: ::: list Note that in an adjacency list, the order doesn't matter, meaning we could have listed the vertex in any order. ::: A useful thing we could do with adjacency list is to represent weighted graphsedges with an associated weight to them. It can easily be done by storing it with vertex \\(\\color{ocre2}v\\) in \\(\\color{ocre2}u\\) 's adjacency list. ::: center ::: A potential disadvantage of the adjacency-list representation is that there is no quicker way to determine if a given edge \\(\\color{ocre2}(u,v)\\) is present in the graph. We would need to search for \\(\\color{ocre2}v\\) in the adjacency list \\(\\color{ocre2}\\textit{Adj}[u]\\) . If we want to check the edge \\(\\color{ocre2}(2,4)\\) , then we would need search through \\(\\color{ocre2}\\textit{Adj}[2]\\) . ::: center ::: The worst-case running time would be the number of adjacent vertices, which is not ideal. A solution would be to use an adjacency-matrix representation, which requires a constant time \\(\\color{ocre2}\\text{O}(1)\\) , but at the cost of using asymptotically more memory. -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency Matrix The adjacency-matrix representation of a graph consists of a \\(\\color{ocre2}|V| \\times |V|\\) matrixassuming the vertices are numbered from \\(\\color{ocre2}1\\) , \\(\\color{ocre2}2\\) , ..., to \\(\\color{ocre2}|V|\\) . We can represent the elements inside the matrix \\(\\color{ocre2}A\\) as \\(\\color{ocre2}a_{ij}\\) , where \\(\\color{ocre2}i\\) and \\(\\color{ocre2}j\\) indicate the row and column. For an undirected graph, if there is an edge \\(\\color{ocre2}(i,j), (j,i) \\in E\\) , then set \\(\\color{ocre2}a_{ij} = 1\\) , otherwise, \\(\\color{ocre2}a_{ij} = 0\\) . ::: center ::: Notice how \\(\\color{ocre2}a_{22}\\) is \\(\\color{ocre2}0\\) , since the edge \\(\\color{ocre2}(2,2)\\) does not exist. ::: center ::: For a directed graph, if there is an edge \\(\\color{ocre2}\\langle i, j \\rangle \\in E\\) , then set \\(\\color{ocre2}a_{ij} = 1\\) , otherwise, \\(\\color{ocre2}a_{ij} = 0\\) . ::: center ::: Like the adjacency-list representation of a graph, an adjacency matrix can represent a weighted graph. Instead of storing \\(\\color{ocre2}0\\) 's and \\(\\color{ocre2}1\\) 's, we store the weight of the given edge. ::: center ::: If an edge does not exist, we can store a [nil]{.smallcaps} value, depicted as empty in the diagram above. -3ex -0.1ex -.4ex 0.5ex .2ex Comparison As we have demonstrated both are applicable to undirected and directed graphs, each with their own advantages and disadvantages. **Adjacency List** **Adjacency Matrix** **Space:** $\\color{ocre2}\\Theta(|V + E|)$ $\\color{ocre2}\\Theta(|V|^2)$ Time: List all vertices adjacent to \\(\\color{ocre2}u\\) \\(\\color{ocre2}\\Theta(\\deg(u))\\) \\(\\color{ocre2}\\Theta(|V|)\\) Time: Determine if \\(\\color{ocre2}(u,v) \\in E\\) \\(\\color{ocre2}\\Theta(\\deg(u))\\) \\(\\color{ocre2}\\Theta(1)\\) The choice of which one to use comes down to the following criteria: The adjacency-list representation provides a compact way to represent sparse graphsthose for which \\(\\color{ocre2}|E|\\) is much less than \\(\\color{ocre2}|V|^2\\) . However, if \\(\\color{ocre2}|E|\\) is close to \\(\\color{ocre2}|V|^2\\) , then we may choose an adjacency-matrix representation since it almost have the same space complexity as the adjacency-list. Alternatively, if we need to be able to tell quickly if there is an edge connecting two given vertices, an adjacency-matrix representation is used. -4ex -1ex -.4ex 1ex .2ex Graph Traversals A traversal (or graph searching) is a systematic procedure for exploring a connected graph by examining all its vertices and/or edges. There's two types of traversal algorithms: Breadth-First Search (BFS) Depth-First Search (DFS) -3ex -0.1ex -.4ex 0.5ex .2ex Breadth-First Search Breadth-first search (BFS) is one of the simplest algorithms for searching a graph, which uses a queue data structure. For simplicity, we will use a tree to describe breadth-first search: Let's start at the root of tree. Let's add \\(\\color{ocre2}A\\) to the queue. ::: center ::: We want to explore all the vertices that are adjacent to \\(\\color{ocre2}A\\) , which are \\(\\color{ocre2}B\\) and \\(\\color{ocre2}C\\) . We will add them to the queue. ::: center ::: Note that the order they are placed in queue does not matter. We could have stored \\(\\color{ocre2}C\\) first. Since we finished \\\"exploring\\\" \\(\\color{ocre2}A\\) , we will move on, then \\(\\color{ocre2}B\\) is next in queue. ::: center ::: Likewise, we add all vertices adjacent to \\(\\color{ocre2}B\\) , which are \\(\\color{ocre2}D\\) and \\(\\color{ocre2}E\\) , to the queue. ::: center ::: Since we finished \\\"exploring\\\" \\(\\color{ocre2}B\\) , we will move to \\(\\color{ocre2}C\\) . ::: center ::: We will add the vertices adjacent to \\(\\color{ocre2}C\\) . which are \\(\\color{ocre2}F\\) and \\(\\color{ocre2}G\\) to the queue. ::: center ::: When we move to vertex \\(\\color{ocre2}D\\) , you will see there are no adjacent vertices, thus we don't add anything to the queue and move on to \\(\\color{ocre2}E\\) . ::: center ::: The same can be said for \\(\\color{ocre2}E\\) through \\(\\color{ocre2}G\\) , in which we finish our breadth-first search. ::: center ::: This is a simplified explanation, but should provide a general idea of how it works. We associate the vertex colors to guide the algorithm: White vertices have not been discovered. All vertices start out white. Grey vertices are discovered but not fully explored. Black vertices are discovered and fully explored. The algorithm attaches several attributes to each vertex, such as color [ u.color ]{style=\"background-color: light-gray\"}, parent [ u.\\pi ]{style=\"background-color: light-gray\"}, and distance [ u.d ]{style=\"background-color: light-gray\"} which is computed by the algorithm. To keep track of progress, breadth-first search colors each vertex white, grey, or black. Distance is used to represent the smallest number of edges that must be traverse from the starting vertex [ s ]{style=\"background-color: light-gray\"} to end vertex [ v ]{style=\"background-color: light-gray\"}. ::: algorithm [BFS]{.smallcaps} \\((G,s)\\) ::: algorithmic ::: ::: We start off by painting every vertex [ white ]{style=\"background-color: light-gray\"} except our starting vertex [ s ]{style=\"background-color: light-gray\"} and setting the distance to [ \\infty ]{style=\"background-color: light-gray\"}, as we not sure how far it is from the starting vertex or whether it is even reachable. ::: algorithm ::: algorithmic ::: ::: Then we paint our starting vertex [ s ]{style=\"background-color: light-gray\"} to [ grey ]{style=\"background-color: light-gray\"}, set the distance to [ 0 ]{style=\"background-color: light-gray\"}, since it's our starting point. The parent of [ s ]{style=\"background-color: light-gray\"} is [ nil ]{style=\"background-color: light-gray\"}doesn't exist. ::: center ::: Note that vertex [ s ]{style=\"background-color: light-gray\"} is something that we chose, which in this example is [ 5 ]{style=\"background-color: light-gray\"}. ::: algorithm ::: algorithmic ::: ::: Then we initialize [ Q ]{style=\"background-color: light-gray\"} to the queue containing just the vertex [ s ]{style=\"background-color: light-gray\"} which is [ 5 ]{style=\"background-color: light-gray\"}. ::: center ::: ::: algorithm ::: algorithmic ::: ::: The [ while ]{style=\"background-color: light-gray\"} loop functions similarly to what we have demonstrated in the first example. ::: center ::: The total running time of the [BFS]{.smallcaps} procedure is \\(\\color{ocre2}\\text{O}(V + E)\\) . As you may have notice, it is particularly useful for finding the shortest path from the starting vertex [ s ]{style=\"background-color: light-gray\"} to some vertex [ v ]{style=\"background-color: light-gray\"} in the graph. -3ex -0.1ex -.4ex 0.5ex .2ex Depth-First Search Depth-first search (DFS) as the name implies, searches \\\"deeper\\\" first until it cannot go further at which point it backtracks and continues, which uses a stack data structure. Let's use the same tree as we have used for BFS, to compare the difference: As before, we will start at the root of three. Let's add \\(\\color{ocre2}A\\) to the stack. ::: center ::: Then we arbitrarily pick an edge outwards of \\(\\color{ocre2}A\\) , which will choose \\(\\color{ocre2}B\\) and add to the stack. ::: center ::: Note there's multiple ways, so we could have also chosen to go with vertex \\(\\color{ocre2}C\\) instead. Continue to pick an edge outwards, which there is only one, so will choose \\(\\color{ocre2}D\\) . ::: center ::: Since there's no more vertices to explore, we backtrack to vertex \\(\\color{ocre2}B\\) . ::: center ::: Continue to pick an edge outwards that has not been visited yet, which is vertex \\(\\color{ocre2}E\\) . ::: center ::: Then we backtrack all the way to vertex \\(\\color{ocre2}A\\) , since all of vertex \\(\\color{ocre2}B\\) and \\(\\color{ocre2}E\\) has been explored. ::: center ::: Then, we repeat the same steps for the right sub-tree, by picking some arbitrary edge outwards until we have fully discovered every vertex. DFS uses the same color scheme as we previously described in BFS. However, one unique thing about the algorithm is the it uses two timestamps for: when it first discovers the vertex [ u.d ]{style=\"background-color: light-gray\"} and ... when it finishes exploring the vertex [ u.f ]{style=\"background-color: light-gray\"}. This is similar to BFS, which we paint every vertex [ white ]{style=\"background-color: light-gray\"}. The [ time ]{style=\"background-color: light-gray\"} is set to [ 0 ]{style=\"background-color: light-gray\"}, which will use to compute the discovery time [ u.d ]{style=\"background-color: light-gray\"} and finishing time [ u.f ]{style=\"background-color: light-gray\"}. ::: algorithm [DFS]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: For this example, we'll use a directed graph as it's \\\"easier\\\" to pick an edge outwards and demonstrate. ::: center ::: For demonstration purposes, the discovery time and finishing time is denoted below the vertex, shown on the right. ::: algorithm [DFS-Visit]{.smallcaps} \\((G,u)\\) ::: algorithmic ::: ::: We'll choose vertex [ 8 ]{style=\"background-color: light-gray\"} as our starting vertex. It'll recursively call [ DFS-Visit ]{style=\"background-color: light-gray\"} until it reaches a dead end then it colors the vertex black. ::: center ::: Once the recursion finishes, it goes to the next [ v ]{style=\"background-color: light-gray\"} that is in [ Adj[u] ]{style=\"background-color: light-gray\"} and repeats the same thing. ::: center ::: Since there's no more [ v ]{style=\"background-color: light-gray\"} (or vertex to explore) in [ Adj[u] ]{style=\"background-color: light-gray\"}. We pick a new [ u ]{style=\"background-color: light-gray\"}. ::: center ::: The total running time of the [DFS]{.smallcaps} procedure is \\(\\color{ocre2}\\text{O}(V + E)\\) , similar to [BFS]{.smallcaps} procedure. It provides valuable information about the structure of a graph and is more suitable for decision treelike a maze. We can use depth-first search to perform a topological sort of a directed acyclic graph (dag)a directed graph with no cycles. ::: center ::: By performing topological sort, we can find the topological orderingordering of its vertices along a horizontal line so that all directed edges go from left to right. ::: center ::: Note how the order they are listed in regarding their finishing times. The [Topological-Sort]{.smallcaps} sorts a dag by: ::: algorithm [Topological-Sort]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: We can perform [Topological-Sort]{.smallcaps} in \\(\\color{ocre2}\\text{O}(V + E)\\) time, since DFS takes \\(\\color{ocre2}\\text{O}(V + E)\\) . Elementary Graph Algorithms \u00b6 -4ex -1ex -.4ex 1ex .2ex Minimum Spanning Tree The minimum spanning tree is the spanning tree of a weighted graph with minimum total edge weight. What this means, is suppose we have a connected graph with weighted edges. ::: center ::: We are interested in the subset of the edges which connects all vertices together, while minimizing the total edge cost. We can form three variations of the graph's spanning tree: ::: center ::: As you can see, tree 1 in particular is our minimum spanning tree because it posses the minimum total edge weight of \\(\\color{ocre2}71\\) . Obviously when there's more vertices and edges, it will be much harder to figure out the MST. And so we'll go over two types of algorithms which does the following: Kruskal's algorithm Prim's algorithm For both algorithm, we will try and find the minimum spanning tree of the following completed graph below and compare them after. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Kruskal's Algorithm This algorithm creates a forest of trees. It works by picking the smallest edge then checking if it forms a cycle with the spanning tree formed so far. If not, the edge is added, otherwise, discard it. We start off by selecting the smallest edge in the graph, which is \\(\\color{ocre2}(6,1)\\) , as it has an edge weight of \\(\\color{ocre2}10\\) . ::: center ::: Then, we select the next smallest edge, which is \\(\\color{ocre2}(4,3)\\) , with an edge weight of \\(\\color{ocre2}12\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(7,2)\\) , with an edge weight of \\(\\color{ocre2}14\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(2,3)\\) , with an edge weight of \\(\\color{ocre2}16\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(7,4)\\) , with an edge weight of \\(\\color{ocre2}18\\) . However, notice that it forms a cycle, so we discard it. ::: center ::: Instead, we go to the next minimum edge, \\(\\color{ocre2}(5,4)\\) , with an edge weight of \\(\\color{ocre2}22\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(5,6)\\) , with an edge weight of \\(\\color{ocre2}25\\) . ::: center ::: Once we have all our vertices or have \\(\\color{ocre2}|V| - 1\\) edges, we are done with the algorithm. Our MST is complete, which has the weight of \\(\\color{ocre2}99\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Prim's Algorithm This algorithm starts with one node. It then adds a node one by one that is unconnected to the new graph. For this example, let's start at vertex \\(\\color{ocre2}6\\) . Since we started with \\(\\color{ocre2}6\\) , we have to select the smallest edge that is connected to \\(\\color{ocre2}6\\) , which are either: \\(\\color{ocre2}25\\) or \\(\\color{ocre2}10\\) . ::: center ::: We'll select the edge weight of \\(\\color{ocre2}10\\) , since that's the smallest. Likewise, we now have to select the smallest edge that is connected to either \\(\\color{ocre2}6\\) or \\(\\color{ocre2}1\\) , which will pick edge \\(\\color{ocre2}(6,5)\\) , with an edge weight of \\(\\color{ocre2}25\\) . ::: center ::: We are basically repeating the same step, but now with more and more edges to pick from. The dashed lines indicate the possible edges to select. ::: center ::: While it was not demonstrated in this example, but if an edge is selected and it forms a cycle, we will discard it and choose the next minimum edge, similar to Kruskal's algorithm. Once we have all our vertices or have \\(\\color{ocre2}|V| - 1\\) edges, we are done with the algorithm, which also have a weight of \\(\\color{ocre2}99\\) . As you can see, we have obtained the same MST for both Kruskal and Prim's algorithm, so which one to choose? Kruskal's algorithm runs faster in sparse graph, with the time complexity of \\(\\color{ocre2}\\text{O}(E\\log{V})\\) . Prim's algorithm runs faster in dense graph, with the time complexity of \\(\\color{ocre2}\\text{O}(E\\log{V})\\) , but it can be improved up to \\(\\color{ocre2}\\text{O}(E + V\\log{V})\\) using Fibonacci heaps. -4ex -1ex -.4ex 1ex .2ex Shortest Path We define the shortest path as the minimum length path from a vertex to another vertex in \\(\\color{ocre2}G\\) , if such a path exists. Previously, we have done something similar with breadth-first search with an unweighted graph, in which each edge has a weight of \\(\\color{ocre2}1\\) . ::: center ::: We are particularly interested in the single-source shortest path's problem, that is given a graph \\(\\color{ocre2}G = (V,E)\\) , what is the shortest path from a given source vertex \\(\\color{ocre2}s \\in V\\) to each vertex \\(\\color{ocre2}v \\in V\\) . ::: center ::: Suppose we chose \\(\\color{ocre2}A\\) as our source vertex, then the shortest-paths tree are: ::: center ::: If you calculate the weight from the source vertex \\(\\color{ocre2}A\\) to every other vertex, you will get the minimum edge weight possible. From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}B\\) , the total edge weight is \\(\\color{ocre2}3\\) . From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}C\\) , the total edge weight is \\(\\color{ocre2}5\\) . From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}D\\) , the total edge weight is \\(\\color{ocre2}9\\) . From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}E\\) , the total edge weight is \\(\\color{ocre2}11\\) . You also may have notice that the shortest-paths tree are not unique, meaning there can also be more than one shortest-path tree, nonetheless should still provide the same answers. In this section, we'll go over two algorithms which does the following: Dijkstra's algorithm Bellman-Ford's algorithm As will demonstrate, each algorithm have their advantages and disadvantages when choosing one over the other. Both algorithm uses the [Relax]{.smallcaps} procedure, but implement them in varying ways. ::: algorithm [Relax]{.smallcaps} \\((u,v,w)\\) ::: algorithmic ::: ::: The process of \\\"relaxing\\\" an edge is to check if its worth going through the edge \\(\\color{ocre2}\\langle u,v \\rangle\\) which would improve the shortest path from source vertex to some vertex [ v ]{style=\"background-color: light-gray\"}. Dijkstra's Algorithm relax each edge exactly once. Bellman-Ford's Algorithm relaxes each edge \\(\\color{ocre2}|V| - 1\\) times. For example, assume we have the obtained the following weighted, directed graph and we wanna find the shortest path to vertex \\(\\color{ocre2}D\\) . Suppose we start by relaxing the edge \\(\\color{ocre2}\\langle C,D \\rangle\\) . ::: center ::: Every vertex initially start with a distance of \\(\\color{ocre2}\\infty\\) and since \\(\\color{ocre2}\\infty > 7 + 6 = 13\\) , we used the edge \\(\\color{ocre2}\\langle C,D \\rangle\\) . But now let's relax the other edge, \\(\\color{ocre2}\\langle B,D \\rangle\\) . ::: center ::: Since \\(\\color{ocre2}13 > 6 + 4 = 10\\) , we now use the edge \\(\\color{ocre2}\\langle B,D \\rangle\\) instead of \\(\\color{ocre2}\\langle C,D \\rangle\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Dijkstra's Algorithm As we shall see, Dijkstra's algorithm is pretty similar to Prim's algorithm, which we covered in Minimum Spanning Tree. One limitation of this algorithm is that all edge weights must be non-negative. Let's pick \\(\\color{ocre2}A\\) as our source vertex. There's a table in the right-side, which will use to keep track of distances from the source vertex to each vertex. ::: center ::: We put infinity for the other vertices as we haven't visited them yet. A change in the distance will be indicated by the grey boxes in each step. Next, we examine the edges leaving \\(\\color{ocre2}A\\) . As denoted in the table, we can reach \\(\\color{ocre2}B\\) and \\(\\color{ocre2}C\\) with an edge weight of \\(\\color{ocre2}10\\) and \\(\\color{ocre2}3\\) respectively. ::: center ::: We always pick the smallest edge weight of which the vertex hasn't been discovered. In this case, it's \\(\\color{ocre2}\\langle A,C \\rangle\\) . ::: center ::: Notice how we don't include the edge \\(\\color{ocre2}\\langle A,B \\rangle\\) in consideration as \\(\\color{ocre2}B\\) is now reachable from \\(\\color{ocre2}\\langle A,C \\rangle\\) and \\(\\color{ocre2}\\langle C,B \\rangle\\) with a smaller total edge weight of \\(\\color{ocre2}7\\) . Now \\(\\color{ocre2}D\\) and \\(\\color{ocre2}E\\) are now reachable. As usual, we'll pick the next smallest edge, which is \\(\\color{ocre2}\\langle C, E \\rangle\\) . ::: center ::: Similar to what was described before, we don't consider the edge \\(\\color{ocre2}\\langle E, D \\rangle\\) , since \\(\\color{ocre2}D\\) has a shortest path using the edge \\(\\color{ocre2}\\langle C, D \\rangle\\) instead of \\(\\color{ocre2}\\langle E, D \\rangle\\) . Our only options left are \\(\\color{ocre2}\\langle C,B \\rangle\\) and \\(\\color{ocre2}\\langle C,D \\rangle\\) . We pick \\(\\color{ocre2}\\langle C, B \\rangle\\) , as it has the smallest edge weight of \\(\\color{ocre2}7\\) . ::: center ::: From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}D\\) , the edge \\(\\color{ocre2}\\langle C, D \\rangle\\) will results in a distance of \\(\\color{ocre2}11\\) , while \\(\\color{ocre2}\\langle B,D \\rangle\\) will result in a distance of \\(\\color{ocre2}9\\) . So all that's left is to pick the last remaining edge, which is \\(\\color{ocre2}\\langle B, D \\rangle\\) . ::: center ::: All the edges have been discovered, so we are done with the algorithm. -3ex -0.1ex -.4ex 0.5ex .2ex Bellman-Ford's Algorithm Bellman-Ford's algorithm is more general than Dijkstra's algorithm, such that it can deal with negative edge weights. However, it is a bit more time consuming in comparison. As usual, let start with vertex \\(\\color{ocre2}A\\) as our source vertex and the distances to each vertex listed in the right. ::: center ::: With Bellman-Ford's algorithm, we want to relax all the edges. In other words, we should test out all the possible edges that will result in the shortest path. So this will be our first iteration. Starting at \\(\\color{ocre2}A\\) , we can reach \\(\\color{ocre2}B\\) and \\(\\color{ocre2}C\\) with a weight of \\(\\color{ocre2}10\\) and \\(\\color{ocre2}3\\) , using the edges, \\(\\color{ocre2}\\langle A,B \\rangle\\) and \\(\\color{ocre2}\\langle A,C \\rangle\\) respectively. ::: center ::: From \\(\\color{ocre2}B\\) , we can reach \\(\\color{ocre2}D\\) at a total weight of \\(\\color{ocre2}12\\) using the edge \\(\\color{ocre2}\\langle B,D \\rangle\\) . Note that we won't use the edge \\(\\color{ocre2}\\langle B,C \\rangle\\) , as it result in a longer path from \\(\\color{ocre2}A\\) to \\(\\color{ocre2}C\\) . ::: center ::: From \\(\\color{ocre2}C\\) , we can reach \\(\\color{ocre2}E\\) at a total weight of \\(\\color{ocre2}5\\) using the edge \\(\\color{ocre2}\\langle C,E \\rangle\\) . ::: center ::: Also, notice if we use the edge \\(\\color{ocre2}\\langle C,B \\rangle\\) , we will get a shorter path to \\(\\color{ocre2}B\\) with a total weight of \\(\\color{ocre2}7\\) . Consequently, it also lowers the total weight of \\(\\color{ocre2}D\\) to \\(\\color{ocre2}9\\) . ::: center ::: Since we found a better path, we'll remove the edge \\(\\color{ocre2}\\langle A,B \\rangle\\) . From \\(\\color{ocre2}D\\) , we only have one edge to work with, which is \\(\\color{ocre2}\\langle D,E \\rangle\\) , however, notice how this increases the total weight of \\(\\color{ocre2}E\\) from \\(\\color{ocre2}5\\) to \\(\\color{ocre2}16\\) , so, we don't use it. ::: center ::: Similarly for \\(\\color{ocre2}E\\) , we have the edge \\(\\color{ocre2}\\langle E,D \\rangle\\) , however, this also increase the weight of \\(\\color{ocre2}D\\) by \\(\\color{ocre2}9\\) to \\(\\color{ocre2}12\\) , so we don't use it. ::: center ::: Since we have checked all vertex. we're done with our first iteration. The algorithm at most takes \\(\\color{ocre2}|V| - 1\\) iterations to fully obtain the shortest-tree path. However, it can sometimes be less if there's no changes occurring after the next iteration. In our second iteration, we proceed to the do the following, but using the graph we got from our first iteration. ::: center ::: Our only option is \\(\\color{ocre2}\\langle A,B \\rangle\\) , however, this doesn't improve the distances of \\(\\color{ocre2}B\\) , so we don't update the graph. Same thing can be said for \\(\\color{ocre2}B\\) , \\(\\color{ocre2}C\\) , \\(\\color{ocre2}D\\) and \\(\\color{ocre2}E\\) . So after our second iteration, we are finished. ::: list In the actual exam, they might provide you the edges in which you should relax them by order. The process will still be the same as described from above, but the order may be different, as they might ask you to relax the edges of \\(\\color{ocre2}E\\) before \\(\\color{ocre2}D\\) or etc. ::: One useful property of Bellman-Ford algorithm is that we can also use it to check for the existence of negative cycleone in which the overall sum of the cycle becomes negative. ::: center ::: If you add the weights of its edges, it's negative ( \\(\\color{ocre2}-6 + 3 + 2 = -1\\) ). The concept of a shortest path is meaningless if there is a negative cycle, as we'll have a continuous loop. Refer to the example below. To demonstrate this, let's use the following weighted, directed graph and set \\(\\color{ocre2}A\\) as the source vertex. As you can see, we will loop through the cycle \\(\\color{ocre2}B\\) , \\(\\color{ocre2}C\\) , and \\(\\color{ocre2}D\\) , such that total weight keeps decreasing. ::: center ::: As mentioned before, Bell-man Ford's algorithm runs for \\(\\color{ocre2}|V| - 1\\) iterations and it guarantees that at the end, the distances are guaranteed to be correct or [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is false. ::: center ::: However, as you can see that is not the case, such that [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is still true, thus, it will detect a negative cycle. As a summary, comparing the two algorithms we've covered in this section:: The Dijkstra's algorithm is less time consuming, as it has a time complexity of \\(\\color{ocre2}\\text{O}(E\\log{V})\\) , compared to Bellman-Ford's algorithm which is \\(\\color{ocre2}\\text{O}(VE)\\) . Bellman-Ford works when there is negative weight edge, it also detects the negative weight cycle. Dynamic programming approach is taken to implement the algorithm. While Dijkstra's algorithm doesn't work when there is a negative weight edge. Greedy approach is taken to implement the algorithm.","title":"COE428"},{"location":"W2022/COE428/COE428/#coe428","text":"This will cover various topics for the course COE428: Algorithms and Data Structures, using the textbook, Introduction to Algorithms , by T. Cormen, C. Leiserson, R. Rive, and lectures notes provided by the professor, Dr. Reza Samavi. Other resources used: Abdul Bari - Algorithms Last Updated: 2022-04-16","title":"COE428"},{"location":"W2022/COE428/COE428/#introduction-to-algorithms","text":"","title":"Introduction to Algorithms"},{"location":"W2022/COE428/COE428/#algorithm-analysis","text":"An algorithm is a sequence of computational step that transform the input into the output. You will typically see them displayed as pseudocode shown below: It is very convenient to classify algorithms based on the relative amount of time or relative amount of space they require and specify as a function of the input size. Thus, we have the notions of: - Time Complexity: Running time of the program as a function of the size of input. - Space Complexity: Amount of computer memory required during the program execution, as a function of the input size. - Running Time The time complexity of an algorithm can be measured by characterizing running time as a function, \\(\\color{ocre2}T(n)\\) , of the input size, \\(\\color{ocre2}n\\) . We count the number of primitive operations that are executed: Assigning a value to a variable Calling a method Performing an arithmetic operation Comparing two values Indexing into an array Returning from a method ::: algorithm ::: algorithmic \\(temp = a\\) ; \\(a = b\\) ; \\(b = temp\\) ; ::: ::: For this example, we say the running time is \\(\\color{ocre2}T(n) = 3\\) , since there's three primitive operations executed by an algorithm. When it comes to loops, it becomes a bit more complex. ::: algorithm ::: algorithmic \\(x = s[0]\\) \\(x = s[i]\\) ::: ::: We define the size of the input array to be \\(\\color{ocre2}n\\) : In line 1, its indexing an array, \\(\\color{ocre2}s[0]\\) , then assigning it to \\(\\color{ocre2}x\\) In line 2, inside the for loop: It first assigns \\(\\color{ocre2}i = 1\\) On each iteration, it makes a comparison, \\(\\color{ocre2}<\\) , for \\(\\color{ocre2}n\\) times In line 3, the if statement repeats for : Its indexing an array, \\(\\color{ocre2}s[i]\\) , then makes a comparison, \\(\\color{ocre2}>\\) In line 4, there's two possibilities that could occur: If true, then its indexing an array, \\(\\color{ocre2}s[i]\\) , then assigning it to \\(\\color{ocre2}x\\) If false, then it won't execute At the end, \\(\\color{ocre2}i++\\) , is incremented, then assigned back to \\(\\color{ocre2}i\\) The last line returns \\(\\color{ocre2}x\\) The number of primitive operations executed by algorithm can be characterized in two ways: Best-Case Analysis: \\(\\color{ocre2}T(n) = 2 + 1 + n + (n-1)(2 + 0 + 2) + 1 = 5n\\) Worst-Case Analysis: \\(\\color{ocre2}T(n) = 2 + 1 + n + (n-1)(2 + 2 + 2) + 1 = 7n - 2\\) ... to which it is bounded by two linear functions, \\(\\color{ocre2}5n \\leq T(n) \\leq 7n - 2\\) -4ex -1ex -.4ex 1ex .2ex Classes of Functions Let's first go over some common functions that characterize the running time of an algorithm: Constant function: \\(\\color{ocre2}f(n) = c\\) Linear function: \\(\\color{ocre2}f(n) = n\\) n-log-n function: \\(\\color{ocre2}f(n) = n\\log{n}\\) Quadratic function: \\(\\color{ocre2}f(n) = n^2\\) Cubic function: \\(\\color{ocre2}f(n) = n^3\\) Exponents: \\(\\color{ocre2}f(n) = b^n\\) Logarithms: \\(\\color{ocre2}f(n) = \\log_2{n}\\) A review from MTH is the summation formula, which will be useful when analyzing the time complexity of algorithms: Arithmetic series: \\(\\displaystyle\\color{ocre2}\\sum_{k = 0}^{n} k = 1 + 2 + \\cdots + n = \\frac{n(n+1)}{2}\\) Geometric series: \\(\\displaystyle\\color{ocre2}\\sum_{k = 0}^{n} x^k = 1 + x + x^2 + \\cdots + x^n = \\frac{1-x^{n+1}}{1-x}\\) ... and the following terms which you might recall from coding: \\(\\color{ocre2}\\lfloor x \\rfloor\\) or floor(x) is largest integer less than to equal to \\(x\\) . \\(\\color{ocre2}\\lceil x \\rceil\\) or ceiling(x) is least integer greater than to equal to \\(x\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Growth Rate of Running Time When choosing between algorithms, we care most about asymptotic performance; how the algorithm increases with the input size. ::: center ::: If you notice, as the input size increases, certain class of functions grows much more rapidly than others. ::: center ::: If we let the value of the growth-rate function represent the units of time, an algorithm with the function \\(\\color{ocre2}f(n) = \\log_2{n}\\) would be much more efficient than an algorithm with the function \\(\\color{ocre2}f(n) = 2^n\\) . In general, the order-of-growth can be classified to be: \\( \\(\\color{ocre2}1 < \\log{n} < \\sqrt{n} < n < n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n\\) \\)","title":"Algorithm Analysis"},{"location":"W2022/COE428/COE428/#analyzing-and-designing-algorithms","text":"In this lecture, we will go over three different types of sorting algorithm: insertion sort, merge sort, and selection sort. The purpose of sorting algorithms is to solve the following problem: Input: A sequence of \\(\\color{ocre2}n\\) numbers \\(\\color{ocre2}\\langle a_1, a_2, \\dots, a_n \\rangle\\) Output: A permutation (reordering) \\(\\color{ocre2}\\langle a'_1, a'_2, \\dots, a'_n \\rangle\\) of the input sequence such that \\(\\color{ocre2}a'_1 \\leq a'_2 \\leq \\dots \\leq a'_n\\) . ::: dBox ::: definitionT Definition 2.1 (Key). The sequence are typically stored in array. We also refer to the number as keys. ::: ::: -4ex -1ex -.4ex 1ex .2ex Insertion Sort Insertion sort is a sorting algorithm that places an unsorted element at its suitable place in each iteration. ::: algorithm [Insertion-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: Iterate from [ A[1] ]{style=\"background-color: light-gray\"} to [ A[n] ]{style=\"background-color: light-gray\"} over the array. Compare the current element [ key = A[j] ]{style=\"background-color: light-gray\"} to its predecessor. If the element is smaller than its predecessor, compare it to the elements before. Move the greater elements one position up to make space for the swapped element. ::: list Note that iterations starts at [ A[1] ]{style=\"background-color: light-gray\"}, not [ A[0] ]{style=\"background-color: light-gray\"}. For the sake of convenience, we assume a fictitious record [ A[0] ]{style=\"background-color: light-gray\"} as the sentinel value with key of \\(\\color{ocre2}-\\infty\\) . ::: It maybe easier to visualize this using images to better understand the psuedocode written. Suppose we start out with the following array with 6 elements. ::: center ::: If we apply the insertion sort algorithm, the following array would be sorted as shown below. Let's denote the [ key ]{style=\"background-color: light-gray\"} in green. ::: center ::: Since we care most about the asymptotic performance, we are interested on finding the running time \\(\\color{ocre2}T(n)\\) . ::: algorithm ::: algorithmic \\(key = A[j]\\) \\(i = j - 1\\) \\(A[i+1] = A[i]\\) \\(i = i - 1\\) \\(A[i+1] = key\\) ::: ::: The lectures note and textbook goes in-depth deriving the following running time of insertion sort: \\( \\(\\color{ocre2}T(n) = c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\sum_{j = 2}^n t_j + c_6\\sum_{j = 2}^n (t_j - 1) + c_7\\sum_{j = 2}^n (t_j - 1) + c_8(n - 1)\\) \\) where \\(\\color{ocre2}t_j\\) is the number of times the while loop is executed for that value of \\(\\color{ocre2}j\\) . The main takeaway is knowing how it sorts and the function for best and worst-case running time of the following algorithm is. In the next lecture, we will go more in-depth on analyzing the time complexity using asymptotic notations, which simplifies all of this stuff. -3ex -0.1ex -.4ex 0.5ex .2ex Best-Case Complexity The best-case scenario is when the array is already sorted. ::: center ::: In this example, the while loop does the comparison but never enters the loop, since it always find that [ A[i] ]{style=\"background-color: light-gray\"} is always less than or equal to [ key ]{style=\"background-color: light-gray\"}. ::: center ::: Thus, \\(\\color{ocre2}t_j = 1\\) , we can derive the number of comparisons for every outer loop iteration: \\( \\(\\color{ocre2}\\sum_{j=2}^n t_j \\to \\sum_{j=2}^n 1 = (n - 2) + 1 = n - 1\\) \\) Substituting this in the equation to the running time simplifies \\(\\color{ocre2}T(n)\\) to \\( \\({\\color{ocre2} \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5(n - 1) + c_8(n - 1) \\\\ &= (c_1 + c_2 + c_4 + c_5 + c_8)n - (c_2 + c_4 + 5 + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(\\color{ocre2}c_n\\) simplify to some constants \\(\\color{ocre2}a\\) and \\(\\color{ocre2}b\\) then \\( \\(\\color{ocre2}T(n) = an - b\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Worst-Case Complexity The worst-case scenario is when the array is sorted in reversefrom largest to smallest. ::: center ::: In this example, \\(\\color{ocre2}t_j\\) has to compare with all elements to the left \\(\\color{ocre2}j\\) -th positioncompare with \\(\\color{ocre2}j - 1\\) elements. Thus, \\(\\color{ocre2}t_j = j\\) , we can derive the number of comparisons for every outer loop iteration \\( \\(\\color{ocre2}\\sum_{j = 2}^{n} t_j \\to \\sum_{j = 2}^{n} j = 2 + 3 + 4 + \\dots + n = \\bigg[\\sum_{j=1}^n j\\bigg] - 1 = \\frac{n(n+1)}{2} - 1\\) \\) and as well the number of moves inside the while loop: \\( \\(\\color{ocre2}\\sum_{j = 2}^{n} (t_j - 1) \\to \\sum_{j = 2}^{n} (j - 1) = 1 + 2 + 3 + \\dots + n - 1 = \\frac{n(n-1)}{2}\\) \\) Substituting this in the equation to the running time simplifies \\(\\color{ocre2}T(n)\\) to \\( \\({\\color{ocre2} \\begin{split} T(n) &= c_1n + c_2(n - 1) + c_4(n - 1) + c_5\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + (c_6 + c_7)\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_8(n - 1) \\\\ &= \\bigg[\\frac{c_5}{2} + \\frac{c_6}{2} + \\frac{c_7}{2}\\bigg]n^2 + (c_1 + \\dots + c_8)n - (c_2 + \\dots + c_8) \\end{split}}\\) \\) or equivalently, if we let \\(\\color{ocre2}c_n\\) simplify to some constants \\(\\color{ocre2}a\\) , \\(\\color{ocre2}b\\) and \\(\\color{ocre2}c\\) then \\( \\(\\color{ocre2}T(n) = an^2 + bn - c\\) \\) -4ex -1ex -.4ex 1ex .2ex Merge Sort Merge sort closely follows the divide-and-conquer paradigm. Intuitively, it operates as follows. Divide: Divide the \\(\\color{ocre2}n\\) -element sequence to be sorted into two subsequences of \\(\\color{ocre2}n = 2\\) elements each. Conquer: Sort the two subsequences recursively using merge sort. Combine: Merge the two sorted subsequences to produce the sorted answer. ::: algorithm [Merge-Sort]{.smallcaps} \\((A,p,r) \\to A[p \\dots r]\\) ::: algorithmic \\(q = \\lfloor (p + r)/2 \\rfloor\\) [Merge-Sort( \\(A,p,q\\) )]{.smallcaps} [Merge-Sort( \\(A,q + 1,r\\) )]{.smallcaps} [Merge( \\(A,p,q,r\\) )]{.smallcaps} ::: ::: Split the deck into two piles, until these become simple enoughan array of size \\(\\color{ocre2}1\\) . Sort the left pile and sort the right pile using [ Merge-Sort() ]{style=\"background-color: light-gray\"}. Merge both piles into the final pile. ::: list In the [ Merge-Sort(A,p,r) ]{style=\"background-color: light-gray\"}, the floor function is used to determine [ q ]{style=\"background-color: light-gray\"}, so in the case there's a decimalit will result in an integer, ex. \\(\\color{ocre2}\\lfloor 7.5\\rfloor = 7\\) . ::: It may also be helpful to use a diagram like before to fully understand what's happening. The number in red denotes the order in which steps are processed. ::: center ::: If you prefer are more concrete example, look at the code I wrote on the left-side, which demonstrate how recursion works in this sorting algorithm. Each indent indicates the recursion depth. Step 1 to 3: Calls [ Merge-Sort(A,p,q) ]{style=\"background-color: light-gray\"} to split the left children with different values of \\(\\color{ocre2}q\\) and \\(\\color{ocre2}r\\) (passing parameter by value). Step 4: Since left child can no longer split, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on right child. Step 5: If both left and right child are already split, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Step 6: Trace back to tree structure and find the node that does not complete the splitting, call [ Merge-Sort(A,q+1,r) ]{style=\"background-color: light-gray\"} to work on the right children. Step 7 to 8: The same process is done as for Step 3 and 4. Step 9: Like in Step 5, merge them by [ Merge(A,p,q,r) ]{style=\"background-color: light-gray\"}. Most of the steps are just repeated for the other half of the array, until all children have complete the splitting, then they are merged together. -3ex -0.1ex -.4ex 0.5ex .2ex Merge Algorithm The key operation of the merge sort algorithm is the merging of two sorted sequences, after divide and conquer. ::: algorithm [Merge]{.smallcaps} \\((A,p,q,r) \\to A[p \\dots q]\\) and \\(A[q + 1 \\dots r]\\) where \\(p \\leq q \\leq r\\) ::: algorithmic \\(n_1 = q - p + 1\\) \\(n_2 = r - q\\) Let L[1 ... \\(n_1 + 1\\) ] and L[1 ... \\(n_2 + 1\\) ] be new arrays \\(L[i] = A[p + i - 1]\\) \\(R[j] = A[q + j]\\) \\(L[n_1 + 1] = \\infty\\) \\(R[n_2 + 1] = \\infty\\) \\(i = 1\\) \\(j = 1\\) \\(A[k] = L[i]\\) \\(i = i + 1\\) \\(j = j + 1\\) ::: ::: It may look like a lot, but it's pretty simple. Most of the code are explained in the comments listed in the right. The main focus here is the [ for ]{style=\"background-color: light-gray\"} loop in Line 12 to 17. ::: center ::: The heavily shaded elements in [ A ]{style=\"background-color: light-gray\"} contain values that will be copied over, and heavily shaded elements in [ L[] ]{style=\"background-color: light-gray\"} and [ R[] ]{style=\"background-color: light-gray\"} contain values that have already been copied back into [ A[] ]{style=\"background-color: light-gray\"}. The lightly shaded elements in [ A[] ]{style=\"background-color: light-gray\"} indicate their final value. -3ex -0.1ex -.4ex 0.5ex .2ex Time Complexity Let's discuss the time complexity of the following algorithm, which we can break down to the divide-and-conquer paradigm. The time to split deck takes can be denoted by \\(\\color{ocre2}c_1\\) , as it takes constant timedoes not depend on any input. The time to sort left pile and sort right pile can be denoted by \\(\\color{ocre2}2T(n/2)\\) , due to recursion, where the size is now divided by two, \\(\\color{ocre2}n/2\\) . The time to merge piles can be denoted by \\(\\color{ocre2}c_2n + c_3\\) , as it takes linear timeonly the [ for ]{style=\"background-color: light-gray\"} loop depends on input size, while the rest take constant time, thus simplified to that. The time complexity results to \\( \\(\\color{ocre2}T(n) = c_1 + T(n/2) + T(n/2) + c_2n + c_3\\) \\) Our goal is to determine the most rapidly growing term in \\(\\color{ocre2}T(n)\\) and so we can set a few rules. We set constants \\(\\color{ocre2}c_n\\) to either: \\(\\color{ocre2}0\\) , if they will not be significant in the most rapidly growing term or ... \\(\\color{ocre2}1\\) , if they will be For \\(\\color{ocre2}T(n)\\) , when \\(\\color{ocre2}n > 1\\) , we can set \\(\\color{ocre2}c_1\\) and \\(\\color{ocre2}c_3\\) to \\(\\color{ocre2}0\\) and \\(\\color{ocre2}c_2\\) to \\(\\color{ocre2}1\\) , which simplifies to: \\( \\(\\color{ocre2}T(n) = \\begin{cases}c_1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) ::: list Note that when \\(\\color{ocre2}n = 1\\) , the code inside [ if ... else ]{style=\"background-color: light-gray\"} wont run, as there's only one element \\(\\color{ocre2}p \\nless r\\) or \\(\\color{ocre2}1 \\nless 1\\) , so we write it as a constant. ::: In order to solve the recurrence, when \\(\\color{ocre2}n > 1\\) , we need a base case, so for simplicity, \\(\\color{ocre2}T(1) = 0\\) and we can make a deduction from it. ::: tabu c | c c | c \\(\\color{ocre2}n\\) & \\(\\color{ocre2}T(n/2)\\) & \\(\\color{ocre2}2T(n/2) + n\\) & \\(\\color{ocre2}n\\log_2{n}\\) \\ \\(\\color{ocre2}2\\) & \\(\\color{ocre2}0\\) & \\(\\color{ocre2}2(0) + 2 = 2\\) & \\(\\color{ocre2}2\\) \\ \\(\\color{ocre2}4\\) & \\(\\color{ocre2}2\\) & \\(\\color{ocre2}2(2) + 4 = 8\\) & \\(\\color{ocre2}8\\) \\ \\(\\color{ocre2}8\\) & \\(\\color{ocre2}8\\) & \\(\\color{ocre2}2(8) + 8 = 24\\) & \\(\\color{ocre2}24\\) \\ \\(\\color{ocre2}16\\) & \\(\\color{ocre2}24\\) & \\(\\color{ocre2}2(24) + 16 = 64\\) & \\(\\color{ocre2}64\\) \\ \\(\\color{ocre2}32\\) & \\(\\color{ocre2}64\\) & \\(\\color{ocre2}2(64) + 32 = 160\\) & \\(\\color{ocre2}160\\) \\ ::: Examining the numbers allows us to form an educated guess it is growing by a function of \\(\\color{ocre2}n\\log_2{n}\\) , which can also be deducted by drawing a recursion tree. We start by representing \\(\\color{ocre2}T(n) = 2T(n/2) + n\\) as a graph where we put the non-recursive part ( \\(\\color{ocre2}n\\) in this case) on the top row and put each recursive part on a row below. ::: center ::: Then expand downwards for the next level. ::: center ::: Repeat the same process. Eventually, it will reach a certain height which it reaches the base case and stop. ::: center ::: ::: {#height_avl} If you notice the sum of the non-recursive elements for each level is \\(\\color{ocre2}n\\) . Let's denote the depth or height of the tree as \\(\\color{ocre2}h\\) and so we can say the time complexity is ::: \\( \\(\\color{ocre2}T(n) = n \\times h\\) \\) It will eventually reach the base case which we set to some constant when \\(\\color{ocre2}n=1\\) , where \\(\\color{ocre2}T(1) = 1\\) . We can rewrite the fraction in terms of the depth, \\(\\color{ocre2}h\\) , where \\( \\(\\color{ocre2}\\frac{n}{2^h} = 1 \\to n = 2^h \\to h = \\log_2{n}\\) \\) Thus, the time complexity is \\( \\(\\color{ocre2}T(n) = n\\log_2{n}\\) \\) If you recall the order-of-growth from Lecture 2, we know that \\(\\color{ocre2}n\\log{n} < n^2\\) , and so merge sorting beats insertion sort in the worst-case scenario, as it grows much more slowly. -4ex -1ex -.4ex 1ex .2ex Selection Sort The final sorting algorithm will cover is selection sort. ::: algorithm [Selection-Sort \\((A,n) \\to A[1 \\dots\\ n]\\) ]{.smallcaps} ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Select the first element as [ min ]{style=\"background-color: light-gray\"} Compare [ min ]{style=\"background-color: light-gray\"} with the second element. If the second element is smaller than minimum, assign the second element as [ min ]{style=\"background-color: light-gray\"}. Repeat until last element. After each iteration, [ min ]{style=\"background-color: light-gray\"} is placed in the front of the unsorted list. For each iteration, indexing starts from the first unsorted element. The steps are repeated until sorted. Let's use the example as we did for insertion sort, which is the following array with 6 elements. ::: center ::: The first iteration would like something like this. Let's denote the [ min ]{style=\"background-color: light-gray\"} in green and the line to the element it's being compared to. The arrow indicates a swap to be made. ::: center ::: In the next iteration, the first unsorted element is [ A[2] ]{style=\"background-color: light-gray\"}, so it starts at \\(\\color{ocre2}3\\) . ::: center ::: Then it is repeated until all the elements are placed at their correct positions. -3ex -0.1ex -.4ex 0.5ex .2ex Time Complexity As we have covered for the other algorithm, let's analyze the time complexity of selection sort. ::: algorithm ::: algorithmic \\(min = i\\) \\(min = j\\) Interchange \\(A[i]\\) with \\(A[min]\\) ::: ::: Combining each one of them, we get the following running time of selection sort: \\( \\(\\color{ocre2}T(n) = c_1n + c_2(n-1) + c_3\\sum_{j=2}^n j + c_4\\sum_{j = 2}^n (j - 1) + c_5(n-1)\\) \\) As we have previously done with insertion sort, we can simplify the summation using the arithmetic series \\( \\({\\color{ocre2} \\begin{split} T(n) &= c_1n + c_2(n-1) + c_3\\bigg[\\frac{n(n+1)}{2} - 1\\bigg] + c_4\\bigg[\\frac{n(n-1)}{2}\\bigg] + c_5(n-1) \\\\ &= \\bigg[\\frac{c_3}{2}+\\frac{c_4}{2}\\bigg]n^2 + (c_1 + \\dots + c_5)n - (c_2 + \\dots + c_5) \\end{split}}\\) \\) or equivalently, if we let \\(\\color{ocre2}c_n\\) simplify to some constants \\(\\color{ocre2}a\\) , \\(\\color{ocre2}b\\) and \\(\\color{ocre2}c\\) then \\( \\(\\color{ocre2}T(n) = an^2 + bn - c\\) \\) Comparing it to the other two algorithms discussed, selection sort is on par with insertion sort in the worst-case scenario and so merge sorting is better than selection sort as well.","title":"Analyzing and Designing Algorithms"},{"location":"W2022/COE428/COE428/#complexity-analysis","text":"-4ex -1ex -.4ex 1ex .2ex Asymptotic Notations As covered briefly in the growth rate of running time, it's hard to determine which algorithm is better with no prior knowledge of the input size, so we consider the asymptotic behavior of the two functions for very large input size \\(\\color{ocre2}n\\) . We use specific notations called asymptotic notations to express mathematical properties of asymptotic efficiency. ::: dBox ::: definitionT Definition 3.1 (Asymptotic efficiency). The study of how the running time of an algorithm increases as the size of the input increases without bound. ::: ::: There are three asymptotic notations, which will go over in this lecture: Big-Oh notation, \\(\\color{ocre2}\\text{O}()\\) , for the upper bound or worst-case complexity Big-Omega notation, \\(\\color{ocre2}\\Omega()\\) , for the lower bound or best-case complexity Theta notation, \\(\\color{ocre2}\\Theta()\\) , for the average bound or average-case complexity We can apply these to the previous lecture, which we covered three different sorting algorithms with varying time complexity: Algorithm Time Complexity Best Worst Insertion Sort \\(\\color{ocre2}\\Omega(n)\\) \\(\\color{ocre2}\\text{O}(n^2)\\) Merge Sort \\(\\color{ocre2}\\Omega(n\\log{n})\\) \\(\\color{ocre2}\\text{O}(n\\log{n})\\) Selection Sort \\(\\color{ocre2}\\Omega(n^2)\\) \\(\\color{ocre2}\\text{O}(n^2)\\) -3ex -0.1ex -.4ex 0.5ex .2ex Big-Oh Notation (O-notation) The notation represents the upper bound of the running time of an algorithm. Thus, it gives the worst-case complexity of an algorithm. \\( \\({\\color{ocre2} \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq f(n) \\leq cg(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Some tips for determining \\(\\color{ocre2}\\text{O}()\\) complexity: Ignore the constants: \\( \\(\\color{ocre2}5n \\to n\\) \\) Certain terms dominate other, which we ignore lower order terms: \\( \\(\\color{ocre2}\\text{O}(1) < \\text{O}(\\log{n}) < \\text{O}(n) < \\text{O}(n\\log{n}) < \\text{O}(n^2) < \\cdots < \\text{O}(2^n) < \\cdots < \\text{O}(n!) < \\text{O}(n^n)\\) \\) It might be easier to understand if we have examples to determine the big-Oh notation. ::: exampleT Example 3.1 . Determine the upper bound \\(\\color{ocre2}\\text{O}()\\) for \\(\\color{ocre2}f(n)\\) : \\(\\color{ocre2}f_A(n) = an^2 + bn + c\\) is \\(\\color{ocre2}\\text{O}(n^2)\\) \\(\\color{ocre2}f_B(n) = 2n + 3\\) is \\(\\color{ocre2}\\text{O}(n)\\) \\(\\color{ocre2}f_C(n) = 5 + (15 \\cdot 20)\\) is \\(\\color{ocre2}\\text{O}(1)\\) \\(\\color{ocre2}f_D(n) = n^2\\log{n} + n\\) is \\(\\color{ocre2}\\text{O}(n^2\\log{n})\\) ::: ::: list When writing the big-Oh notation, try to write the closest function to the running time. While the function \\(\\color{ocre2}\\text{O}(n^2)\\) is true for \\(\\color{ocre2}f_B(n)\\) , the function \\(\\color{ocre2}\\text{O}(n)\\) is the closest to \\(\\color{ocre2}f_B(n)\\) . ::: The rules for determining the \\(\\color{ocre2}\\text{O}()\\) complexity are as listed: If \\(\\color{ocre2}g(n) = \\text{O}(G(n))\\) and \\(\\color{ocre2}f(n) = \\text{O}(F(n))\\) , then: \\( \\(\\color{ocre2}f(n) + g(n) = \\text{O}(F(n)) + \\text{O}(G(n)) = \\text{O}(\\text{max}[F(n), G(n)])\\) \\) \\( \\(\\color{ocre2}f(n) \\cdot g(n) = \\text{O}(F(n)) \\cdot \\text{O}(G(n)) = \\text{O}(F(n) \\cdot G(n))\\) \\) If \\(\\color{ocre2}g(n) = \\text{O}(kG(n))\\) , where \\(\\color{ocre2}k\\) is a constant, then \\(\\color{ocre2}g(n) = \\text{O}(G(n))\\) . If \\(\\color{ocre2}f(n)\\) is a polynomial of degree \\(\\color{ocre2}d\\ (P(n) = \\sum_{i=0}^d a_in^i\\) where \\(\\color{ocre2}a_d \\neq 0)\\) , then \\(\\color{ocre2}f(n)\\) is \\(\\color{ocre2}\\text{O}(n^d)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Big-Omega Notation ( \\(\\Omega\\) -notation) The notation represents the lower bound of the running time of an algorithm. Thus, it provides the best-case complexity of an algorithm. \\( \\({\\color{ocre2} \\begin{aligned} \\Omega(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c \\text{ and } n_0 \\text{ such that } 0 \\leq cg(n) \\leq f(n) \\\\ & \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: The rules for determining \\(\\color{ocre2}\\text{O}()\\) complexity is also true for determining the \\(\\color{ocre2}\\Omega()\\) complexity. Let's use an example from before. ::: exampleT Example 3.2 . Determine the lower bound \\(\\color{ocre2}\\Omega()\\) for \\(\\color{ocre2}f(n) = 2n + 3\\) : If we look at the order-of-growth for functions, \\(\\color{ocre2}T(n)\\) belongs to the linear function, \\(\\color{ocre2}n\\) and if we define our lower and upper bounds as such ( \\({\\color{ocre2}\\overunderbraces{&\\br{2}{\\text{Lower bound}}}% {&1 < \\log{n} < \\sqrt{n} <& n &< n\\log{n} < n^2 < n^3 < \\cdots < 2^n < 3^n < \\cdots < n! < n^n&} {& &\\br{2}{\\text{Upper bound}}}}\\) \\) So the lower bound can be defined by any of the following: ( \\(\\color{ocre2}\\Omega(1) < \\Omega(\\log{n}) < \\Omega(n)\\) \\) Similar to the upper bound, we want the function closest to \\(\\color{ocre2}f(n)\\) and so \\(\\color{ocre2}f(n) = 2n + 3\\) is \\(\\color{ocre2}\\Omega(n)\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Theta Notation ( \\(\\Theta\\) -notation) The next notation encloses the function from above and below. Since it represents the upper and the lower bound of the running time of an algorithm, it is used for analyzing the average-case complexity of an algorithm. \\( \\({\\color{ocre2} \\begin{aligned} \\text{O}(g(n)) = \\{f(n) \\mid & \\text{ there exists positive constant } c_1,\\ c_2, \\text{ and } n_0 \\text{ such that } \\\\ &\\ 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) \\text{ for all } n \\geq n_0 \\end{aligned}}\\) \\) ::: center ::: Equivalently, \\(\\color{ocre2}f(n)\\) is \\(\\color{ocre2}\\Theta(g(n))\\) if and only if \\(\\color{ocre2}f(n)\\) is both \\(\\color{ocre2}\\text{O}(g(n))\\) and \\(\\color{ocre2}\\Omega(g(n))\\) . One notable example which we used previously is the function \\(\\color{ocre2}f(n) = 2n + 3\\) , which as we demonstrated in previous notations are \\(\\color{ocre2}\\text{O}(n)\\) and \\(\\color{ocre2}\\Omega(n)\\) , thus \\(\\color{ocre2}f(n)\\) is \\(\\color{ocre2}\\Theta(n)\\) . -4ex -1ex -.4ex 1ex .2ex Complexity of Code Structures Loops are considered as dynamic if they depend on input size, otherwise they are static statements, everything within a loop is considered as static statementtakes a constant amount of time, \\(\\color{ocre2}\\text{O}(1)\\) . The complexity is determined by: number of iterations in the loops \\(\\color{ocre2}\\times\\) number of static statement -3ex -0.1ex -.4ex 0.5ex .2ex For Loop The following example is a simple for loop: for (int i = 0; i < n; i++) { // statement } The for loop is a dynamic statement, as it depends on the size of \\(\\color{ocre2}n\\) . We are interested in the amount of times [ statement ]{style=\"background-color: light-gray\"} runs, which determines the time complexity of the following loop. Suppose \\(\\color{ocre2}n = 3\\) then let's determine how many iterations: ::: tabu c c c Iteration & \\(i\\) &\\ & i = 0 &\\ 2 & i = 1 &\\ 3 & i = 2 &\\ 4 & i = 3 &\\ ::: You can see that the loop executes \\(\\color{ocre2}3\\) times or in general, we can say \\(\\color{ocre2}n\\) times. Thus, the time complexity is: \\( \\(\\color{ocre2}n \\cdot 1 = \\text{O}(n)\\) \\) Note that there might be few variations of the for loop. Suppose there are also consecutive statements: for (int i = 0; i < n; i++) { // statement } for (int i = 1; i <= n; i++) { // statement } In both examples, the loop executes for \\(\\color{ocre2}n\\) times. When we have consecutive statements, we would just add them together. If you recall, we ignore any constants of lower order terms. Thus, the time complexity is: \\( \\(\\color{ocre2}\\underbrace{n \\cdot 1}_{\\substack{\\text{The first} \\\\ \\text{for loop}}} + \\underbrace{n \\cdot 1}_{\\substack{\\text{The second} \\\\ \\text{for loop}}} = 2n = \\text{O}(n)\\) \\) Note that this is not always the case for every for loop, as it depends on the initialization, condition test, and update statement. Suppose we have the following for loop to analyze: for (int i = 1; i <= n; i = i * 2) { // statement } Let's list out each iterations of the loop, till \\(\\color{ocre2}k\\) iterations, since we do not know how many times this loop will execute. ::: tabu c c c Iteration & \\(i\\) \\ & i = 1 & \\(2^0\\) \\ 2 & i = 2 & \\(2^1\\) \\ 3 & i = 4 & \\(2^2\\) \\ \u22ee& \u22ee\\ \\(k\\) & i = \\(2^{k - 1}\\) &\\ ::: From the condition, we know that the loop will terminate once [ i > n ]{style=\"background-color: light-gray\"}. So we assume \\(\\color{ocre2}i = n\\) , when it has reach \\(\\color{ocre2}k\\) iterations; our very last iteration. Then we will solve for \\(\\color{ocre2}k\\) : \\( \\({\\color{ocre2} \\begin{split} 2^{k - 1} &= n \\\\ k - 1 &= \\log_2{n} \\\\ k &= \\log_2{n} + 1 \\end{split}}\\) \\) If you recall from earlier, we ignore lower order terms. Thus, the time complexity is: \\( \\(\\color{ocre2}(\\log_2{n} + 1) \\cdot 1 = \\text{O}(\\log_2{n})\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Nested Loop The following example is a nested for loop: for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement } } As we covered earlier, the following for loop executes \\(\\color{ocre2}n\\) times. Suppose now we have an inner loop, which also executes \\(\\color{ocre2}n\\) times, then the statement is run \\(\\color{ocre2}n \\times n\\) times. Thus, the time complexity is: \\( \\(\\color{ocre2}(n \\cdot n) \\cdot 1 = \\text{O}(n^2)\\) \\) The general formula for a nested loop is the time complexity of the outer loop times the inner loops. This also applies if we have a outer while loop with an inner for loop. -3ex -0.1ex -.4ex 0.5ex .2ex If Else Statement The following example is an if else statement: if (n == 0) { // statement 1 } else { for (int i = 0; i < n; i++) { // statement 2 } } If you notice, there's two possibilities that could occur: the if part, where[ statement 1 ]{style=\"background-color: light-gray\"} will run once, \\(\\color{ocre2}\\text{O}(1)\\) or the else part, where [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(\\color{ocre2}n\\) times, \\(\\color{ocre2}\\text{O}(n)\\) . In general, the time complexity of an if else statement is: \\( \\(\\color{ocre2}\\text{O}(if-else) = \\text{O}\\Big(\\text{max}\\Big[\\text{O}(\\text{condition1}), \\text{O}(\\text{condition2}), \\dots, \\text{O}(\\text{branch}1), \\text{O}(\\text{branch2}), \\dots\\Big]\\Big)\\) \\) As we are typically interested in the worst-cases, we only consider the branch with the largest running time. The condition runs once and then we add whichever is larger, which is the else part, thus, the time complexity is: \\( \\(\\color{ocre2}1 + n = \\text{O}(n)\\) \\) or equivalently \\( \\(\\color{ocre2}\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(1), \\text{O}(n)\\Big]\\Big) = \\text{O}(n)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Switch Statement The following example is a switch statement: switch (key) { case 'a': for (int i = 0; i < n; i++) { // statement 1 } case 'b': for (int i = 0; i < n; i++) { for (int j = 0; j < n, j++) { // statement 2 } } default: // statement 3 break; } Similar to the if else statement, we only consider the case with the largest running time, including the default case. In this example, for [ case \u2019b\u2019 ]{style=\"background-color: light-gray\"}, [ statement 2 ]{style=\"background-color: light-gray\"} will run for \\(n^2\\) times, \\(\\color{ocre2}\\text{O}(n^2)\\) . Thus, the time complexity is: \\( \\(\\color{ocre2}1 + n^2 = \\text{O}(n^2)\\) \\) or equivalently \\( \\(\\color{ocre2}\\text{O}\\Big(\\text{max}\\Big[\\text{O}(1), \\text{O}(n), \\text{O}(n^2), \\text{O}(1)\\Big]\\Big) = \\text{O}(n^2)\\) \\)","title":"Complexity Analysis"},{"location":"W2022/COE428/COE428/#recurrence-equations","text":"-4ex -1ex -.4ex 1ex .2ex Introduction In Lecture 2, we described the worst-case running time \\(\\color{ocre2}T(n)\\) of merge-sort procedure by the recurrence: \\( \\(\\color{ocre2}T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) whose solution we claimed to be \\(\\color{ocre2}T(n) = \\Theta(n\\log n)\\) . Previously, we didn't really have a general method for finding the form of recurrences. Our goal for this lecture is to go in-depth in ways we can analyze recursive algorithms and form a general formula. ::: dBox ::: definitionT Definition 4.1 (Recursive algorithm). An algorithm which calls itself to solve smaller problems. ::: ::: Recurrence can be polymorphic, meaning it can take many forms: A recursive algorithm which divides to two problem with equal sizes. \\( \\(\\color{ocre2}T(n) = 2T(n/2) + \\Theta(n)\\) \\) A recursive algorithm might divide subproblems into unequal sizes. \\( \\(\\color{ocre2}T(n) = T(2n/3) + T(n/3) + \\Theta(n)\\) \\) They are not necessarily constrained to being a constant fraction of the original problem size. \\( \\(\\color{ocre2}T(n) = T(n-1) + \\Theta(1)\\) \\) -4ex -1ex -.4ex 1ex .2ex Finding the Asymptotic Bounds There are three methods for solving recurrencesthat is, for obtaining asymptotic \" \\(\\color{ocre2}\\Theta\\) \" or \" \\(\\color{ocre2}\\text{O}\\) \" bounds on the solution: Substitution Method Recursion-Tree Method Master Method -3ex -0.1ex -.4ex 0.5ex .2ex Substitution Method This method is powerful, but we must be able to guess the form of the answer in order to apply it. It comprises of the following steps: Step 1: Try a few substitutions to find a pattern. Step 2: Guess the recurrence formula after \\(\\color{ocre2}k\\) iterations (in terms of \\(\\color{ocre2}k\\) and \\(\\color{ocre2}n\\) ). Step 3: Set \\(\\color{ocre2}k\\) so we get the base case. Step 4: Put \\(\\color{ocre2}k\\) back into the formula to find a potential closed form. Step 5: Prove the potential closed form using induction. Using the merge-sort algorithm as an example, which has the following recurrence: \\( \\(\\color{ocre2}T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ 2T(n/2) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) Let's go step-by-step, as described. The easiest way to find a pattern, is by simply writing out the first few iterations. Let's denote \\(\\color{ocre2}k\\) as our number of iterations starting from \\(\\color{ocre2}1\\) . \\( \\({\\color{ocre2}\\begin{split} k &= 1 & T(n) &= 2T(n/2) + n \\\\ k &= 2 & T(n) &= 2\\Big[2T(n/4) + n/2\\Big] + n = 4 \\cdot T(n/4) + 2n \\\\ k &= 3 & T(n) &= 2\\bigg[2\\Big[2T(n/8) + n/4\\Big] + n/2\\bigg] + n = 8 \\cdot T(n/8) + 3n \\\\ \\end{split}}\\) \\) Our goal is to generalize this for \\(\\color{ocre2}k\\) iterations. In other words, relating each of the constants to \\(\\color{ocre2}k\\) . We can rewrite it as such \\( \\({\\color{ocre2}\\begin{split} k &= 1 & T(n) &= 2^1 \\cdot T(n/2^1) + 1n \\\\ k &= 2 & T(n) &= 2^2 \\cdot T(n/2^2) + 2n \\\\ k &= 3 & T(n) &= 2^3 \\cdot T(n/2^3) + 3n \\\\ \\end{split}}\\) \\) Thus, we can form a general formula, using in terms of \\(\\color{ocre2}k\\) and \\(\\color{ocre2}n\\) \\( \\(\\color{ocre2}T(n) = 2^k \\cdot T(n/2^k) + kn\\) \\) We know the base case is set to \\(\\color{ocre2}T(1) = 1\\) . From our general formula, we can determine how many iterations there are in terms of \\(\\color{ocre2}n\\) to reach the base case, by solving for \\(\\color{ocre2}k\\) . \\( \\({\\color{ocre2}\\begin{split} \\frac{n}{2^k} = 1 \\ \\to\\ n &= 2^k \\\\ k &= \\log_2{n} \\end{split}}\\) \\) Substituting \\(\\color{ocre2}k = \\log_2{n}\\) back into the general formula, we get a potential closed form, as \\(\\color{ocre2}T()\\) is no longer inside our formula. \\( \\({\\color{ocre2}\\begin{split} T(n) &= 2^{\\log_2{n}} \\cdot T(n/2^{\\log_2{n}}) + n\\log_2{n} \\\\ &= n + n\\log_2{n} \\end{split}}\\) \\) We can guess that the solution is \\(\\color{ocre2}T(n) = \\text{O}(n\\log{n})\\) . However, we need a definite proof that this is true, by using mathematical induction for the following statement. \\( \\(\\color{ocre2}0 \\leq T(n) \\leq cn\\log{n} \\hspace{1cm} \\exists c > 0,\\ \\forall n \\geq n_0\\) \\) -2ex -0.1ex -.2ex .2ex .2ex Mathematical Induction In order to prove something is true, we use mathematical induction. We must show that we can choose the constant \\(\\color{ocre2}c\\) large enough so that \\(\\color{ocre2}T(n) \\leq cn\\log{n}\\) is true. Remember, the base case is \\(\\color{ocre2}T(1) = 1\\) . Then for \\(\\color{ocre2}n = 1\\) , it yields \\(\\color{ocre2}T(1) \\nleq c(1)\\log{1} = 0\\) . Consequently, the base case fails to hold, so what now? For asymptotic notation we can specify a specific bound, \\(\\color{ocre2}\\forall n \\geq n_0\\) , where \\(\\color{ocre2}n_0\\) is something we can choose. Thus \\(\\color{ocre2}n_0 = 2\\) , removing it from consideration in the induction proof. The induction proof consists of three parts: the base case, inductive hypothesis and inductive step. Let's assume \\(\\color{ocre2}n\\) is some power of \\(\\color{ocre2}2\\) or \\(\\color{ocre2}n = 2^k\\) , for sake of convenience. Base Case: Let \\(\\color{ocre2}k = 1\\) or \\(\\color{ocre2}n = 2\\) then: \\( \\(\\color{ocre2}T(2) = 2T(1) + 2 = 2 + 2 = 4 \\leq c(2)\\log{2}\\) \\) We can see that the inequality holds true for the base case, such that there \\(\\color{ocre2}c \\geq 2\\) . Inductive hypothesis: We will now assume that our proposition, \\(\\color{ocre2}T(n) = \\text{O}(n\\log{n})\\) , holds true for \\(\\color{ocre2}k -1\\) , which equivalently is \\(\\color{ocre2}n/2\\) , therefore: \\( \\(\\color{ocre2}T(n/2) \\leq c(n/2)\\log{(n/2)}\\) \\) To prove the inductive step, one assumes the induction hypothesis for \\(\\color{ocre2}k-1\\) and then uses this assumption to prove that the statement holds for \\(\\color{ocre2}k\\) . If instead, we assume our hypothesis to hold for \\(\\color{ocre2}k\\) , then we must prove it holds for \\(\\color{ocre2}k+1\\) . Inductive step: From our hypothesis, prove the guess of correct for \\(\\color{ocre2}k\\) . Using the following: \\( \\(\\color{ocre2}T(n) = 2T(n/2) + n\\) \\) Since we know \\(\\color{ocre2}T(n/2) \\leq c(n/2)\\log{(n/2)}\\) , then we can rewrite it as such: \\( \\({\\color{ocre2}\\begin{split} T(n) &\\leq 2\\Big[c(n/2)\\log{(n/2)}\\Big] + n \\\\ &\\leq cn\\log{(n/2)} + n = cn\\log{n} - cn\\log{2} + n \\\\ &\\leq cn\\log{n} + (1 - c)n \\\\ &\\leq cn\\log{n}\\qquad (\\forall c \\geq 1) \\end{split}}\\) \\) From the inductive step, we proved that proposition is true as we found that there exists some value of \\(\\color{ocre2}c\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Recursion-Tree Method In a recursion tree, we sum the costs within each level of the tree to obtain a set of per-level costs, and then we sum all the per-level costs to determine the total cost of all levels of the recursion. Step 1: Start by substituting the parent with non-recursive part of the formula and adding child nodes for each recursive part. Step 2: Expand each node repeating the step above, until you reach the base case. We already covered how to do this using merge-sort algorithm, so let's start off simple, by using the following recurrence: \\( \\(\\color{ocre2}T(n) = \\begin{cases} 1 & \\text{if}\\ n = 1 \\\\ T(n-1) + n & \\text{if}\\ n > 1 \\end{cases}\\) \\) As usual, let's go step-by-step. The non-recursive part, \\(\\color{ocre2}n\\) , will be the parent node and the recursive part, \\(\\color{ocre2}T(n-1)\\) , will be the child node. The costs within each level is displayed in the right-hand side. ::: center ::: Expand on \\(\\color{ocre2}T(n-1)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(\\color{ocre2}T(1) = 1\\) . The fully expanded tree has height \\(\\color{ocre2}n\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the arithmetic series, thus: \\( \\(\\color{ocre2}T(n) = 1 + \\cdots + (n-2) + (n-1) + n = \\frac{n(n+2)}{2} = \\Theta(n^2)\\) \\) Suppose you consider something a bit more complex, which divides the subproblems into unequal sizes, for the following recurrence: \\( \\(\\color{ocre2}T(n) = T(n/4) + T(n/2) + n^2\\) \\) The non-recursive part, \\(\\color{ocre2}n^2\\) , will be the parent node and the recursive part, \\(\\color{ocre2}T(n/4)\\) and \\(\\color{ocre2}T(n/2)\\) , will be the child nodes. ::: center ::: Expand on \\(\\color{ocre2}T(n/4)\\) and \\(\\color{ocre2}T(n/2)\\) , similar to the previous step. ::: center ::: Eventually, it will reach the base case of \\(\\color{ocre2}T(1)\\) . ::: center ::: Notice how the sum of all the per-level cost is equivalently the geometric series, thus: \\( \\(\\color{ocre2}T(n) = n^2\\bigg[1 + \\Big[\\frac{5}{16}\\Big] + \\Big[\\frac{5}{16}\\Big]^2 + \\cdots\\bigg] = \\Theta(n^2)\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Master Method The master method provides a \"cookbook\" method for solving recurrences of the form: \\( \\(\\color{ocre2}T(n) = aT(n/b) + f(n)\\) \\) where \\(\\color{ocre2}a \\geq 1\\) , \\(\\color{ocre2}b > 1\\) and \\(\\color{ocre2}f(n)\\) be a function of \\( \\(\\color{ocre2}f(n) = n^k\\log^p{n}\\) \\) Note that there are various variations of the master theorem, but this is definition is what I found the easiest to understand. It consists of memorizing these three cases: Case One: If \\(\\color{ocre2}\\log_b{a} > k\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^{\\log_b{a}})\\) . Case Two: If \\(\\color{ocre2}\\log_b{a} = k\\) and ... (a) \\(\\color{ocre2}p > -1\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k\\log^{p+1}{n})\\) . (b) \\(\\color{ocre2}p = -1\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k\\log({\\log{n}}))\\) . (c) \\(\\color{ocre2}p < -1\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k)\\) . Case Three: If \\(\\color{ocre2}\\log_ba < k\\) and ... (a) \\(\\color{ocre2}p \\geq 0\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^k\\log^{p}{n})\\) . (b) \\(\\color{ocre2}p < 0\\) , then \\(\\color{ocre2}T(n) =\\Theta(n^k)\\) . It looks a bit complicated at first glance, but once we get to the examples, it becomes quite easy. ::: exampleT Example 4.1 . Suppose we have the following recurrence: ( \\(\\color{ocre2}T(n) = 2T(n/2) + 1\\) \\) We know \\(\\color{ocre2}a = 2\\) and \\(\\color{ocre2}b = 2\\) , but how do we get \\(\\color{ocre2}k\\) and \\(\\color{ocre2}p\\) ? We can rewrite it in the form of \\(\\color{ocre2}n^k\\log^p{n}\\) : ( \\(\\color{ocre2}f(n) = 1 = n^0\\log^0{n}\\) \\) You can confirm that both equations are identical, thus \\(\\color{ocre2}k = 0\\) and \\(\\color{ocre2}p = 0\\) . Since \\(\\color{ocre2}\\log_2{2} > k\\) , then \\(\\color{ocre2}T(n) = \\Theta(n^{\\log_b{a}})\\) . Substituting in the values for \\(\\color{ocre2}a\\) and \\(\\color{ocre2}b\\) , we get: ( \\(\\color{ocre2}T(n) = \\Theta(n^{\\log_2{2}}) = \\Theta(n)\\) \\) ::: You can refer to this [video]{.underline} for more examples covering the three cases.","title":"Recurrence Equations"},{"location":"W2022/COE428/COE428/#elementary-data-structures","text":"-4ex -1ex -.4ex 1ex .2ex Stacks Stacks are dynamic sets in which the element removed from the set by the delete operation is prespecified. What defines a stack is that it implements a last-in, first-out (LIFO) principle, so only the top element is accessible. ::: center ::: There are three main methods on a stack: [ push(S,x) ]{style=\"background-color: light-gray\"} - Inserts an object \\(\\colorbox{light-gray}{\\texttt{x}}\\) onto top of Stack [ S ]{style=\"background-color: light-gray\"}. [ pop(S) ]{style=\"background-color: light-gray\"} - Removes the top object of stack [ S ]{style=\"background-color: light-gray\"}; if the stack is empty, an error occurs. [ top(S) ]{style=\"background-color: light-gray\"} - Returns the top object of the stack [ S ]{style=\"background-color: light-gray\"}, without removing it; if the stack is empty, an error occurs. ::: center ::: The following support methods should also be defined: [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in stack [ S ]{style=\"background-color: light-gray\"}. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if stack [ S ]{style=\"background-color: light-gray\"} is empty. We can implement each of the stack operations with just a few lines of code: ::: algorithm [Stack-Empty]{.smallcaps} \\((S)\\) ::: algorithmic \\(\\textsc{True}\\) \\(\\textsc{False}\\) ::: ::: ::: algorithm [Push]{.smallcaps} \\((S,x)\\) ::: algorithmic \\(S\\,.\\,top = S\\,.\\,top + 1\\) \\(S[S\\,.\\,top ] = x\\) ::: ::: ::: algorithm [Pop]{.smallcaps} \\((S)\\) ::: algorithmic \\\"underflow\\\" \\(S\\,.\\,top = S\\,.\\,top - 1\\) \\(S[S\\,.\\,top + 1]\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a stack [ S ]{style=\"background-color: light-gray\"} with \\(\\color{ocre2}7\\) elements. Let \\(\\color{ocre2}S\\,.\\,top\\) be a pointer to keep track of the last element (or top). When \\(\\color{ocre2}S\\,.\\,top = 0\\) , there is no elements and is empty, so stack [ S ]{style=\"background-color: light-gray\"} has \\(\\color{ocre2}0\\) elements. ::: center ::: When we call [ push(S,15) ]{style=\"background-color: light-gray\"}, \\(\\color{ocre2}S\\,.\\,top\\) moves up by \\(\\color{ocre2}1\\) and inserts element \\(\\color{ocre2}15\\) to the stack. ::: center ::: Suppose we call the following: [ push(S,6) ]{style=\"background-color: light-gray\"}, [ push(S,2) ]{style=\"background-color: light-gray\"} and [ push(S,3) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ pop(S) ]{style=\"background-color: light-gray\"}, \\(\\color{ocre2}S\\,.\\,top\\) moves down by \\(\\color{ocre2}1\\) and returns the element that was removed, which is element \\(\\color{ocre2}3\\) . ::: center ::: Although element \\(\\color{ocre2}3\\) still appears in the array, it is no longer in the stack. When we call [ push(S,9) ]{style=\"background-color: light-gray\"}, \\(\\color{ocre2}S\\,.\\,top\\) moves up by \\(\\color{ocre2}1\\) and overwrites element \\(\\color{ocre2}3\\) with \\(\\color{ocre2}9\\) . ::: center ::: If you notice, when pushing an element or popping an element off the stack, it takes a constant amount of time. Let \\(\\color{ocre2}n\\) be the numbers of elements in the stack. Each operation runs in time \\(\\color{ocre2}\\text{O}(1)\\) . The space used is \\(\\color{ocre2}\\text{O}(n)\\) . There are a few limitations we must consider: The maximum size of the stack must be defined priority and cannot be changed. When pushing a new element into a full stack, it causes an implementation error. -4ex -1ex -.4ex 1ex .2ex Queue Queue are another type of dynamic sets, which implements first-in, first-out (FIFO) principle, so queue items are removed in exactly the same order as they were added to the queue. ::: center ::: There are exist the following operations on a queue: [ enqueue(Q,x) ]{style=\"background-color: light-gray\"} - Inserts an element \\(\\colorbox{light-gray}{\\texttt{x}}\\) at the rear of the queue [ Q ]{style=\"background-color: light-gray\"}. [ dequeue(Q) ]{style=\"background-color: light-gray\"} - Removes the element at the front of queue [ Q ]{style=\"background-color: light-gray\"}. [ front() ]{style=\"background-color: light-gray\"} - Returns the front element of the queue without removing it. [ new() ]{style=\"background-color: light-gray\"} - Creates an empty queue. [ size() ]{style=\"background-color: light-gray\"} - Returns the number of objects in queue. [ isEmpty() ]{style=\"background-color: light-gray\"} - Indicates if queue is empty. ::: center ::: Assume \\(\\color{ocre2}n = Q\\,.\\,length\\) . The pseudocode for enqueue and dequeue is shown below: ::: algorithm [Enqueue]{.smallcaps} \\((Q,x)\\) ::: algorithmic \\(Q[Q\\,.\\,tail] = x\\) \\(Q\\,.\\,tail = 1\\) ::: ::: ::: algorithm [Dequeue]{.smallcaps} \\((Q)\\) ::: algorithmic \\(x = Q[Q\\,.\\,head]\\) \\(Q\\,.\\,head = 1\\) ::: ::: Note that we didn't account for the error when underflow and overflow occurs. -3ex -0.1ex -.4ex 0.5ex .2ex Performance and Limitations Let's now look at an array implementation of a queue [ Q ]{style=\"background-color: light-gray\"} with \\(\\color{ocre2}7\\) elements. Let \\(\\color{ocre2}Q\\,.\\,head\\) be a pointer for the front of the queue and \\(\\color{ocre2}Q\\,.\\,tail\\) be the back of the queue. When \\(\\color{ocre2}Q\\,.\\,head = \\color{ocre2}Q\\,.\\,tail\\) , there is no elements, so queue [ Q ]{style=\"background-color: light-gray\"} has \\(\\color{ocre2}0\\) elements. ::: center ::: When we call [ enqueue(Q,15) ]{style=\"background-color: light-gray\"}, element \\(\\color{ocre2}15\\) is added to the queue then \\(\\color{ocre2}Q\\,.\\,tail\\) moves up by \\(\\color{ocre2}1\\) . ::: center ::: Suppose we call the following: [ enqueue(Q,6) ]{style=\"background-color: light-gray\"}, [ enqueue(Q,2) ]{style=\"background-color: light-gray\"} and [ enqueue(Q,9) ]{style=\"background-color: light-gray\"}. The array is shown below. ::: center ::: When we call [ dequeue(Q) ]{style=\"background-color: light-gray\"}, element \\(\\color{ocre2}15\\) located at the front of queue indicated by \\(\\color{ocre2}Q\\,.\\,head\\) , is removed then \\(\\color{ocre2}Q\\,.\\,head\\) moves up by \\(\\color{ocre2}1\\) to element \\(\\color{ocre2}6\\) . ::: center ::: As the final scenario, suppose we filled the array from \\(\\color{ocre2}Q[2 .. 7]\\) , as shown below. ::: center ::: When we call \\(\\colorbox{light-gray}{\\texttt{enqueue(Q,x)}}\\) or add one more element, \\(\\color{ocre2}Q\\,.\\,tail\\) will have to move up by one where \\(\\color{ocre2}Q\\,.\\,head = \\color{ocre2}Q\\,.\\,tail\\) . ::: center ::: But, if you recall, this means the queue is empty, which is not the case and so the queue overflows. Similar to a stack, when enqueueing or dequeueing an element, it takes a constant amount of time. Let \\(\\color{ocre2}n\\) be the numbers of elements in the queue. Each operation runs in time \\(\\color{ocre2}\\text{O}(1)\\) . There are also a few limitations we must consider which carries over for queue: The maximum size of the stack must be defined priority and cannot be changed. If we attempt to dequeue an element from an empty queue, the queue underflows. If we attempt to enqueue an element from a full queue, the queue overflows and so we can only store \\(\\color{ocre2}n - 1\\) elements. -4ex -1ex -.4ex 1ex .2ex Linked Lists A collection of nodes that together form a linear ordering. Unlike an array, however, in which the linear order is determined by the array indices, the order in a linked list is determined by a pointer in each object. It consists of: A sequence of nodes Each node contains a value and link reference to some other node The last node contains a null link -3ex -0.1ex -.4ex 0.5ex .2ex Singly Linked Lists The most basic of all linked data structures, which are used to implement stacks and queues. Each node has data and a pointer to the next node. ::: center ::: Searching a singly linked list. ::: algorithm [List-Search]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,head\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: To search a list of \\(\\color{ocre2}n\\) elements, the [List-Search]{.smallcaps} procedure takes \\(\\color{ocre2}\\Theta(n)\\) time in the worst-case, since it may have to search the entire listsimilar to insertion sort. Inserting into a singly linked list. The [List-Insert]{.smallcaps} procedure splices the inserted element, [ x ]{style=\"background-color: light-gray\"}, onto the front of the linked list. ::: center ::: The running time for [List-Insert]{.smallcaps} on a list of \\(\\color{ocre2}n\\) elements is \\(\\color{ocre2}\\text{O}(1)\\) . Deleting from a singly linked list. The [List-Delete]{.smallcaps} procedure removes an element, [ x ]{style=\"background-color: light-gray\"}, from a linked list by getting a pointer to \\(\\colorbox{light-gray}{\\texttt{x}}\\) and it splices [ x ]{style=\"background-color: light-gray\"} out of the list by updating pointers. ::: center ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\color{ocre2}\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\color{ocre2}\\text{O}(n)\\) time is required in the worst case. Some applications of singly linked lists are: Implement stacks and queues, as shown below. Dynamic memory allocation, which will cover in the very end. -3ex -0.1ex -.4ex 0.5ex .2ex Doubly Linked Lists We add a pointer to the previous node. Thus, we can go in either direction: forward or backward. ::: center ::: Searching a doubly linked list. A singly and linked list uses the same algorithm for searching. Thus, both take \\(\\color{ocre2}\\Theta(n)\\) times in the worst-case to search through a list of \\(\\color{ocre2}n\\) elements. <!-- --> Inserting into a doubly linked list. The [List-Insert]{.smallcaps} procedure is also similar to the singly, but now we also have to account for the previous pointer. ::: center ::: ::: algorithm [List-Insert]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,head\\) \\(L\\,.\\,head\\,.\\,prev = x\\) \\(L\\,.\\,head = x\\) \\(x\\,.\\,prev =\\) [nil]{.smallcaps} ::: ::: The running time for [List-Insert]{.smallcaps} on a list of \\(\\color{ocre2}n\\) elements is \\(\\color{ocre2}\\text{O}(1)\\) . Deleting from a doubly linked list. Likewise, same thing can be said for the [List-Delete]{.smallcaps} procedure, in which we now have to also assign the previous pointer ::: center ::: ::: algorithm [List-Delete]{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(L\\,.\\,head = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: The running time for [List-Delete]{.smallcaps} runs in \\(\\color{ocre2}\\text{O}(1)\\) time, but if we wish to delete an element with a given key, \\(\\color{ocre2}\\text{O}(n)\\) time is required in the worst case. Some applications of doubly linked lists are: Browsers to implement backward and forward navigation of visited web pagesthe back and forward button. Various application to implement Undo and Redo functionality. -3ex -0.1ex -.4ex 0.5ex .2ex Circularly Linked Lists A circularly singly linked list is a variation of a linked list in which the last element is linked to the first element. This forms a circular loop. ::: center ::: A circularly doubly linked list, in which in addition to the one above, the first element is linked to the last element. ::: center ::: In a circularly linked list, we used a sentinelrepresented by the dark grey node [ L.nil ]{style=\"background-color: light-gray\"}. ::: center ::: It represents [nil]{.smallcaps} which lies between the head and tail. It functions like any other object in a doubly linked list, which it has a pointer from the previous and next node. Below are the procedures used for circularly doubly linked list with sentinel. ::: algorithm [List-Search/]{.smallcaps} \\((L,k)\\) ::: algorithmic \\(x = L\\,.\\,nil\\,.\\,next\\) \\(x = x\\,.\\,next\\) \\(x\\) ::: ::: ::: algorithm [List-Insert']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,next = L\\,.\\,nil\\,.\\,next\\) \\(L\\,.\\,nil\\,.\\,next\\,.\\,prev = x\\) \\(L\\,.\\,nil\\,.\\,next = x\\) \\(x\\,.\\,prev = L\\,.\\,nil\\) ::: ::: ::: algorithm [List-Delete']{.smallcaps} \\((L,x)\\) ::: algorithmic \\(x\\,.\\,prev\\,.\\,next = x\\,.\\,next\\) \\(x\\,.\\,next\\,.\\,prev = x\\,.\\,prev\\) ::: ::: Some applications of circularly linked lists are: Useful for implementation of queue. Circular lists are useful in applications to repeatedly go around the list. Circular doubly linked lists are used for implementation of advanced data structures like Fibonacci Heap. -3ex -0.1ex -.4ex 0.5ex .2ex Implementing Pointers and Objects We can implement pointers and objects in languages that do not provide them by synthesizing them from arrays and array indices. For this example, let's use the following doubly linked list: ::: center ::: Single-array representation of objects. Analogous to storing an object in the memory. ::: center ::: Each object is represented by a contiguous sub-array of length \\(\\color{ocre2}3\\) . The three attributes [ key ]{style=\"background-color: light-gray\"}, [ next ]{style=\"background-color: light-gray\"}, and [ prev ]{style=\"background-color: light-gray\"} correspond to the offsets: \\(\\color{ocre2}0\\) , \\(\\color{ocre2}1\\) , and \\(\\color{ocre2}2\\) of the sub-array. Multiple-array representation of objects. We can represent a collection of objects that have the same attributes by using an array for each attribute. ::: center ::: You can think of each column (or vertical slice) as a single object. The pointers resides in the [ next ]{style=\"background-color: light-gray\"} and [ prev ]{style=\"background-color: light-gray\"} array, which point to the index where the next object resides. Allocating and freeing objects. To insert a key into a dynamic set represented by a doubly linked list, we must allocate a pointer to a currently unused object in the linked-list representation. ::: center ::: We keep the free objects in a singly linked list (only [ next ]{style=\"background-color: light-gray\"} pointer), which we call the free list. The free list acts like a stackthe next object allocated is the last one freed. -4ex -1ex -.4ex 1ex .2ex Heaps The (binary) heap data structure is an array of object that we can view as a nearly complete binary tree. Each node of the tree corresponds to an element of the array. ::: center ::: There are two kinds of binary heap. In both kinds, the values in the nodes satisfy a heap property, the specifics of which depend on the kind of heap. Max-heap. The max-heap property is that for every node \\(\\color{ocre2}i\\) other than the root: \\( \\(\\color{ocre2}A[\\textsc{Parent}(i)] \\geq A[i]\\) \\) which means that a child node can't have a greater value than its parent. Min-heap. The min-heap property is the opposite, which for every node \\(\\color{ocre2}i\\) other than the root: \\( \\(\\color{ocre2}A[\\textsc{Parent}(i)] \\leq A[i]\\) \\) which means that a parent node can't have a greater value than its child nodes. If all the nodes satisfy the heap property, then a binary tree is a heap. However, if a node does not have the heap property, the node is swapped with the parent. This operation is called sifting up. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Constructing a Heap A heap can be stored as an array \\(\\color{ocre2}A\\) , where the: Root of tree is \\(\\color{ocre2}A[1]\\) . The cell at index \\(\\color{ocre2}0\\) is not used, thus we start at index \\(\\color{ocre2}1\\) . Parent of \\(\\color{ocre2}A[i]\\) is \\(\\color{ocre2}A[\\lfloor i/2 \\rfloor]\\) . Left child of \\(\\color{ocre2}A[i]\\) is \\(\\color{ocre2}A[2i]\\) . Right child of \\(\\color{ocre2}A[i]\\) is \\(\\color{ocre2}A[2i + 1]\\) . To construct a heap: Start with a single node. Add a node to the right of the rightmost node in the deepest level. If the deepest level is full, start a new level. Each time we add a node, we may destroy heap property of its parent node. To fix this, sift up until either: We reach nodes whose values don't need to be swappedthe parent node is larger than both children. We reach the root. Suppose we have an array \\(\\color{ocre2}A = [8, 10, 5, 12, 14]\\) , we would construct the heap as such: ::: center ::: Our final heap should look like this: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Maintaining Heap Property To implement this: Represent an arbitrary array as a binary tree. Devise a [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm that maintains the heap property of any given node \\(\\color{ocre2}i\\) in the heap with sub-trees \\(\\color{ocre2}l\\) and \\(\\color{ocre2}r\\) rooted at \\(\\color{ocre2}i\\) th children, given to be heaps. Devise a [ Build-Max-Heap() ]{style=\"background-color: light-gray\"} algorithm that uses [ Max-Heapify() ]{style=\"background-color: light-gray\"} algorithm to construct a heap. ::: algorithm [Max-Heapify]{.smallcaps} \\((A,n)\\) ::: algorithmic ::: ::: ::: algorithm [Build-Max-Heap]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The worst-case time complexity of: [Max-Heapify]{.smallcaps} is \\(\\color{ocre2}\\text{O}(\\log{n})\\) [Build-Max-Heap]{.smallcaps} is \\(\\color{ocre2}\\text{O}(n)\\) The heapsort algorithm is based on the heap data structure, which uses these two main parts: building a max-heap and sorting it, to sort ::: algorithm [Heapsort]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: Thus, heapsort has a worst-case time complexity of \\(\\color{ocre2}\\text{O}(n\\log{n})\\) like merge sort, but heapsort has a space complexity of \\(\\color{ocre2}\\text{O}(1)\\) , since it sorts in-place, taking a constant amount of memory. -3ex -0.1ex -.4ex 0.5ex .2ex Priority Queue One of the most popular implementations of a heap, a priority queue is a data structure for maintaining a set \\(\\color{ocre2}S\\) of elements, each with an associated value called a key. As with heaps, there are two kinds of priority queues: max-priority queue and min-priority queue. ::: center ::: We will focus here on how to implement max-priority queues, which are in turn based on max-heaps. A max-priority queue supports dynamic-set operations: [ Insert(S,x) ]{style=\"background-color: light-gray\"} - Inserts element [ x ]{style=\"background-color: light-gray\"} into set [ S ]{style=\"background-color: light-gray\"}. [ Maximum(S) ]{style=\"background-color: light-gray\"} - Returns an element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Extract-Max(S) ]{style=\"background-color: light-gray\"} - Removes and returns element of [ S ]{style=\"background-color: light-gray\"} with largest key. [ Increase-Key(S,x,k) ]{style=\"background-color: light-gray\"} - Increases value of element [ x ]{style=\"background-color: light-gray\"}'s key to [ k ]{style=\"background-color: light-gray\"}. Assume [ k \\geq x ]{style=\"background-color: light-gray\"}'s current key value. The procedure [Heap-Maximum]{.smallcaps} has a running time of \\(\\color{ocre2}\\Theta(1)\\) . ::: algorithm [Heap-Maximum]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Extract-Max]{.smallcaps} has a running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Extract-Max]{.smallcaps} \\((A)\\) ::: algorithmic ::: ::: The procedure [Heap-Increase-Key]{.smallcaps} has a running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) . ::: algorithm [Heap-Increase-Key]{.smallcaps} \\((A,i,key)\\) ::: algorithmic ::: ::: The procedure [Max-Heap-Insert]{.smallcaps} has a running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) . ::: algorithm [Max-Heap-Insert]{.smallcaps} \\((A,key)\\) ::: algorithmic ::: ::: In summary, a heap can support any priority-queue operation on a set of size \\(\\color{ocre2}n\\) in \\(\\color{ocre2}\\text{O}(\\log{n})\\) time.","title":"Elementary Data Structures"},{"location":"W2022/COE428/COE428/#hash-tables","text":"-4ex -1ex -.4ex 1ex .2ex Introduction Many applications require a dynamic set that supports only the dictionary operations. ::: dBox ::: definitionT Definition 6.1 (Dictionary). A data structure that stores (key, value) pairs and supports the operations [Insert]{.smallcaps}, [Search]{.smallcaps}, and [Delete]{.smallcaps}. ::: ::: So far we have seen a couple ways to implement dictionaries, such as linked lists. Now we will learn how to use a hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex How It Works? A hash tables takes a key (typically a string of characters or numbers) and passes it through a hash function to convert it into an index of the array to store the associated value. ::: center ::: Suppose you need to find a value of the key, you do not need to iterate through all items in the collection, because you can just use the hash function to easily find the index. Using a hash table offers a very fast lookup for a value based on the key, which should be the \\(\\color{ocre2}\\text{O}(1)\\) operation. It is a generalization of an ordinary array. -3ex -0.1ex -.4ex 0.5ex .2ex Sample Problem As an example, you can think of a phone book. In the phone book, a person's name can be considered as a key, by which we can find a phone number. Case One: The simple and straightforward way to lookup number is to check all names in the phone book until we find a matching name. The worst-case search time is \\(\\color{ocre2}\\text{O}(n)\\) . Case Two: Use a hash function that helps us to lookup entries much faster. Suppose we have a person's name \\\"James Davis\\\" with the phone number \\\"416-999-1234\\\". A hash function takes the key and maps it to an integer that is within the size of the array: \\( \\(\\color{ocre2}\\text{String}\\ \\Rightarrow\\ \\boxed{\\text{Hash Function}}\\ \\Rightarrow\\ \\text{Index}\\) \\) Then it stores the value of the phone number to an index of the array. If we continue to add more people, it would map each one to an index of the array. ::: center ::: If we wanna lookup a person's phone number, all we need is the person's name and we can easily find the index it is stored in the array, by passing it through a hash function. Obviously, this is a watered-down explanation and doesn't go in-depthlike the possibility when two or more keys hash to the same slot. But before moving further, let's understand how direct-address table works to see the benefits of using hash tables instead. -3ex -0.1ex -.4ex 0.5ex .2ex Direct Address Table With an ordinary array, we store element whose key is \\(\\color{ocre2}k\\) in position \\(\\color{ocre2}k\\) of the array. ::: center ::: ::: dBox ::: definitionT Definition 6.2 (Direct addressing). Given a key \\(\\color{ocre2}k\\) , we find the element whose key is \\(\\color{ocre2}k\\) by just looking in the \\(\\color{ocre2}k\\) th position of the array. ::: ::: A direct-address table (DAT) uses the keys as indices of the array and stores the values at those bucket locations. ::: center ::: It does facilitate fast searching, fast inserting and fast deletion operations: Inserting or deleting an element in the table, is the same as you would do for an array, hence we can do that in \\(\\color{ocre2}\\text{O}(1)\\) time as we already know the index (via key). Searching an element takes \\(\\color{ocre2}\\text{O}(1)\\) times, as we can easily access an element in an array in linear time if we already know the index of that element. Direct addressing is applicable when we can afford to allocate an array with one position for every possible key, and so it comes at a cost: It cannot handle collisionstwo keys are equal and contain different values. It is not recommended using the direct address table if the key values are very large. It has serious disadvantages, making it not suitable for the practical usage of current world scenarios, which is why we make use of hash tables. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Table As a recap, from the introduction, instead of storing an element with key \\(\\color{ocre2}k\\) in index \\(\\color{ocre2}k\\) , we use a hash function \\(\\color{ocre2}h\\) and store the element in index \\(\\color{ocre2}h(k)\\) . ::: center ::: ::: dBox ::: definitionT Definition 6.3 (Hash function). A hash function \\(\\color{ocre2}h\\) maps all possible keys to the slots of an array \\(\\color{ocre2}T[0 \\dots n - 1]\\) . ::: ::: While hash table offer the same time complexity of \\(\\color{ocre2}\\text{O}(1)\\) when we talk about insertion, deletion, or searching an element, the main focus is in its ability to maintains the size constraint. The problem with DAT is if the universe \\(\\color{ocre2}U\\) of keys is large, storing a table of size of \\(\\color{ocre2}|U|\\) may be impractical or impossible. Often, the set of keys \\(\\color{ocre2}K\\) actually stored is small, compared to \\(\\color{ocre2}U\\) . ::: problem Problem 6.1 . Suppose we have a key of \\(\\color{ocre2}7898\\) , which in turn is a large number. ::: Case One: Using a DAT table, we would need a huge array, for the key in index \\(\\color{ocre2}7898\\) to store the value at \\(\\color{ocre2}T[7898]\\) . In turn, we are wasting too much space, as most of the allocated space for the array is wasted. Case Two: But, in the case of a hash table, we can process this key via a hash function. The hash function \\(\\color{ocre2}h(7898)\\) maps it to an index within the hash table \\(\\color{ocre2}T[0 \\dots n - 1]\\) . Regarding the size of the hash table \\(\\color{ocre2}n\\) it typically varies, as it depends in part on choice of the hash function and collision resolution, where a situation might arise when two or more keys hash to the same slot. -4ex -1ex -.4ex 1ex .2ex Hash Function A good hash function should minimizes collision as mush as possible. It is usually specified as the composition of two functions \\(\\color{ocre2}h(k) = h_2\\big(h_1(k)\\big)\\) : Hash code. \\(\\color{ocre2}h_1: \\text{keys}\\ \\to\\ \\text{integers}\\) Compression function. \\(\\color{ocre2}h_2: \\text{integers}\\ \\to\\ [0 \\dots n - 1]\\) ::: center ::: The goal of the hash function is to disperse the keys in an apparently random way. -3ex -0.1ex -.4ex 0.5ex .2ex Hash Code As mentioned previously, keys can be a string of characters. Thus, if the keys are not natural numbers, we find a way to interpret them as natural numbers. Some popular hash code maps are: Summing hash code. By adding up the [ASCII values]{.underline} of each letters in a string, we get an integer in return. For example, if the key is \\\"stop\\\": \\( \\(\\color{ocre2}h_1(\"stop\") = 115 + 116 + 111 + 112\\) \\) However, this is not suitable for strings cause two different strings can have the same set of letters, but have different meaning\\\"post\\\", \\\"tops\\\", and \\\"pots\\\" will have the same hash code. Polynomial hash code. A better hash code takes into account the position of each character. Using the example from before: \\( \\(\\color{ocre2}h_1(\"stop\") = (115 \\times a^0) + (116 \\times a^1) + (111 \\times a^2) + (112 \\times a^3)\\) \\) where \\(\\color{ocre2}a\\) is a non-zero constantcompared to \\\"post\\\", \\\"tops\\\", and \\\"pots\\\", all have unique hash codes, which is ideal. -3ex -0.1ex -.4ex 0.5ex .2ex Compression Function The hash code typically returns a large range of integers and so the compression functions maps it in the range \\(\\color{ocre2}[0 \\dots n - 1]\\) , the indices of the hash table. There's two methods: Division Method. A simple-modulo based compression rule: \\( \\(\\color{ocre2}h_2(k) = k\\ \\text{mod}\\ n\\) \\) The size \\(\\color{ocre2}n\\) of the hash table is usually chosen to be a prime number, to help spread out the distribution of hash values. MAD Method. The Multiply-Add-Divide method still use \\(\\color{ocre2}\\text{mod}\\ n\\) to get the numbers in the range, but a little fancier by spreading the numbers out first: \\( \\(\\color{ocre2}h_2(k) = [(ak + b)\\ \\text{mod}\\ p]\\ \\text{mod}\\ n\\) \\) The values \\(\\color{ocre2}a\\) and \\(\\color{ocre2}b\\) are chosen at random as positive integers and \\(\\color{ocre2}p\\) is a prime number, where \\(\\color{ocre2}p > n\\) . With the addition of \\(\\color{ocre2}(ak + b)\\ \\text{mod}\\ p\\) , it eliminates patterns provided by \\(\\color{ocre2}k\\ \\text{mod}\\ n\\) . Both incorporate the modulo operator, as it guarantees the output to be within the size of the hash table. Suppose we have a key of \\(\\color{ocre2}7898\\) from the previous example and a hash table with \\(\\color{ocre2}23\\) slots: \\( \\(\\color{ocre2}h_2(7898) = 7898\\ \\text{mod}\\ 23 = 9\\) \\) Then the key will be mapped to index \\(\\color{ocre2}9\\) of the hash table. -4ex -1ex -.4ex 1ex .2ex Collision Handling Collision occurs when different elements are mapped to the same index of the arraywhen \\(\\color{ocre2}h(k_1) = h(k_2)\\) , but \\(\\color{ocre2}k_1 \\neq k_2\\) . ::: center ::: Avoiding collision is ideal, nonetheless, it is impossible, so we use closed or open addressing to overcome this problem. Each of them have their pros and cons. -3ex -0.1ex -.4ex 0.5ex .2ex Closed Addressing Closed addressing (or open hashing) is also known as separate chaining. When collision occurs, the index keeps a reference to a linked list or dynamic array that stores all items with the same index. Let \\(\\color{ocre2}e_1\\) and \\(\\color{ocre2}e_2\\) represent the values attached to \\(\\color{ocre2}k_1\\) and \\(\\color{ocre2}k_2\\) respectively. ::: center ::: Separate chaining is fairly simple to implement and faster than open addressing in general. However, it is memory inefficient as it requires a secondary data structure and longs chains can result in \\(\\color{ocre2}\\text{O}(n)\\) times. -3ex -0.1ex -.4ex 0.5ex .2ex Open Addressing Instead of referencing to a list or an array, open addressing (or closed hashing) resolves collision by searching for another empty bucket. ::: center ::: There's three types of open addressing: Linear Probing. When collision occurs, we linearly probe for the next bucket by increasing the index linearly until it finds an empty bucket: \\( \\(\\color{ocre2}\\text{Index} = \\big[h(k) + i\\big]\\ \\text{mod}\\ n\\) \\) where \\(\\color{ocre2}i\\) increases by one each iteration, until it finds an empty bucket. Quadratic Probing. Similar to the previous one, but instead we increase the index quadratically until it finds an empty bucket: \\( \\(\\color{ocre2}\\text{Index} = \\big[h(k) + i^2\\big]\\ \\text{mod}\\ n\\) \\) where \\(\\color{ocre2}i\\) increases by one each iteration, until it finds an empty bucket. Double Hashing. Using a secondary hash function \\(\\color{ocre2}h'(k)\\) , it places the colliding item in the first available cell by: \\( \\(\\color{ocre2}\\text{Index} = \\big[h(k) + jh'(k)\\big]\\ \\text{mod}\\ n\\) \\) where \\(\\color{ocre2}j\\) increases by one each iteration, until it finds an empty bucket. The secondary hash function cannot have zero values and is typically written as such: \\( \\(\\color{ocre2}h'(k) = q -(k\\ \\text{mod}\\ q)\\) \\) where \\(\\color{ocre2}q\\) is a prime number, such that \\(\\color{ocre2}q > n\\) . Unlike separate chaining, open addressing is more memory efficient, as it stores element in empty indices. However, it can create cluster: Linear probing can result in primary clustering. Quadratic probing can result in secondary clustering. Compared to the two, double hashing distributes the keys more evenly and produces a uniform distribution of records throughout the hash table.","title":"Hash Tables"},{"location":"W2022/COE428/COE428/#trees","text":"-4ex -1ex -.4ex 1ex .2ex Introduction A tree is a dynamic set of nodes storing elements in a parent-child relationship (edge) with the following properties: It has a special node called root. Each node different from the root has a parent node. There is a single unique path along the edges from the root to any particular nodedoesn't have any cycles. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Tree Terminology In a tree, we often refer to certain parts of tree, which are listed below. For reference: ::: center ::: Root: The top element with no parent ( \\(\\color{ocre2}A\\) ). Siblings: Children of the same parent ( \\(\\color{ocre2}G, H\\) both have the parent \\(\\color{ocre2}C\\) ). External node: Also referred to as leave, ndoes with no children ( \\(\\color{ocre2}E, I, J, K, G, H\\) ). Internal node: nodes with one or more children ( \\(\\color{ocre2}A, B, C, F\\) ). Ancestors: A node that is connected to all lower-level node ( \\(\\color{ocre2}A, B, F\\) are ancestors of \\(\\color{ocre2}I, J, K\\) ). Descendants: The connected lower-level nodes ( \\(\\color{ocre2}I\\) is a descendant of \\(\\color{ocre2}A, B, F\\) ). Depth of a node: Number of ancestors ( \\(\\color{ocre2}I\\) has a depth of \\(\\color{ocre2}3\\) ). Height of a tree: The max node depth (The height of tree is \\(\\color{ocre2}3\\) ). Sub-tree: A tree consisting of a node and all its descendants (Refer to the red triangle above). -3ex -0.1ex -.4ex 0.5ex .2ex Tree Traversals A traversal is defined as a systematic way of accessing or visiting all nodes of a tree. Let's use the following tree as an example: ::: center ::: There's three ways a tree can be traverse, but we'll only go over two of them. The last one will be covered in the next section. Preorder traversal. Root is visited first and then sub-trees rooted at its children are visited recursively ( \\(\\color{ocre2}A \\to B \\to D \\to E \\to C \\to F \\to G\\) ). Postorder traversal. Recursively traverse the sub-trees rooted at children and then visit the root itself ( \\(\\color{ocre2}D \\to E \\to F \\to G \\to B \\to C \\to A\\) ). -4ex -1ex -.4ex 1ex .2ex Binary Search Tree Search trees are designed to support efficient search operations, including [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps}. A binary tree is a tree with the following: Each internal node has at most two children. The children of a node are an ordered pairleft child, right child and left sub-tree, right sub-tree. The keys satisfy the binary-search tree property: \\(\\color{ocre2}u.key \\leq v.key \\leq w.key\\) Node \\(\\color{ocre2}u\\) is a node (any node) in the left sub-tree of node \\(\\color{ocre2}v\\) . Node \\(\\color{ocre2}w\\) is a node (any node) in the right sub-tree of node \\(\\color{ocre2}v\\) . ::: center ::: In other words, the value of the key of the parent should be between the value of the key of the left child and right child. A binary search tree (BST) is organized, as the name suggests, in a binary tree, where [ root[T] ]{style=\"background-color: light-gray\"} points to the root of tree [ T ]{style=\"background-color: light-gray\"} and each node contains the fields: [ key ]{style=\"background-color: light-gray\"} (and possibly other satellite data) [ left ]{style=\"background-color: light-gray\"} which points to left child. [ right ]{style=\"background-color: light-gray\"} which points to right child. [ p ]{style=\"background-color: light-gray\"} which points to parent, where [ p[root[T]] = nil ]{style=\"background-color: light-gray\"} -3ex -0.1ex -.4ex 0.5ex .2ex Inorder Traversal The binary-search tree property allows us to print out all the keys in sorted tree by a simple recursive algorithm, called an inorder tree walk, which can be visualized as such: ::: center ::: \\( \\(\\color{ocre2}D \\to B \\to E \\to A \\to F \\to C \\to G\\) \\) How [Inorder-Tree-Walk]{.smallcaps} works: Check to make sure that [ x ]{style=\"background-color: light-gray\"} is not [ nil ]{style=\"background-color: light-gray\"}. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s left sub-tree. Print [ x ]{style=\"background-color: light-gray\"}'s key. Recursively, print the keys of the nodes in the [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. ::: algorithm [Inorder-Tree-Walk]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Querying Binary search tree can support such queries as [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} operations. The [Tree-Search]{.smallcaps} procedure starts at the root and traces a simple path downward in the tree. The running time is \\(\\color{ocre2}\\text{O}(h)\\) , where \\(\\color{ocre2}h\\) is the height of the tree. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: The [Iterative-Tree-Search]{.smallcaps} is more efficient in which works by \\\"unrolling\\\" the recursion into a while loop. ::: algorithm [Tree-Search]{.smallcaps} \\((x,k)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: The binary search-tree property guarantees that: the leftmost node is the minimum key of the binary search tree the rightmost node is the maximum key of the binary search tree Thus, the [Tree-Minimum]{.smallcaps} and [Tree-Maximum]{.smallcaps} procedure traverse the appropriate points until [nil]{.smallcaps} is reached. The running time for both is \\(\\color{ocre2}\\text{O}(h)\\) . ::: algorithm [Tree-Minimum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: ::: algorithm [Tree-Maximum]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: Refer to the diagram below: ::: center ::: Before going over the procedure for successor and predecessor, let's define what it means. Assuming all keys are unique, if \\(\\color{ocre2}x\\) has two children: The successor is the minimum value in its right sub-tree. The predecessor is the maximum value in its left sub-tree. Refer to this example using the key value of \\(\\color{ocre2}25\\) : ::: center ::: If you recall from earlier, when we performed inorder traversal, we can find the successor and predecessor based entirely on the tree structure. ::: algorithm [Tree-Successor]{.smallcaps} \\((x)\\) ::: algorithmic ::: ::: We can break the code for [Tree-Successor]{.smallcaps} into two cases: If [ x.right ]{style=\"background-color: light-gray\"} is non-empty, then [ x ]{style=\"background-color: light-gray\"}'s successor is the minimum in [ x ]{style=\"background-color: light-gray\"}'s right sub-tree. If [ x.right ]{style=\"background-color: light-gray\"} is empty, then go up the tree until the current node is a left child. If you cannot go up further and you reached root, then [ x ]{style=\"background-color: light-gray\"} is the largest element. For example, if we want to find the successor of the key value of \\(\\color{ocre2}20\\) : The right sub-tree is empty, so we go up the tree to the key value of \\(\\color{ocre2}19\\) . Since \\(\\color{ocre2}20\\) is not a left child or located in the left sub-tree of \\(\\color{ocre2}19\\) , go up the tree to the key value of \\(\\color{ocre2}15\\) . Likewise, it is not a left child of \\(\\color{ocre2}15\\) , so go up the tree to the key value of \\(\\color{ocre2}25\\) . The key value of \\(\\color{ocre2}25\\) has \\(\\color{ocre2}20\\) as a left child, therefore, the successor of \\(\\color{ocre2}20\\) is \\(\\color{ocre2}25\\) . Refer to the diagram below: ::: center ::: The [Tree-Predecessor]{.smallcaps} procedure is symmetric to [Tree-Predecessor]{.smallcaps} procedure, which instead uses [ x\u2006.\u2006left ]{style=\"background-color: light-gray\"}. The running time for both is \\(\\color{ocre2}\\text{O}(h)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion The operations of insertion and deletion cause the dynamic set represented by a binary search tree to change. Thus, the binary-search tree property must hold after this change. The [Tree-Insert]{.smallcaps} procedure works quite similar to [Tree-Search]{.smallcaps} and [Iterative-Tree-Search]{.smallcaps}, which begins at the root of the tree. ::: algorithm [Tree-Insert]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: In the code, we are trying to insert [ z ]{style=\"background-color: light-gray\"} to the tree [ T ]{style=\"background-color: light-gray\"}: The pointer [ x ]{style=\"background-color: light-gray\"} traces a simple path downward looking for a [nil]{.smallcaps} to replace with the input [ z ]{style=\"background-color: light-gray\"}. The trailing pointer [ y ]{style=\"background-color: light-gray\"} maintains the parent of [ x ]{style=\"background-color: light-gray\"}. Suppose we want to insert an item with key \\(\\color{ocre2}9\\) . The while loop in lines 3-8 can be expressed as: ::: center ::: The [nil]{.smallcaps} occupies the position where we wish to place the input item [ z ]{style=\"background-color: light-gray\"}. The lines 10-15 set the pointers that cause [ z ]{style=\"background-color: light-gray\"} to be inserted. Deletion is somewhat more tricky than insertion. The process for deleting node [ z ]{style=\"background-color: light-gray\"} can be broken into three cases: Case One: If [ z ]{style=\"background-color: light-gray\"} has no children, then we simply remove it by modifying it's parent to replace [ z ]{style=\"background-color: light-gray\"} with [nil]{.smallcaps}. ::: center ::: Case Two: If [ z ]{style=\"background-color: light-gray\"} has one child, then delete [ z ]{style=\"background-color: light-gray\"} by making the parent of [ z ]{style=\"background-color: light-gray\"} point to [ z ]{style=\"background-color: light-gray\"}'s child, instead of [ z ]{style=\"background-color: light-gray\"}. ::: center ::: Case Three: If [ z ]{style=\"background-color: light-gray\"} has two children, then delete [ z ]{style=\"background-color: light-gray\"}'s successor, [ y ]{style=\"background-color: light-gray\"}, from the tree (via Case One or Case Two) and replace [ z ]{style=\"background-color: light-gray\"}'s key and satellite data with [ y ]{style=\"background-color: light-gray\"}. ::: center ::: The [Tree-Delete]{.smallcaps} procedure executes the three cases as follows. The running time is \\(\\color{ocre2}\\text{O}(h)\\) . ::: algorithm [Tree-Delete]{.smallcaps} \\((T,z)\\) ::: algorithmic ::: ::: As you may have notice, the running time for these operations: [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, [Successor]{.smallcaps}, [Insert]{.smallcaps}, and [Delete]{.smallcaps} all take \\(\\color{ocre2}\\text{O}(h)\\) , where \\(\\color{ocre2}h\\) is the height of the tree. These operations are fast if the height of the tree is small. For binary search trees given we have \\(\\color{ocre2}n\\) items, the minimum height of a binary tree can be \\(\\color{ocre2}\\log{n}\\) and the maximum be \\(\\color{ocre2}n\\) . It can be depicted as such: ::: center ::: Ideally we want make sure the height of the binary tree is always \\(\\color{ocre2}\\log{n}\\) , as it provides the worst-case running time of \\(\\color{ocre2}\\text{O}(\\log{n})\\) , thus comes the motivation for the next topic. -3ex -0.1ex -.4ex 0.5ex .2ex Balanced Search Tree One way we can ensure our tree is always balanced is by implementing a self-balancing binary search tree. A search-tree data structure for which a height of \\(\\color{ocre2}\\log{n}\\) is guaranteed when implementing dynamic set of \\(\\color{ocre2}n\\) items. AVL Tree Red-Black Tree It ensures the \\(\\color{ocre2}\\text{O}(\\log{n})\\) time complexity at all times, by maintaining the binary-search tree property and height-balance property of the tree, whenever insertion or deletion is performed. -4ex -1ex -.4ex 1ex .2ex Red-Black Trees Red-black trees are one of many search-tree schemes that are \"balanced\" in order to guarantee that basic dynamic-set operations take \\(\\color{ocre2}\\text{O}(\\log{n})\\) time in the worst case. ::: center ::: It is a binary tree that satisfies the following red-black properties: Every node is either red or black. The root and leaves ([nil]{.smallcaps}) are black. If a node is red, then both of its children are black. For each node, all simple paths from the node to ([nil]{.smallcaps}) descendant leaves contain the same number of black nodes. To expand more on property 4, let's find the black-height of the key value of \\(\\color{ocre2}7\\) . These are all the simple paths that can be taken indicated by the grey dashed arrow above: 7, 3, [nil]{.smallcaps} 7, 18, 10, 8, [nil]{.smallcaps} 7, 18, 10, 11, [nil]{.smallcaps} 7, 18, 22, 26, [nil]{.smallcaps} If we don't include the root node, notice how all simple paths consists of the (same number of) \\(\\color{ocre2}2\\) black nodes. Likewise, the same can be said for every node in the tree. The red-black tree is a BST, so we can implement the dynamic-set operations [Search]{.smallcaps}, [Minimum]{.smallcaps}, [Maximum]{.smallcaps}, [Predecessor]{.smallcaps}, and [Successor]{.smallcaps} in \\(\\color{ocre2}\\text{O}(\\log{n})\\) time. -3ex -0.1ex -.4ex 0.5ex .2ex Recoloring and Rotation However, it does not directly support the dynamic-set operations [Insert]{.smallcaps} and [Delete]{.smallcaps}. Because they modify the tree, the result may violate the red-black properties. We must change color of some nodes via recoloring Restructure the links of the tree via rotation For starters, let's go over the relationship in a binary tree: ::: center ::: There's two types of procedures called [Left-Rotate]{.smallcaps} and [Right-Rotate]{.smallcaps}. ::: center ::: The letters \\(\\color{ocre2}\\alpha\\) , \\(\\color{ocre2}\\beta\\) , and \\(\\color{ocre2}\\gamma\\) represent an arbitrary sub-treeall of them have the same black-height. First, determine if recoloring needs to be done. Case One: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is red. Proceed with recoloring. Push \\(\\color{ocre2}C\\) 's black onto \\(\\color{ocre2}A\\) and \\(\\color{ocre2}D\\) . Recurse and check for \\(\\color{ocre2}C\\) 's uncle if it exists. ::: center ::: Case Two: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LR or RL imbalance. ::: center ::: There are four restructuring configurations depending on whether the double red nodes ( \\(\\color{ocre2}A\\) and \\(\\color{ocre2}B\\) ) are left or right children. ::: center ::: If there's a LR imbalance, perform [Left-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: If there's a RL imbalance, perform [Right-Rotate]{.smallcaps} on middle node. Proceed to Case Three. ::: center ::: Case Three: If [ x ]{style=\"background-color: light-gray\"}'s uncle, [ y ]{style=\"background-color: light-gray\"}, is black and there's a LL or RR imbalance. Preserve the color If there's a LL imbalance, perform [Right-Rotate]{.smallcaps} on top node. ::: center ::: If there's a RR imbalance, perform [Left-Rotate]{.smallcaps} on top node. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Insertion and Deletion To perform insertion, we insert [ x ]{style=\"background-color: light-gray\"} and color it red. The motivation for using the color red is that only property 2 and 3 might be brokenthese violation are fairly easy to fix. The [RB-Insert]{.smallcaps} procedure performs the following three cases describe in the previous section. The running time is \\(\\color{ocre2}\\text{O}(\\log{n})\\) . Suppose we want to insert \\(\\color{ocre2}15\\) using the red-black tree shown in the beginning, then we would insert as we normally would in a BST and color it red. Refer to the diagram below: ::: center ::: Just like deleting a node in a BST, it's just as complicated to delete a node in a red-black tree. The process for deleting node can be broken into three cases: Case One: If the deleted node is red, perform the deletion as you would in BST. No color changes should occur. ::: center ::: Case Two: If the deleted node is black and has one red child. Reattach the red child in place of the black node we removed, then recolor the red node as black to fix black-height of the tree. ::: center ::: Case Three: If the deleted node is black. Reattach a black child in place of the black node we removed, then recolor as a double black. ::: center ::: The double black is to keep track of where we violated the black depth property. Denoted as [ r ]{style=\"background-color: light-gray\"} and the sibling of [ r ]{style=\"background-color: light-gray\"} as [ y ]{style=\"background-color: light-gray\"}, we'll divide this into three sub-cases based on [ y ]{style=\"background-color: light-gray\"}: ::: list The color of [ x ]{style=\"background-color: light-gray\"}, parent of [ z ]{style=\"background-color: light-gray\"}, displayed can be black or red. These three sub-cases differ only on the color of [ y ]{style=\"background-color: light-gray\"}, sibling of [ r ]{style=\"background-color: light-gray\"}. ::: Case Three (a): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and has a red child [ z ]{style=\"background-color: light-gray\"}. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a left child, perform [Right-Rotate]{.smallcaps} on [ y ]{style=\"background-color: light-gray\"}, then proceed below. ::: center ::: If [ z ]{style=\"background-color: light-gray\"} is a right child, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ x ]{style=\"background-color: light-gray\"} and [ z ]{style=\"background-color: light-gray\"} black, give [ y ]{style=\"background-color: light-gray\"} the former color of [ x ]{style=\"background-color: light-gray\"}, and color [ r ]{style=\"background-color: light-gray\"} black. ::: center ::: As you can see, we managed to achieve the same configuration as the original tree prior to the deletion of node. We basically converted the red node to be a black node, thus maintaining the red-black tree property. Case Three (b): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is black and both children of [ y ]{style=\"background-color: light-gray\"} are black. ::: center ::: If [ x ]{style=\"background-color: light-gray\"} is red, we color it black, then we color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: Otherwise, we only color [ r ]{style=\"background-color: light-gray\"} black and [ y ]{style=\"background-color: light-gray\"} red. ::: center ::: In this case, we are essentially removing one black-height from the other sub-tree, to deal with the double black. Case Three (c): The sibling [ y ]{style=\"background-color: light-gray\"} of [ r ]{style=\"background-color: light-gray\"} is red. Perform an adjustment operation. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the right child of [ x ]{style=\"background-color: light-gray\"}, perform [Left-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: If [ y ]{style=\"background-color: light-gray\"} is the left child of [ x ]{style=\"background-color: light-gray\"}, perform [Right-Rotate]{.smallcaps} on [ x ]{style=\"background-color: light-gray\"}. We color [ y ]{style=\"background-color: light-gray\"} black and [ x ]{style=\"background-color: light-gray\"} red. ::: center ::: The sibling of [ r ]{style=\"background-color: light-gray\"} should be black now, thus solve using Case Three (a) or Case Three (b). ::: list In Case Three (a) and Case Three (b), if [ r ]{style=\"background-color: light-gray\"} is in the right-side instead of the left-side, the direction of rotation changese.g. [Right-Rotate]{.smallcaps} instead of [Left-Rotate]{.smallcaps} and vice-versa, as we have done in Case Three (c). ::: -3ex -0.1ex -.4ex 0.5ex .2ex Comparing AVL and Red-Black Trees Since both provide dynamic-set operations in \\(\\color{ocre2}\\text{O}(\\log{n})\\) time, which one to choose? AVL trees provide faster lookups than Red Black Trees because they are more strictly balanced. Red-Black Trees provide faster insertion and removal operations than AVL trees as fewer rotations are done due to relatively relaxed balancing. AVL trees store balance factors or heights with each node, thus requires storage for an integer per node whereas Red Black Tree requires only 1 bit of information per node. Red-Black Trees are used in most of the language libraries like map, multi-map, multi-set in C++ whereas AVL trees are used in databases where faster retrievals are required.","title":"Trees"},{"location":"W2022/COE428/COE428/#graph","text":"-4ex -1ex -.4ex 1ex .2ex Properties of a Graph A graph should consists of the following: Vertices (nodes), which specify some entities we are interested in. Edges (lines), which specify the relationship between entities. Weights (number in lines), which specify the weight the edge represent. The formal definition of a graph is a pair \\(\\color{ocre2}(V,E)\\) where: \\(\\color{ocre2}V\\) is a collection of nodes, called vertices. \\(\\color{ocre2}E\\) is a collection of pairs of vertices, called edges. ::: exampleT Example 8.1 . We can represent the following graph using the given vertices and edges: \\(\\color{ocre2}V = \\{a,b,c,d,e,f\\}\\) \\(\\color{ocre2}E = \\{(a,c),(b,c),(c,f),(b,d),(d,f),(c,d)\\}\\) ::: center ::: ::: A graph can be categorized into one of two types, depending on the edge type: Undirected Graph. Edges do not have a directionundirected edge are unordered pair of vertices, such that \\(\\color{ocre2}(u,v)\\) and \\(\\color{ocre2}(v,u)\\) are the same edge. ::: center ::: Directed Graph. Edges with directiondirected edges are ordered pair of vertices, such that \\(\\color{ocre2}\\langle u,v \\rangle\\) and \\(\\color{ocre2}\\langle v,u \\rangle\\) are two different edges. ::: center ::: To distinguish between the two edge types, we use round brackets \\(\\color{ocre2}( )\\) for unordered pairs and angle brackets \\(\\color{ocre2}\\langle \\rangle\\) for ordered pairs. -3ex -0.1ex -.4ex 0.5ex .2ex Graph Terminology We will go over a few graph terminologies, some of which you should be familiar with. The degree of a vertex is the number of incident edges of this vertex. Below are some examples. Pay close attention to the degree of vertex \\(\\color{ocre2}z\\) . ::: center ::: Let \\(\\color{ocre2}m\\) be the number of edges and \\(\\color{ocre2}\\deg(a)\\) be the degree of vertex \\(\\color{ocre2}a\\) , then \\( \\(\\color{ocre2}\\sum_{a \\in V}\\deg(a) = 2m\\) \\) For undirected graphs, parallel edges are edges that have the same endpoints, whereas for directed graph, they are edges that have the same origin and destination. ::: center ::: Self-loop is an edge whose endpoints coincide, such as the edge \\(\\color{ocre2}(z,z)\\) ::: center ::: In this course, we will deal almost exclusively with simple graphs, which are graphs that do not have a parallel edge or self-loop. Let \\(\\color{ocre2}n\\) be the number of vertices and \\(\\color{ocre2}m\\) the number of edges, then \\( \\(\\color{ocre2}m \\leq \\frac{n(n - 1)}{2}\\) \\) There are various definitions used to describe the movement in a graph: A path is a sequence of vertices, such that consecutive vertices are adjacent. A simple path is path such that all its vertices are distinct. ::: center ::: A cycle is a path on which the first vertex is equal to the last vertex. A simple cycle is a cycle such that all its vertices are distinct, except the first and last one. ::: center ::: Lastly, we'll cover the characteristics of a connected graph and the definition of a subgraph. A connected graph is a graph in which there is a path from any vertex to any other vertex in the graph. ::: center ::: We can say a tree is a connected graph without a cycleany two vertices are connected by exactly one path. ::: center ::: A subgraph of a graph \\(\\color{ocre2}(V,E)\\) is a pair \\(\\color{ocre2}(V', E')\\) where \\(\\color{ocre2}V' \\subseteq V\\) and \\(\\color{ocre2}E' \\subseteq E\\) . Both endpoints of edges in \\(\\color{ocre2}E'\\) are in \\(\\color{ocre2}V'\\) . ::: center ::: Then a spanning tree is a subgraph of a connected graph, which includes all vertices of the connected graph. ::: center ::: -4ex -1ex -.4ex 1ex .2ex Representations of Graphs We can choose between two standard ways to represent a graph \\(\\color{ocre2}G = (V,E)\\) , as a collection of: Adjacency list Adjacency matrix Either way applies to both directed and undirected graph. They are useful in representing dense and sparse graphs: Sparse graphs. A graph with only a few edge. Dense graphs. The number of edges is close to the maximal number of edges. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency List The adjacency-list representation of a graph consists of an array \\(\\color{ocre2}\\textit{Adj}\\) of \\(\\color{ocre2}|V|\\) list, one for each vertex in \\(\\color{ocre2}V\\) . For an undirected graph, the adjacency list \\(\\color{ocre2}\\textit{Adj}[u]\\) contains all the vertices \\(\\color{ocre2}v\\) such that there is an edge \\(\\color{ocre2}(u,v), (v,u) \\in E\\) . ::: center ::: Alternatively, you can think of it as a list of all the vertices adjacent to \\(\\color{ocre2}u\\) . ::: center ::: <!-- --> - For a directed graph, the adjacency list \\(\\color{ocre2}\\textit{Adj}[u]\\) contains all the vertices \\(\\color{ocre2}v\\) such that there is an edge \\(\\color{ocre2}\\langle u,v \\rangle \\in E\\) . ::: center ![image](Figure/8/Adjacency/adjacency_list2.png){height=\"3cm\"} ::: Alternatively, you can think of it as a list of destinations given the origin $\\color{ocre2}u$. ::: center ![image](Figure/8/Adjacency/adjacency_list2a.png){height=\"2.5cm\"} ::: ::: list Note that in an adjacency list, the order doesn't matter, meaning we could have listed the vertex in any order. ::: A useful thing we could do with adjacency list is to represent weighted graphsedges with an associated weight to them. It can easily be done by storing it with vertex \\(\\color{ocre2}v\\) in \\(\\color{ocre2}u\\) 's adjacency list. ::: center ::: A potential disadvantage of the adjacency-list representation is that there is no quicker way to determine if a given edge \\(\\color{ocre2}(u,v)\\) is present in the graph. We would need to search for \\(\\color{ocre2}v\\) in the adjacency list \\(\\color{ocre2}\\textit{Adj}[u]\\) . If we want to check the edge \\(\\color{ocre2}(2,4)\\) , then we would need search through \\(\\color{ocre2}\\textit{Adj}[2]\\) . ::: center ::: The worst-case running time would be the number of adjacent vertices, which is not ideal. A solution would be to use an adjacency-matrix representation, which requires a constant time \\(\\color{ocre2}\\text{O}(1)\\) , but at the cost of using asymptotically more memory. -3ex -0.1ex -.4ex 0.5ex .2ex Adjacency Matrix The adjacency-matrix representation of a graph consists of a \\(\\color{ocre2}|V| \\times |V|\\) matrixassuming the vertices are numbered from \\(\\color{ocre2}1\\) , \\(\\color{ocre2}2\\) , ..., to \\(\\color{ocre2}|V|\\) . We can represent the elements inside the matrix \\(\\color{ocre2}A\\) as \\(\\color{ocre2}a_{ij}\\) , where \\(\\color{ocre2}i\\) and \\(\\color{ocre2}j\\) indicate the row and column. For an undirected graph, if there is an edge \\(\\color{ocre2}(i,j), (j,i) \\in E\\) , then set \\(\\color{ocre2}a_{ij} = 1\\) , otherwise, \\(\\color{ocre2}a_{ij} = 0\\) . ::: center ::: Notice how \\(\\color{ocre2}a_{22}\\) is \\(\\color{ocre2}0\\) , since the edge \\(\\color{ocre2}(2,2)\\) does not exist. ::: center ::: For a directed graph, if there is an edge \\(\\color{ocre2}\\langle i, j \\rangle \\in E\\) , then set \\(\\color{ocre2}a_{ij} = 1\\) , otherwise, \\(\\color{ocre2}a_{ij} = 0\\) . ::: center ::: Like the adjacency-list representation of a graph, an adjacency matrix can represent a weighted graph. Instead of storing \\(\\color{ocre2}0\\) 's and \\(\\color{ocre2}1\\) 's, we store the weight of the given edge. ::: center ::: If an edge does not exist, we can store a [nil]{.smallcaps} value, depicted as empty in the diagram above. -3ex -0.1ex -.4ex 0.5ex .2ex Comparison As we have demonstrated both are applicable to undirected and directed graphs, each with their own advantages and disadvantages. **Adjacency List** **Adjacency Matrix** **Space:** $\\color{ocre2}\\Theta(|V + E|)$ $\\color{ocre2}\\Theta(|V|^2)$ Time: List all vertices adjacent to \\(\\color{ocre2}u\\) \\(\\color{ocre2}\\Theta(\\deg(u))\\) \\(\\color{ocre2}\\Theta(|V|)\\) Time: Determine if \\(\\color{ocre2}(u,v) \\in E\\) \\(\\color{ocre2}\\Theta(\\deg(u))\\) \\(\\color{ocre2}\\Theta(1)\\) The choice of which one to use comes down to the following criteria: The adjacency-list representation provides a compact way to represent sparse graphsthose for which \\(\\color{ocre2}|E|\\) is much less than \\(\\color{ocre2}|V|^2\\) . However, if \\(\\color{ocre2}|E|\\) is close to \\(\\color{ocre2}|V|^2\\) , then we may choose an adjacency-matrix representation since it almost have the same space complexity as the adjacency-list. Alternatively, if we need to be able to tell quickly if there is an edge connecting two given vertices, an adjacency-matrix representation is used. -4ex -1ex -.4ex 1ex .2ex Graph Traversals A traversal (or graph searching) is a systematic procedure for exploring a connected graph by examining all its vertices and/or edges. There's two types of traversal algorithms: Breadth-First Search (BFS) Depth-First Search (DFS) -3ex -0.1ex -.4ex 0.5ex .2ex Breadth-First Search Breadth-first search (BFS) is one of the simplest algorithms for searching a graph, which uses a queue data structure. For simplicity, we will use a tree to describe breadth-first search: Let's start at the root of tree. Let's add \\(\\color{ocre2}A\\) to the queue. ::: center ::: We want to explore all the vertices that are adjacent to \\(\\color{ocre2}A\\) , which are \\(\\color{ocre2}B\\) and \\(\\color{ocre2}C\\) . We will add them to the queue. ::: center ::: Note that the order they are placed in queue does not matter. We could have stored \\(\\color{ocre2}C\\) first. Since we finished \\\"exploring\\\" \\(\\color{ocre2}A\\) , we will move on, then \\(\\color{ocre2}B\\) is next in queue. ::: center ::: Likewise, we add all vertices adjacent to \\(\\color{ocre2}B\\) , which are \\(\\color{ocre2}D\\) and \\(\\color{ocre2}E\\) , to the queue. ::: center ::: Since we finished \\\"exploring\\\" \\(\\color{ocre2}B\\) , we will move to \\(\\color{ocre2}C\\) . ::: center ::: We will add the vertices adjacent to \\(\\color{ocre2}C\\) . which are \\(\\color{ocre2}F\\) and \\(\\color{ocre2}G\\) to the queue. ::: center ::: When we move to vertex \\(\\color{ocre2}D\\) , you will see there are no adjacent vertices, thus we don't add anything to the queue and move on to \\(\\color{ocre2}E\\) . ::: center ::: The same can be said for \\(\\color{ocre2}E\\) through \\(\\color{ocre2}G\\) , in which we finish our breadth-first search. ::: center ::: This is a simplified explanation, but should provide a general idea of how it works. We associate the vertex colors to guide the algorithm: White vertices have not been discovered. All vertices start out white. Grey vertices are discovered but not fully explored. Black vertices are discovered and fully explored. The algorithm attaches several attributes to each vertex, such as color [ u.color ]{style=\"background-color: light-gray\"}, parent [ u.\\pi ]{style=\"background-color: light-gray\"}, and distance [ u.d ]{style=\"background-color: light-gray\"} which is computed by the algorithm. To keep track of progress, breadth-first search colors each vertex white, grey, or black. Distance is used to represent the smallest number of edges that must be traverse from the starting vertex [ s ]{style=\"background-color: light-gray\"} to end vertex [ v ]{style=\"background-color: light-gray\"}. ::: algorithm [BFS]{.smallcaps} \\((G,s)\\) ::: algorithmic ::: ::: We start off by painting every vertex [ white ]{style=\"background-color: light-gray\"} except our starting vertex [ s ]{style=\"background-color: light-gray\"} and setting the distance to [ \\infty ]{style=\"background-color: light-gray\"}, as we not sure how far it is from the starting vertex or whether it is even reachable. ::: algorithm ::: algorithmic ::: ::: Then we paint our starting vertex [ s ]{style=\"background-color: light-gray\"} to [ grey ]{style=\"background-color: light-gray\"}, set the distance to [ 0 ]{style=\"background-color: light-gray\"}, since it's our starting point. The parent of [ s ]{style=\"background-color: light-gray\"} is [ nil ]{style=\"background-color: light-gray\"}doesn't exist. ::: center ::: Note that vertex [ s ]{style=\"background-color: light-gray\"} is something that we chose, which in this example is [ 5 ]{style=\"background-color: light-gray\"}. ::: algorithm ::: algorithmic ::: ::: Then we initialize [ Q ]{style=\"background-color: light-gray\"} to the queue containing just the vertex [ s ]{style=\"background-color: light-gray\"} which is [ 5 ]{style=\"background-color: light-gray\"}. ::: center ::: ::: algorithm ::: algorithmic ::: ::: The [ while ]{style=\"background-color: light-gray\"} loop functions similarly to what we have demonstrated in the first example. ::: center ::: The total running time of the [BFS]{.smallcaps} procedure is \\(\\color{ocre2}\\text{O}(V + E)\\) . As you may have notice, it is particularly useful for finding the shortest path from the starting vertex [ s ]{style=\"background-color: light-gray\"} to some vertex [ v ]{style=\"background-color: light-gray\"} in the graph. -3ex -0.1ex -.4ex 0.5ex .2ex Depth-First Search Depth-first search (DFS) as the name implies, searches \\\"deeper\\\" first until it cannot go further at which point it backtracks and continues, which uses a stack data structure. Let's use the same tree as we have used for BFS, to compare the difference: As before, we will start at the root of three. Let's add \\(\\color{ocre2}A\\) to the stack. ::: center ::: Then we arbitrarily pick an edge outwards of \\(\\color{ocre2}A\\) , which will choose \\(\\color{ocre2}B\\) and add to the stack. ::: center ::: Note there's multiple ways, so we could have also chosen to go with vertex \\(\\color{ocre2}C\\) instead. Continue to pick an edge outwards, which there is only one, so will choose \\(\\color{ocre2}D\\) . ::: center ::: Since there's no more vertices to explore, we backtrack to vertex \\(\\color{ocre2}B\\) . ::: center ::: Continue to pick an edge outwards that has not been visited yet, which is vertex \\(\\color{ocre2}E\\) . ::: center ::: Then we backtrack all the way to vertex \\(\\color{ocre2}A\\) , since all of vertex \\(\\color{ocre2}B\\) and \\(\\color{ocre2}E\\) has been explored. ::: center ::: Then, we repeat the same steps for the right sub-tree, by picking some arbitrary edge outwards until we have fully discovered every vertex. DFS uses the same color scheme as we previously described in BFS. However, one unique thing about the algorithm is the it uses two timestamps for: when it first discovers the vertex [ u.d ]{style=\"background-color: light-gray\"} and ... when it finishes exploring the vertex [ u.f ]{style=\"background-color: light-gray\"}. This is similar to BFS, which we paint every vertex [ white ]{style=\"background-color: light-gray\"}. The [ time ]{style=\"background-color: light-gray\"} is set to [ 0 ]{style=\"background-color: light-gray\"}, which will use to compute the discovery time [ u.d ]{style=\"background-color: light-gray\"} and finishing time [ u.f ]{style=\"background-color: light-gray\"}. ::: algorithm [DFS]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: For this example, we'll use a directed graph as it's \\\"easier\\\" to pick an edge outwards and demonstrate. ::: center ::: For demonstration purposes, the discovery time and finishing time is denoted below the vertex, shown on the right. ::: algorithm [DFS-Visit]{.smallcaps} \\((G,u)\\) ::: algorithmic ::: ::: We'll choose vertex [ 8 ]{style=\"background-color: light-gray\"} as our starting vertex. It'll recursively call [ DFS-Visit ]{style=\"background-color: light-gray\"} until it reaches a dead end then it colors the vertex black. ::: center ::: Once the recursion finishes, it goes to the next [ v ]{style=\"background-color: light-gray\"} that is in [ Adj[u] ]{style=\"background-color: light-gray\"} and repeats the same thing. ::: center ::: Since there's no more [ v ]{style=\"background-color: light-gray\"} (or vertex to explore) in [ Adj[u] ]{style=\"background-color: light-gray\"}. We pick a new [ u ]{style=\"background-color: light-gray\"}. ::: center ::: The total running time of the [DFS]{.smallcaps} procedure is \\(\\color{ocre2}\\text{O}(V + E)\\) , similar to [BFS]{.smallcaps} procedure. It provides valuable information about the structure of a graph and is more suitable for decision treelike a maze. We can use depth-first search to perform a topological sort of a directed acyclic graph (dag)a directed graph with no cycles. ::: center ::: By performing topological sort, we can find the topological orderingordering of its vertices along a horizontal line so that all directed edges go from left to right. ::: center ::: Note how the order they are listed in regarding their finishing times. The [Topological-Sort]{.smallcaps} sorts a dag by: ::: algorithm [Topological-Sort]{.smallcaps} \\((G)\\) ::: algorithmic ::: ::: We can perform [Topological-Sort]{.smallcaps} in \\(\\color{ocre2}\\text{O}(V + E)\\) time, since DFS takes \\(\\color{ocre2}\\text{O}(V + E)\\) .","title":"Graph"},{"location":"W2022/COE428/COE428/#elementary-graph-algorithms","text":"-4ex -1ex -.4ex 1ex .2ex Minimum Spanning Tree The minimum spanning tree is the spanning tree of a weighted graph with minimum total edge weight. What this means, is suppose we have a connected graph with weighted edges. ::: center ::: We are interested in the subset of the edges which connects all vertices together, while minimizing the total edge cost. We can form three variations of the graph's spanning tree: ::: center ::: As you can see, tree 1 in particular is our minimum spanning tree because it posses the minimum total edge weight of \\(\\color{ocre2}71\\) . Obviously when there's more vertices and edges, it will be much harder to figure out the MST. And so we'll go over two types of algorithms which does the following: Kruskal's algorithm Prim's algorithm For both algorithm, we will try and find the minimum spanning tree of the following completed graph below and compare them after. ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Kruskal's Algorithm This algorithm creates a forest of trees. It works by picking the smallest edge then checking if it forms a cycle with the spanning tree formed so far. If not, the edge is added, otherwise, discard it. We start off by selecting the smallest edge in the graph, which is \\(\\color{ocre2}(6,1)\\) , as it has an edge weight of \\(\\color{ocre2}10\\) . ::: center ::: Then, we select the next smallest edge, which is \\(\\color{ocre2}(4,3)\\) , with an edge weight of \\(\\color{ocre2}12\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(7,2)\\) , with an edge weight of \\(\\color{ocre2}14\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(2,3)\\) , with an edge weight of \\(\\color{ocre2}16\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(7,4)\\) , with an edge weight of \\(\\color{ocre2}18\\) . However, notice that it forms a cycle, so we discard it. ::: center ::: Instead, we go to the next minimum edge, \\(\\color{ocre2}(5,4)\\) , with an edge weight of \\(\\color{ocre2}22\\) . ::: center ::: The next minimum edge is \\(\\color{ocre2}(5,6)\\) , with an edge weight of \\(\\color{ocre2}25\\) . ::: center ::: Once we have all our vertices or have \\(\\color{ocre2}|V| - 1\\) edges, we are done with the algorithm. Our MST is complete, which has the weight of \\(\\color{ocre2}99\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Prim's Algorithm This algorithm starts with one node. It then adds a node one by one that is unconnected to the new graph. For this example, let's start at vertex \\(\\color{ocre2}6\\) . Since we started with \\(\\color{ocre2}6\\) , we have to select the smallest edge that is connected to \\(\\color{ocre2}6\\) , which are either: \\(\\color{ocre2}25\\) or \\(\\color{ocre2}10\\) . ::: center ::: We'll select the edge weight of \\(\\color{ocre2}10\\) , since that's the smallest. Likewise, we now have to select the smallest edge that is connected to either \\(\\color{ocre2}6\\) or \\(\\color{ocre2}1\\) , which will pick edge \\(\\color{ocre2}(6,5)\\) , with an edge weight of \\(\\color{ocre2}25\\) . ::: center ::: We are basically repeating the same step, but now with more and more edges to pick from. The dashed lines indicate the possible edges to select. ::: center ::: While it was not demonstrated in this example, but if an edge is selected and it forms a cycle, we will discard it and choose the next minimum edge, similar to Kruskal's algorithm. Once we have all our vertices or have \\(\\color{ocre2}|V| - 1\\) edges, we are done with the algorithm, which also have a weight of \\(\\color{ocre2}99\\) . As you can see, we have obtained the same MST for both Kruskal and Prim's algorithm, so which one to choose? Kruskal's algorithm runs faster in sparse graph, with the time complexity of \\(\\color{ocre2}\\text{O}(E\\log{V})\\) . Prim's algorithm runs faster in dense graph, with the time complexity of \\(\\color{ocre2}\\text{O}(E\\log{V})\\) , but it can be improved up to \\(\\color{ocre2}\\text{O}(E + V\\log{V})\\) using Fibonacci heaps. -4ex -1ex -.4ex 1ex .2ex Shortest Path We define the shortest path as the minimum length path from a vertex to another vertex in \\(\\color{ocre2}G\\) , if such a path exists. Previously, we have done something similar with breadth-first search with an unweighted graph, in which each edge has a weight of \\(\\color{ocre2}1\\) . ::: center ::: We are particularly interested in the single-source shortest path's problem, that is given a graph \\(\\color{ocre2}G = (V,E)\\) , what is the shortest path from a given source vertex \\(\\color{ocre2}s \\in V\\) to each vertex \\(\\color{ocre2}v \\in V\\) . ::: center ::: Suppose we chose \\(\\color{ocre2}A\\) as our source vertex, then the shortest-paths tree are: ::: center ::: If you calculate the weight from the source vertex \\(\\color{ocre2}A\\) to every other vertex, you will get the minimum edge weight possible. From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}B\\) , the total edge weight is \\(\\color{ocre2}3\\) . From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}C\\) , the total edge weight is \\(\\color{ocre2}5\\) . From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}D\\) , the total edge weight is \\(\\color{ocre2}9\\) . From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}E\\) , the total edge weight is \\(\\color{ocre2}11\\) . You also may have notice that the shortest-paths tree are not unique, meaning there can also be more than one shortest-path tree, nonetheless should still provide the same answers. In this section, we'll go over two algorithms which does the following: Dijkstra's algorithm Bellman-Ford's algorithm As will demonstrate, each algorithm have their advantages and disadvantages when choosing one over the other. Both algorithm uses the [Relax]{.smallcaps} procedure, but implement them in varying ways. ::: algorithm [Relax]{.smallcaps} \\((u,v,w)\\) ::: algorithmic ::: ::: The process of \\\"relaxing\\\" an edge is to check if its worth going through the edge \\(\\color{ocre2}\\langle u,v \\rangle\\) which would improve the shortest path from source vertex to some vertex [ v ]{style=\"background-color: light-gray\"}. Dijkstra's Algorithm relax each edge exactly once. Bellman-Ford's Algorithm relaxes each edge \\(\\color{ocre2}|V| - 1\\) times. For example, assume we have the obtained the following weighted, directed graph and we wanna find the shortest path to vertex \\(\\color{ocre2}D\\) . Suppose we start by relaxing the edge \\(\\color{ocre2}\\langle C,D \\rangle\\) . ::: center ::: Every vertex initially start with a distance of \\(\\color{ocre2}\\infty\\) and since \\(\\color{ocre2}\\infty > 7 + 6 = 13\\) , we used the edge \\(\\color{ocre2}\\langle C,D \\rangle\\) . But now let's relax the other edge, \\(\\color{ocre2}\\langle B,D \\rangle\\) . ::: center ::: Since \\(\\color{ocre2}13 > 6 + 4 = 10\\) , we now use the edge \\(\\color{ocre2}\\langle B,D \\rangle\\) instead of \\(\\color{ocre2}\\langle C,D \\rangle\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Dijkstra's Algorithm As we shall see, Dijkstra's algorithm is pretty similar to Prim's algorithm, which we covered in Minimum Spanning Tree. One limitation of this algorithm is that all edge weights must be non-negative. Let's pick \\(\\color{ocre2}A\\) as our source vertex. There's a table in the right-side, which will use to keep track of distances from the source vertex to each vertex. ::: center ::: We put infinity for the other vertices as we haven't visited them yet. A change in the distance will be indicated by the grey boxes in each step. Next, we examine the edges leaving \\(\\color{ocre2}A\\) . As denoted in the table, we can reach \\(\\color{ocre2}B\\) and \\(\\color{ocre2}C\\) with an edge weight of \\(\\color{ocre2}10\\) and \\(\\color{ocre2}3\\) respectively. ::: center ::: We always pick the smallest edge weight of which the vertex hasn't been discovered. In this case, it's \\(\\color{ocre2}\\langle A,C \\rangle\\) . ::: center ::: Notice how we don't include the edge \\(\\color{ocre2}\\langle A,B \\rangle\\) in consideration as \\(\\color{ocre2}B\\) is now reachable from \\(\\color{ocre2}\\langle A,C \\rangle\\) and \\(\\color{ocre2}\\langle C,B \\rangle\\) with a smaller total edge weight of \\(\\color{ocre2}7\\) . Now \\(\\color{ocre2}D\\) and \\(\\color{ocre2}E\\) are now reachable. As usual, we'll pick the next smallest edge, which is \\(\\color{ocre2}\\langle C, E \\rangle\\) . ::: center ::: Similar to what was described before, we don't consider the edge \\(\\color{ocre2}\\langle E, D \\rangle\\) , since \\(\\color{ocre2}D\\) has a shortest path using the edge \\(\\color{ocre2}\\langle C, D \\rangle\\) instead of \\(\\color{ocre2}\\langle E, D \\rangle\\) . Our only options left are \\(\\color{ocre2}\\langle C,B \\rangle\\) and \\(\\color{ocre2}\\langle C,D \\rangle\\) . We pick \\(\\color{ocre2}\\langle C, B \\rangle\\) , as it has the smallest edge weight of \\(\\color{ocre2}7\\) . ::: center ::: From \\(\\color{ocre2}A\\) to \\(\\color{ocre2}D\\) , the edge \\(\\color{ocre2}\\langle C, D \\rangle\\) will results in a distance of \\(\\color{ocre2}11\\) , while \\(\\color{ocre2}\\langle B,D \\rangle\\) will result in a distance of \\(\\color{ocre2}9\\) . So all that's left is to pick the last remaining edge, which is \\(\\color{ocre2}\\langle B, D \\rangle\\) . ::: center ::: All the edges have been discovered, so we are done with the algorithm. -3ex -0.1ex -.4ex 0.5ex .2ex Bellman-Ford's Algorithm Bellman-Ford's algorithm is more general than Dijkstra's algorithm, such that it can deal with negative edge weights. However, it is a bit more time consuming in comparison. As usual, let start with vertex \\(\\color{ocre2}A\\) as our source vertex and the distances to each vertex listed in the right. ::: center ::: With Bellman-Ford's algorithm, we want to relax all the edges. In other words, we should test out all the possible edges that will result in the shortest path. So this will be our first iteration. Starting at \\(\\color{ocre2}A\\) , we can reach \\(\\color{ocre2}B\\) and \\(\\color{ocre2}C\\) with a weight of \\(\\color{ocre2}10\\) and \\(\\color{ocre2}3\\) , using the edges, \\(\\color{ocre2}\\langle A,B \\rangle\\) and \\(\\color{ocre2}\\langle A,C \\rangle\\) respectively. ::: center ::: From \\(\\color{ocre2}B\\) , we can reach \\(\\color{ocre2}D\\) at a total weight of \\(\\color{ocre2}12\\) using the edge \\(\\color{ocre2}\\langle B,D \\rangle\\) . Note that we won't use the edge \\(\\color{ocre2}\\langle B,C \\rangle\\) , as it result in a longer path from \\(\\color{ocre2}A\\) to \\(\\color{ocre2}C\\) . ::: center ::: From \\(\\color{ocre2}C\\) , we can reach \\(\\color{ocre2}E\\) at a total weight of \\(\\color{ocre2}5\\) using the edge \\(\\color{ocre2}\\langle C,E \\rangle\\) . ::: center ::: Also, notice if we use the edge \\(\\color{ocre2}\\langle C,B \\rangle\\) , we will get a shorter path to \\(\\color{ocre2}B\\) with a total weight of \\(\\color{ocre2}7\\) . Consequently, it also lowers the total weight of \\(\\color{ocre2}D\\) to \\(\\color{ocre2}9\\) . ::: center ::: Since we found a better path, we'll remove the edge \\(\\color{ocre2}\\langle A,B \\rangle\\) . From \\(\\color{ocre2}D\\) , we only have one edge to work with, which is \\(\\color{ocre2}\\langle D,E \\rangle\\) , however, notice how this increases the total weight of \\(\\color{ocre2}E\\) from \\(\\color{ocre2}5\\) to \\(\\color{ocre2}16\\) , so, we don't use it. ::: center ::: Similarly for \\(\\color{ocre2}E\\) , we have the edge \\(\\color{ocre2}\\langle E,D \\rangle\\) , however, this also increase the weight of \\(\\color{ocre2}D\\) by \\(\\color{ocre2}9\\) to \\(\\color{ocre2}12\\) , so we don't use it. ::: center ::: Since we have checked all vertex. we're done with our first iteration. The algorithm at most takes \\(\\color{ocre2}|V| - 1\\) iterations to fully obtain the shortest-tree path. However, it can sometimes be less if there's no changes occurring after the next iteration. In our second iteration, we proceed to the do the following, but using the graph we got from our first iteration. ::: center ::: Our only option is \\(\\color{ocre2}\\langle A,B \\rangle\\) , however, this doesn't improve the distances of \\(\\color{ocre2}B\\) , so we don't update the graph. Same thing can be said for \\(\\color{ocre2}B\\) , \\(\\color{ocre2}C\\) , \\(\\color{ocre2}D\\) and \\(\\color{ocre2}E\\) . So after our second iteration, we are finished. ::: list In the actual exam, they might provide you the edges in which you should relax them by order. The process will still be the same as described from above, but the order may be different, as they might ask you to relax the edges of \\(\\color{ocre2}E\\) before \\(\\color{ocre2}D\\) or etc. ::: One useful property of Bellman-Ford algorithm is that we can also use it to check for the existence of negative cycleone in which the overall sum of the cycle becomes negative. ::: center ::: If you add the weights of its edges, it's negative ( \\(\\color{ocre2}-6 + 3 + 2 = -1\\) ). The concept of a shortest path is meaningless if there is a negative cycle, as we'll have a continuous loop. Refer to the example below. To demonstrate this, let's use the following weighted, directed graph and set \\(\\color{ocre2}A\\) as the source vertex. As you can see, we will loop through the cycle \\(\\color{ocre2}B\\) , \\(\\color{ocre2}C\\) , and \\(\\color{ocre2}D\\) , such that total weight keeps decreasing. ::: center ::: As mentioned before, Bell-man Ford's algorithm runs for \\(\\color{ocre2}|V| - 1\\) iterations and it guarantees that at the end, the distances are guaranteed to be correct or [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is false. ::: center ::: However, as you can see that is not the case, such that [ v.d > u.d + w(u,v) ]{style=\"background-color: light-gray\"} is still true, thus, it will detect a negative cycle. As a summary, comparing the two algorithms we've covered in this section:: The Dijkstra's algorithm is less time consuming, as it has a time complexity of \\(\\color{ocre2}\\text{O}(E\\log{V})\\) , compared to Bellman-Ford's algorithm which is \\(\\color{ocre2}\\text{O}(VE)\\) . Bellman-Ford works when there is negative weight edge, it also detects the negative weight cycle. Dynamic programming approach is taken to implement the algorithm. While Dijkstra's algorithm doesn't work when there is a negative weight edge. Greedy approach is taken to implement the algorithm.","title":"Elementary Graph Algorithms"},{"location":"W2022/MTH314/MTH314/","text":"Introduction This will cover various topics for the course MTH314: Discrete Mathematics for Engineering, using the textbook, Discrete Mathematics , by A. Bonato, the textbook, Discrete Mathematics with Applications , by S. Epp, and lectures notes provided by the professor, Dr. Changping Wang. Other resources used: Discrete Math (Full Course: Sets, Logic, etc) - Dr. Trefor Bazett Last Updated: 2022-04-16 Intro to Sets and Logic \u00b6 The Language of Sets \u00b6 One of the most important fundamentals revolves around sets. Definition 1.1 (Set). A set refers to a collection of objects, written in set-roster notation; using curly brackets \\(\\{ \\}\\) or set-builder notation, which will be discussed later. We use the following notation \\(\\in\\) to represent an element of a set and \\(\\notin\\) when it is not an element of a set. Example 1.1 . *Given \\(A = \\{1,2,3,4,5\\}\\) , we can write it as: \\(1 \\in A\\) \\(3 \\in A\\) \\(6 \\notin A\\) \\(\\pi \\notin A\\) There are certain sets of numbers referred to frequently, so they are given common set notations: Symbol Set of ... Example \\(\\mathbb{N}\\) Non-negative integers or natural numbers \\(\\{0,1,2,3, \\cdots\\}\\) \\(\\mathbb{Z}\\) Integers \\(\\{\\cdots, -2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q}\\) Rational numbers \\(\\{\\frac{p}{q} \\mid p,q \\in \\mathbb{Z}, q \\neq 0\\}\\) \\(\\mathbb{R}\\) Real numbers All of the above number sets Addition to a superscript \\(+\\) or \\(-\\) indicates that only positive or negative elements of the sets: \\(\\mathbb{Z}^+ = \\{1,2,\\cdots\\}\\) \\(\\mathbb{Z}^- = \\{-1,-2,\\cdots\\}\\) \\(\\mathbb{R}^+\\) is a set of positive real numbers \\(\\mathbb{R}^-\\) is a set of negative real numbers Another way to describe a set is using a set-builder notation, which characterize all the elements in the set by stating the property or properties they must have to be members. Definition 1.2 (Set-Builder Notation). *Let \\(S\\) denote a set and let \\(P(x)\\) be a property that elements of \\(S\\) may or may not satisfy. \\[\\{x \\in S \\mid P(x)\\}\\] We may define the following to be the set of all elements \\(x\\) in \\(S\\) such that \\(P(x)\\) is true. Example 1.2 . Let \\(A = \\{x \\in \\mathbb{Z} \\mid -2 < x < 5\\}\\) , the following set can be described as the set of all elements \\(x\\) are integers such that \\(-2 < x < 5\\) , where: \\( \\(A = \\{-1,0,1,2,3,4\\}\\) \\) * Definition 1.3 (Cardinality). *The cardinality of set denotes the number of elements of the set, usually denoted with a vertical bar on each side. Example 1.3 . Let \\(B = \\{2,5,7,9,12\\}\\) , then the cardinality of set \\(B\\) is: \\[|B| = 5\\] As defined earlier, a set is a collection of objects and so how do we define a set with no objects? Definition 1.4 (Empty Set). *A special set that contains no elements is called an empty set, which uses the notation \\(\\varnothing\\) . For an empty set, the cardinality would be \\(|\\varnothing| = 0\\) , since it contains no elements; \\(\\varnothing = \\{\\}\\) . Subsets \u00b6 A basic relation between sets is that of subset, which introduces a new notation \\(\\subseteq\\) for subset and \\(\\subsetneq\\) for proper subset. $$A \\subseteq B$$ $$A \\subsetneq B$$ Definition 1.5 (Subset). If \\(A\\) and \\(B\\) are sets, then \\(A\\) is a subset of \\(B\\) , written as \\(A \\subseteq B\\) , if and only if every element of \\(A\\) is also an element of \\(B\\) . Definition 1.6 (Proper subset). *If \\(A\\) is a subset of \\(B\\) , but not equal to \\(B\\) ; where there is at least one element of \\(B\\) not in \\(A\\) , then \\(A\\) is a proper subset of \\(B\\) , written as \\(A \\subsetneq B\\) . Some online sources or textbooks may use the following notation \\(\\subset\\) instead of \\(\\subsetneq\\) , to represent a proper subset. They are the same thing, but to avoid confusion, I'll be using this notation \\(\\subsetneq\\) . Note that every proper subset is a subset, but not every subset is a proper subset. So the diagram shown in (b) can also be used to demonstrate what a subset looks like. Example 1.4 . *Let \\(A = \\{1,2,3,4,5\\}\\) , \\(B = \\{1,3,5\\}\\) , \\(C = \\{2,3,5\\}\\) , and \\(D = \\{2,3,5\\}\\) then: \\(B \\subseteq A\\) and \\(B \\subsetneq A\\) \\(C \\subseteq A\\) and \\(C \\subsetneq A\\) \\(C\\subseteq D\\) and \\(D \\subseteq C\\) Two sets are equal when they share the exact element or in other words, subsets of each other: \\(C \\subseteq D\\) and \\(D \\subseteq C \\Longleftrightarrow C = D\\) . Power Sets \u00b6 Previously, we went over what a subset is and so will use that to define the following set. Definition 1.7 (Power Set). Let \\(S\\) denote a set, the power set of \\(S\\) , denoted \\(\\mathcal{P}(S)\\) , is the set of all subsets of \\(S\\) . In general, for any finite set \\(S\\) , where \\(|S| = n\\) , we have that \\(|\\mathcal{P}(S)| = 2^n\\) . If you recall every proper subset is considered a subset and so we may use that in the following example. Any set is gonna have various subsets. Example 1.5 . Given a set \\(A = \\{1,3,5\\}\\) , we can define the following to be subsets of \\(A\\) : \\(\\{1\\} \\subseteq A\\) \\(\\{1,3\\} \\subseteq A\\) \\(\\{1,5\\} \\subseteq A\\) \\(\\{1,3,5\\} \\subseteq A\\) and so on The power set is basically all the possible subsets of \\(A\\) that can be formed given a set. The empty set \\(\\varnothing\\) and the set \\(A\\) is always included in the \\(\\mathcal{P}(A)\\) . Example 1.6 . Using the set \\(A = \\{1,3,5\\}\\) , then: \\[\\mathcal{P}(A) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,3\\}, \\{1,5\\}, \\{3,5\\}, \\{1,3,5\\}\\}\\] It becomes quite tricky when we use an empty set. The power set of an empty set, \\(\\mathcal{P}(\\varnothing) = \\{\\varnothing\\}\\) with cardinality of \\(1\\) . We can double check this by \\(|\\varnothing| = 0\\) and so \\(|\\mathcal{P}(\\varnothing)| = 2^0 = 1\\) or cardinality of \\(1\\) . Singleton and Doubleton \u00b6 When a set only has one or two elements, it can be classified into one of two ways: Definition 1.8 (Singleton). A singleton is a set with a single element, \\(\\{x\\}\\) . Definition 1.9 (Doubleton). A doubleton (or unordered pair) is a set with two elements, \\(\\{x,y\\}\\) . An ordered pair uses round brackets instead of curly brackets to indicate that order matters, \\((x,y)\\) , which will be discussed more in later sections. Operations on Sets \u00b6 There are four main set operations to be discussed. Let \\(A\\) and \\(B\\) be subsets of a universal set \\(U\\) . Universal set, denoted \\(U\\) , is the collection of all objects that can occur as elements of the sets under consideration. All other sets are subsets of the universal set. Definition 1.10 (Union). The union of \\(A\\) and \\(B\\) , denoted \\(A \\cup B\\) , is the set of all elements that are at least in one of \\(A\\) or \\(B\\) . \\[A\\ \\cup\\ B = \\{x \\in U \\mid x \\in A \\text{ or } x \\in B\\}\\] Definition 1.11 (Intersection). The intersection of \\(A\\) and \\(B\\) , denoted \\(A \\cap B\\) , is the set of all elements that are common to both \\(A\\) and \\(B\\) . \\[A\\ \\cap\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\in B\\}\\] Definition 1.12 (Difference). The difference of \\(A\\) minus \\(B\\) , denoted \\(A \\smallsetminus B\\) , is the set of all elements that are in \\(A\\) and not in \\(B\\) . \\( \\(A\\ \\smallsetminus\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\notin B\\}\\) \\) * Definition 1.13 (Complement). The complement of \\(A\\) , denoted \\(A^c\\) , is the set of all elements in \\(U\\) that are not in \\(A\\) . \\[A^c = \\{x \\in U | x \\notin A\\}\\] There's one other operation not included, which is the symmetric difference, denoted \\(A \\triangle B\\) , is the set of elements which are in either of the sets A and B, but are not common to both \\(A\\) and \\(B\\) . Set Properties \u00b6 The following theorem consists of set identities, some of which you might be familiar with: Theorem 1.1 . *Let \\(A\\) , \\(B\\) , and \\(C\\) be subsets of a universal set \\(U\\) : Commutative law: Associative law: Distributive law: Complement law: Double complement law: De Morgan's law: Identity law: Idempotent law: Set difference law: Indexed Collection of Sets \u00b6 The definitions of unions and intersections for more than two sets are very similar to the definitions for two sets, which we may generalize in the following way. Definition 1.14 (Union and Intersection). *Let \\(A_i\\) be a subset of a universal set \\(U\\) where \\(i \\geq 1\\) and given a non-negative integer \\(n\\) . \\[\\bigcup\\limits_{i=1}^n\\ A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } 1 \\leq i \\leq n\\}\\] \\[\\bigcap\\limits_{i=1}^n\\ A_i = A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } 1 \\leq i \\leq n\\}\\] ... and generalize to infinite unions and intersections. \\[\\bigcup\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } i \\geq 1 \\}\\] \\[\\bigcap\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } i \\geq 1\\}\\] Recap of the interval notation, \\((\\ )\\) means the endpoints are excluded and \\([\\ ]\\) means they are included. There are three specific types you'll encounter when solving for infinite intersections of a set: \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg[0, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (n, \\infty)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ [n, \\infty)\\) There might be slight variations of the questions, but it should give you a general idea on how to solve for them. Example 1.7 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : Let's think logically, by drawing out the number lines for the first three sets. If you notice the number line slowly decreases in size. As it reaches infinity, it will eventually reach \\(0\\) , which will be the only thing they all share in common. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = \\{0\\}\\] Example 1.8 . *Solve the following set, \\(\\bigcup\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : 1. We are now interested in all of the possible elements of the set. Using the number line from before, if you notice, \\((-1,1)\\) already contains all the elements in \\(A_2\\) and \\(A_3\\) . So we can say that \\((-1,1)\\) contains all the elements shared in the infinite set. \\[\\bigcup\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = (-1,1)\\] The notation for the open interval \\((a,b)\\) is identical to the notation for ordered pair \\((a,b)\\) , context makes it unlikely that the two will be confused. Example 1.9 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(0, \\frac{1}{i}\\Big)\\) : Let's draw out the number lines again for the first three sets. Likewise, as it reaches infinity, it will eventually reach \\(0\\) , however, note that \\(0\\) is not included, from the round brackets, so we say it's an empty set. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg) = \\varnothing\\] If instead it was \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\Big[0, \\frac{1}{i}\\Big)\\) , then our answer would be \\(\\{0\\}\\) , as it is included in the set. ::: ::: exampleT Example 1.10 . Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty)\\) : Let's draw out the first number line when \\(i = 1\\) and when \\(i = 5\\) . ::: center ::: As \\(i\\) goes from \\(1\\) to \\(5\\) , you notice that \\(\\{1,2,3,4\\}\\) is no longer common for all sets. As \\(i \\to \\infty\\) , logically there exists no real number which are common for all sets. We can conclude that the intersection of the infinite set is an empty set. ( \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty) = \\varnothing\\) \\) ::: ::: list Note that the answer doesn't change even if we have \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (i,\\infty)\\) , our answer is still \\(\\varnothing\\) . ::: If you are interested in the mathematical proof, click on the following link: [Archimedean property]{.underline} . -3ex -0.1ex -.4ex 0.5ex .2ex Cartesian Products on Sets In this section, we'll first focus on the notion of ordered pairs and how they work. ::: dBox ::: definitionT Definition 1.15 (Ordered pair). The symbol \\((a,b)\\) denotes the ordered pair with the specification that \\(a\\) is the first element and \\(b\\) is the second element of the pair. ( \\((a,b) = (c,d) \\longrightarrow a = c \\text{ and } b = d\\) \\) ::: ::: ::: dBox ::: definitionT Definition 1.16 (Cartesian product). For sets \\(A\\) and \\(B\\) , the Cartesian product of \\(A\\) and \\(B\\) , denoted \\(A \\times B\\) , is the set of all ordered pairs \\((a,b)\\) . ( \\(A \\times B = \\{(a,b) \\mid a \\in A \\text{ and } b \\in B\\}\\) \\) ::: ::: ::: exampleT Example 1.11 . Let \\(A = \\{x,y\\}\\) and \\(B = \\{1,2,3\\}\\) , find \\(A \\times B\\) and \\(B \\times A\\) : \\(A \\times B = \\{(x,1), (y,1), (x,2), (y,2), (x,3), (y,3)\\}\\) \\(B \\times A = \\{(1,x), (1,y), (2,x), (2,y), (3,x), (3,y)\\}\\) ::: ::: list Note how the order matters, such that \\(A \\times B \\neq B \\times A\\) in the following example above. ::: -4ex -1ex -.4ex 1ex .2ex Logic It's important to first establish one thing, which we define as: ::: dBox ::: definitionT Definition 1.17 (Statement). A statement (or proposition) is a sentence that is true or false, but not both. ::: ::: There are different ways to express a statement as shown below. ::: exampleT Example 1.12 . Determine whether the following are statements and if so, are they true or false? \\\" \\(2\\) is greater than \\(5\\) \\\" is a statement and is logically false. \\\" \\(x > 5\\) \\\" is not a statement because of the variable \\(x\\) , it is undetermined. \\\" \\(\\sqrt{9}\\) is an integer\\\" is a statement and is logically true. \\\"There are \\(7\\) days in a week\\\" is a statement and is logically true. ::: We will now introduce five logical connectives, used to build more complicated logical expressions out of simpler ones. Let \\(P\\) and \\(Q\\) be statement variables. ::: dBox ::: definitionT Definition 1.18 (Negation). The statement \\\"not \\(P\\) \\\", denoted by \\(\\lnot\\ P\\) , is true when \\(P\\) is false. ::: ::: ::: dBox ::: definitionT Definition 1.19 (Conjunction). The statement \\\" \\(P\\) and \\(Q\\) \\\", denoted by \\(P\\ \\land\\ Q\\) , is true when, and only when, both \\(P\\) and \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.20 (Disjunction). The statement \\\" \\(P\\) or \\(Q\\) \\\", denoted by \\(P\\ \\lor\\ Q\\) , is true when at least one of \\(P\\) or \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.21 (Conditional). The statement \\\"If \\(P\\) then \\(Q\\) \\\", denoted by \\(P\\ \\rightarrow\\ Q\\) , is false when \\(P\\) is true and \\(Q\\) is false; otherwise it is true. ::: ::: ::: dBox ::: definitionT Definition 1.22 (Biconditional). The statement \\\" \\(P\\) if and only if \\(Q\\) \\\", denoted by \\(P\\ \\leftrightarrow\\ Q\\) , is true exactly when either \\(P\\) and \\(Q\\) are both true, or when \\(P\\) and \\(Q\\) are both false. ::: ::: ::: list The order of operations goes from \\(\\lnot\\) , \\(\\land\\) , \\(\\lor\\) , \\(\\rightarrow\\) then \\(\\leftrightarrow\\) , if no parenthesis are present. ::: The five logical connectives has the following truth table: \\(P\\) \\(\\lnot\\ P\\) T F F T \\(P\\) \\(Q\\) \\(P\\ \\land\\ Q\\) \\(P\\ \\lor\\ Q\\) \\(P\\ \\rightarrow\\ Q\\) \\(P\\ \\leftrightarrow\\ Q\\) T T T T T T T F F T F F F T F T T F F F F F T T However, when statement is always true or false we can define to be the following: ::: dBox ::: definitionT Definition 1.23 (Tautology). A tautology is a statement form that is always true regardless of the truth values of the individual statement substituted for its statement variables ::: ::: ::: dBox ::: definitionT Definition 1.24 (Contradiction). A contradiction is a statement form that is always false regardless of the truth values of the individual statements substituted for its statement variables. ::: ::: ::: exampleT Example 1.13 . Show that \\(P\\ \\lor\\ \\lnot\\ P\\) is a tautology and that \\(P\\ \\land\\ \\lnot\\ P\\) is a contradiction. \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\lor\\ \\lnot\\ P\\) *T* *F* *T* *F* *T* *T* \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\land\\ \\lnot\\ P\\) *T* *F* *F* *F* *T* *F* ::: -3ex -0.1ex -.4ex 0.5ex .2ex Conditional Statements When asked to rewrite the following sentences using \\(\\rightarrow\\) , they would use the phrases \\\"necessary condition\\\" and \\\"sufficient condition\\\", which implies: The statement \\(P\\) is a necessary condition for \\(Q\\) means that \\(Q\\ \\rightarrow\\ P\\) . The statement \\(P\\) is a sufficient condition for \\(Q\\) means that \\(P\\ \\rightarrow\\ Q\\) . For a conditional statement, \\(P\\ \\rightarrow\\ Q\\) , we can form two related statements: ::: dBox ::: definitionT Definition 1.25 (Inverse). The inverse of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ P\\ \\rightarrow\\ \\lnot\\ Q\\) . ::: ::: ::: dBox ::: definitionT Definition 1.26 (Converse). The converse of \\(P\\ \\rightarrow\\ Q\\) is \\(Q\\ \\rightarrow\\ P\\) . ::: ::: ::: dBox ::: definitionT Definition 1.27 (Contrapositive). The contrapositive of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ Q\\ \\rightarrow\\ \\lnot\\ P\\) . ::: ::: ::: exampleT Example 1.14 . Rewrite the following statement: \\\"If \\(n\\) is prime, then \\(n\\) is odd or \\(n\\) is \\(2\\) \\\" using the: Inverse: If \\(n\\) is not prime, then \\(n\\) is not odd and \\(n\\) is not \\(2\\) . Converse: If \\(n\\) is odd or \\(n\\) is \\(2\\) , then \\(n\\) is a prime. Contrapositive: If \\(n\\) is not odd and \\(n\\) is not \\(2\\) , then \\(n\\) is not a prime. ::: ::: list When writing the negation, watch out for other logical connectives present in the sentences, like \\(\\land\\) and \\(\\lor\\) and make sure to apply De Morgan's laws properly. ::: -3ex -0.1ex -.4ex 0.5ex .2ex Logical Equivalences Two statements are logically equivalent, denoted by \\(\\equiv\\) , if they share the same truth table. ::: tBox ::: theoremeT Theorem 1.1 . Let \\(P\\) and \\(Q\\) be statement variables: Commutative law: Associative law: Distributive law: Double negation law: De Morgan's law: Idempotent law: Implication law: ::: ::: You might notice some similarities to the previous theorem for unions and intersections of a set. ::: tabu *2X[c] Logical Equivalences & Set Properties \\ ::: flushleft For all statements variables \\(P\\) , \\(Q\\) , and \\(R\\) : ::: & ::: flushleft For all sets \\(A\\) , \\(B\\) , and \\(C\\) : ::: \\ (a) \\(P\\ \\lor\\ Q \\equiv Q\\ \\lor\\ P\\) (b) \\(P\\ \\land\\ Q \\equiv Q\\ \\land\\ P\\) & (a) \\(A\\ \\cup\\ B = B\\ \\cup\\ A\\) (b) \\(A\\ \\cap\\ B = B\\ \\cap\\ A\\) \\ (a) \\(P\\ \\land\\ (Q \\land R) \\equiv (P\\ \\land Q)\\ \\land\\ R\\) (b) \\(P\\ \\lor\\ (Q \\lor R) \\equiv (P\\ \\lor Q)\\ \\lor\\ R\\) & (a) \\(A\\ \\cap\\ (B\\ \\cap\\ C) = (A\\ \\cap\\ B)\\ \\cap\\ C\\) (b) \\(A\\ \\cup\\ (B\\ \\cup\\ C) = (A\\ \\cup\\ B)\\ \\cup\\ C\\) \\ (a) \\(P\\ \\land\\ (Q\\ \\lor R) \\equiv (P\\ \\land Q)\\ \\lor\\ (P\\ \\land\\ R)\\) (b) \\(P\\ \\lor\\ (Q\\ \\land R) \\equiv (P\\ \\lor Q)\\ \\land\\ (P\\ \\lor\\ R)\\) & (a) \\(A\\ \\cap\\ (B\\ \\cup\\ C) = (A\\ \\cap\\ B)\\ \\cup\\ (A\\ \\cap\\ C)\\) (b) \\(A\\ \\cup\\ (B\\ \\cap\\ C) = (A\\ \\cup\\ B)\\ \\cap\\ (A\\ \\cup\\ C)\\) \\ (a) \\(\\lnot(\\lnot\\ P) \\equiv P\\) & (a) \\((A^c)^c = A\\) \\ (a) \\(P\\ \\lor\\ P \\equiv P\\) (b) \\(P\\ \\land\\ P \\equiv P\\) & (a) \\(A\\ \\cup\\ A = A\\) (b) \\(A\\ \\cap\\ A = A\\) \\ (a) \\(\\lnot(P\\ \\lor\\ Q) \\equiv \\lnot\\ P\\ \\land\\ \\lnot\\ Q\\) (b) \\(\\lnot(P\\ \\land\\ Q) \\equiv \\lnot\\ P\\ \\lor\\ \\lnot\\ Q\\) & (a) \\((A\\ \\cup\\ B)^c = A^c\\ \\cap\\ B^c\\) (b) \\((A\\ \\cap\\ B)^c = A^c\\ \\cup\\ B^c\\) \\ ::: ::: tabu *2X[c] (a) \\(P\\ \\lor\\ (P\\ \\land\\ Q) \\equiv P\\) (b) \\(P\\ \\land\\ (P\\ \\lor\\ Q) \\equiv P\\) & (a) \\(A\\ \\cup\\ (A\\ \\cap\\ B) = A\\) (b) \\(A\\ \\cap\\ (A\\ \\cup\\ B) = A\\) \\ ::: -3ex -0.1ex -.4ex 0.5ex .2ex Predicates and Quantified Statements We initially discussed that a logical statement is either true or false. So something like \\\" \\(x > 5\\) \\\" is not a statement, but what we define to be a predicate. ::: dBox ::: definitionT Definition 1.28 (Predicate). A predicate \\(P(x)\\) is a sentence that contains a finite number of variables and becomes a statement when specific values are substituted for variables. ::: ::: ::: dBox ::: definitionT Definition 1.29 (Domain). The domain \\(D\\) of a predicate variable is the set of all values that may be substituted in place of variable. ::: ::: A way to obtain statements from predicates is to add quantifiers. Let \\(P(x)\\) be a predicate and \\(D\\) the domain of \\(x\\) . ::: dBox ::: definitionT Definition 1.30 (Universal quantifier). The symbol \\(\\forall\\) is read as \\\"for every\\\" or \\\"for all.\\\" A universal statement is a statement of the form, \\(\\forall x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for every \\(x\\) in \\(D\\) . It is defined to be false if, and only if, \\(P(x)\\) is false for at least one \\(x\\) in \\(D\\) . ::: ::: ::: dBox ::: definitionT Definition 1.31 (Existential quantifier). The symbol \\(\\exists\\) is read as \\\"there exists\\\" or \\\"there is.\\\" An existential statement is a statement of the form, \\(\\exists x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for at least one \\(x\\) in \\(D\\) . It is false if, and only if, \\(P(x)\\) is false for all \\(x\\) in \\(D\\) . ::: ::: ::: exampleT Example 1.15 . Rewrite the following sentences using quantifiers. \\\"Every real number has a non-negative square\\\" rewritten as \\(\\forall x \\in \\mathbb{R},\\ x^2 \\geq 0\\) \\\"There is a positive integer whose square is equal to itself\\\" rewritten as \\(\\exists y \\in \\mathbb{Z}^+,\\ y^2 = y\\) ::: One final topic to discuss is the negation of universal and existential quantifiers. ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Universal statement). The negation of a universal statement is logically equivalent to an existential statement. Symbolically, ( \\(\\lnot(\\forall x \\in D,\\ P(x)) \\equiv \\exists x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Existential statement). The negation of a existential statement is logically equivalent to an universal statement. Symbolically, ( \\(\\lnot(\\exists x \\in D,\\ P(x)) \\equiv \\forall x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: exampleT Example 1.16 . Negate the following statements from the previous exercise. \\\"There exists a real number that has a negative square\\\" or \\(\\exists x \\in \\mathbb{R},\\ x^2 < 0\\) \\\"Every positive integer whose square is not equal to itself\\\" or \\(\\forall x \\in \\mathbb{Z}^+,\\ y^2 \\neq y\\) ::: ::: list In example 1.11, the first statement is true, while, in example 1.12, the first statement is false, since squaring a number will always be positive. ::: There are some cases where certain statements contain multiple quantifiers. ::: exampleT Example 1.17 . Negate the following statement: \\(\\forall x \\in \\mathbb{Z},\\ \\exists y \\in \\mathbb{Z},\\ y > x\\) . We can simply think of \\\" \\(\\exists y \\in \\mathbb{Z},\\ y > x\\) \\\" as the predicate \\(P(x)\\) . ( \\(\\lnot(\\forall x \\in \\mathbb{Z},\\ P(x)) \\equiv \\exists x \\in \\mathbb{Z},\\ \\lnot\\ P(x)\\) \\) Then we take the negation of \\(P(x)\\) . ( \\(\\lnot\\ P(x) \\equiv \\lnot(\\exists y \\in \\mathbb{Z},\\ y > x) \\equiv \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) And we put it all together. ( \\(\\exists x \\in \\mathbb{Z},\\ \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Order of Quantifiers In a statement containing both \\(\\forall\\) and \\(\\exists\\) , changing the order of the quantifiers can significantly change the meaning of the statementyou read from left to right. For example, the following statements are equivalent: \\( \\(\\forall x,\\ \\forall y,\\ P(x) \\equiv \\forall y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\exists y,\\ P(x) \\equiv \\exists y,\\ \\exists x,\\ P(x)\\) \\) However, now consider mixed quantifier and they are no longer equivalent: \\( \\(\\forall x,\\ \\exists y,\\ P(x) \\not\\equiv \\exists y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\forall y,\\ P(x) \\not\\equiv \\forall y,\\ \\exists x,\\ P(x)\\) \\) For a more dramatic context. let's use the following analogy: \\(\\forall x,\\ \\exists y,\\ x \\text{ loves } y\\) and \\(\\exists y,\\ \\forall x,\\ x \\text{ loves } y\\) Note that both statements looks identical, except the order of quantifiers. However, the first statement means everybody loves somebodywhom that somebody could be a different person for each \\(x\\) , ... whereas the second statement means there is one individual who is loved by all people. We can also visualize this using a directed graph, as a shown below: Relations and Functions \u00b6 -4ex -1ex -.4ex 1ex .2ex Binary Relations A relations is something that involves two different sets. A special kind of binary relation is a function. Suppose there are some elements inside \\(X\\) and \\(Y\\) , we can visualize an arrow diagram for our relation. The graph corresponds to the following ordered pairs: \\(\\{(x_1,y_1), (x_2, y_2), (x_3, y_3)\\}\\) ::: dBox ::: definitionT Definition 2.1 (Binary Relation). For sets \\(X\\) and \\(Y\\) , a binary relation \\(R\\) from \\(X\\) to \\(Y\\) is a subset of \\(X \\times Y\\) . Hence, \\(R\\) is a set of ordered pairs \\((x,y)\\) with \\(x \\in X\\) and \\(y \\in Y\\) . We write \\(x \\mathrel{R}y\\) if \\((x,y) \\in R\\) . We say that \\(R\\) is a binary relation on \\(X\\) if \\(X = Y\\) ; that is, \\(R\\subseteq X \\times X\\) . ::: ::: A relation can also be drawn as a directed graph which will prove to be more useful when explaining the properties of relation. Using the same set of ordered pairs from before, it can drawn as such: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Relation There exists various properties that some, but not all, relations satisfy. Let \\(R\\) be a binary relation on a set \\(A\\) : ::: dBox ::: definitionT Definition 2.2 (Reflexive). The relation \\(R\\) is reflexive, if for all \\(x \\in A\\) , \\(x \\mathrel{R}x\\) . \\(R\\) is reflexive \\(\\Leftrightarrow\\) for every \\(x\\) in \\(A\\) , \\((x,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.3 (Symmetric). The relation \\(R\\) is symmetric, if for all \\(x,y \\in A\\) , if \\(x \\mathrel{R}y\\) , then \\(y \\mathrel{R}x\\) . \\(R\\) is symmetric \\(\\Leftrightarrow\\) for every \\(x\\) and \\(y\\) in \\(A\\) , if \\((x,y) \\in R\\) then \\((y,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.4 (Transitive). The relation \\(R\\) is transitive, if for all \\(x,y,z \\in A\\) , if \\(x \\mathrel{R}z\\) and \\(y \\mathrel{R}z\\) , then \\(x \\mathrel{R}z\\) . \\(R\\) is transitive \\(\\Leftrightarrow\\) for every \\(x\\) , \\(y\\) , and \\(z\\) in \\(A\\) , if \\((x,y) \\in R\\) and \\((y,z) \\in R\\) then \\((x,z) \\in R\\) ::: center ::: ::: ::: ::: exampleT Example 2.1 . Suppose we have a set \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0), (0,1), (0,3), (1,0), (1,1), (2,2), (3,0), (3,3)\\}\\) \\) Let's draw out the directed graph. ::: center ::: \\(R\\) is reflexive because there is a loop for each element, as shown for \\((0,0), (1,1), (2,2), (3,3)\\) . \\(R\\) is symmetric because there is an arrow going from one point then back to the other, as shown for \\((0,1), (1,0)\\) and \\((0,3), (3,0)\\) \\(R\\) is not transitive because there's no arrow from \\((1,3)\\) or \\((3,1)\\) which would otherwise make it transitive. ::: Sometimes the set of relation is not given, instead the definition is provided. For example, suppose we have set a \\(A = \\{1,2,3,4\\}\\) and the relation \\(R\\) defined as follows: Properties of Div: \\((x,y) \\in R \\text{ if } x \\mid y\\) The line \\\" \\(\\mid\\) \\\" means \\(x\\) divisible by \\(y\\) . Equivalently, you can think of it as \\(y = xk\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,2),(1,3),(1,4),(2,2),(2,4),(3,3),(4,4)\\}\\) \\) Properties of Congruence Modulo n: \\((x,y) \\in R \\text{ if } n \\mid (x-y)\\) Sometimes, it referred to as \\(x \\equiv y \\ (\\mathrm{mod}\\ n)\\) . Let use \\(n = 2\\) , as an example. We can use the expression before, \\(x-y = 2k\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,3),(2,2),(2,4),(3,1),(3,3),(4,2),(4,4)\\}\\) \\) Properties of \\\"Greater Than\\\": \\((x,y) \\in R \\text{ if } x > y\\) In this case, we are only interested where \\(x\\) is greater than \\(y\\) . The set of relation would be: \\( \\(R = \\{(2,1),(3,1),(3,2),(4,1),(4,2),(4,3)\\}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Equivalence Relation There's are cases when the relation has all three properties discussed. ::: dBox ::: definitionT Definition 2.5 (Equivalence relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is an equivalence relation if it is reflexive, symmetric and transitive, \\((A, R)\\) . ::: ::: A notable example would be the equality sign, which might not make senses at first, but let's break it down. For all real numbers \\(x\\) and \\(y\\) , consider the relation \\(R\\) defined as follows \\( \\((x,y) \\in R \\text{ if } x = y\\) \\) Is \\(R\\) reflexive? Yes, because it is implying the statement \\(x = x\\) , which is true; every real number is equal to itself. Is \\(R\\) symmetric? Yes, because if \\(x = y\\) , then \\(y = x\\) is also true; if one number is equal to another, then the second is equal to the first. Is \\(R\\) transitive? Yes, because if \\(x = y\\) and \\(y = z\\) , then \\(x = z\\) is true; if one real number equals a second and the second equals a third, then the first must also equal the third. Let's introduce this new idea called the equivalence class, as an extension to equivalence relation. Suppose \\(A\\) is a set and \\(R\\) is an equivalence relation on \\(A\\) . ::: dBox ::: definitionT Definition 2.6 (Equivalence class). For each element \\(a\\) in \\(A\\) , the equivalence class of \\(a\\) , denoted \\([a]\\) is the set of all elements \\(x\\) in \\(A\\) such that \\(x\\) is related to \\(a\\) by \\(R\\) . ( \\(= \\{x \\in A \\mid x \\mathrel{R}a \\}\\) \\) ::: ::: ::: list Some textbooks may define it as \\([a] = \\{x \\in A \\mid a \\mathrel{R}x \\}\\) instead. Either one works, as you may recall, it's transitive. ::: ::: exampleT Example 2.2 . Let \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0),(0,4),(1,1),(1,3),(2,2),(3,1),(3,3),(4,0),(4,4)\\}\\) \\) Determine all the elements related to \\(0\\) or \\(x \\mathrel{R}0\\) , where \\(x \\in A\\) . ( \\(0 \\mathrel{R}0,\\ 4 \\mathrel{R}0\\) \\) The elements \\(0\\) and \\(4\\) are essentially what makes up the equivalence class of \\(0\\) , which more formally can be written as ( \\([0] = \\{x \\in A \\mid x \\mathrel{R}0\\} = \\{0,4\\}\\) \\) Thus, find the equivalence class for the rest of the elements of \\(A\\) . ( \\([1] = \\{x \\in A \\mid x \\mathrel{R}1\\} = \\{1,3\\}\\) \\) ( \\([2] = \\{x \\in A \\mid x \\mathrel{R}2\\} = \\{2\\}\\) \\) ( \\([3] = \\{x \\in A \\mid x \\mathrel{R}3\\} = \\{1,3\\}\\) \\) ( \\([4] = \\{x \\in A \\mid x \\mathrel{R}4\\} = \\{0,4\\}\\) \\) Note that \\([4] = [0]\\) and \\([1] = [3]\\) , so the distinct equivalent classes are ( \\(\\{0,4\\}, \\{1,3\\}, \\text{ and } \\{2\\}\\) \\) ::: If you notice, the distinct equivalence classes of an equivalence relation \\(R\\) on a set \\(A\\) form a partition of \\(A\\) . Likewise, the converse is also true, which will discuss in the next section. ::: tBox ::: theoremeT Theorem 2.1 . Let \\(R\\) be an equivalence relation on a set \\(A\\) , where we assume \\(A \\neq \\varnothing\\) . For all \\(x \\in A\\) , \\([x] \\neq \\varnothing\\) . If \\(x \\mathrel{R}y\\) , then \\([x] = [y]\\) . If \\((x,y) \\notin R\\) , then \\([x]\\ \\cap\\ [y] = \\varnothing\\) . ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Relation Induced by Partition As a recap, a partition of a set \\(A\\) is a finite or infinite collection of nonempty, mutually disjoint subsets whose union is \\(A\\) or illustratively, ::: center ::: where \\(A_i\\ \\cap\\ A_j = \\varnothing\\) whenever \\(i \\neq j\\) and \\(A_1\\ \\cup\\ \\cdots\\ \\cup\\ A_6 = A\\) . ::: dBox ::: definitionT Definition 2.7 . Let \\(P\\) be a partition of a set \\(A\\) and define the binary relation \\(R_P\\) , so that \\(x \\mathrel{R}y\\) if \\(x\\) and \\(y\\) are in the same part of the partition. ::: ::: Again, this definition might sound confusing, but let's use the set \\(A\\) from the previous example to get a better understanding. ::: exampleT Example 2.3 . Let \\(A = \\{0,1,2,3,4\\}\\) and consider the following be a partition \\(P\\) of \\(A\\) : ( \\(P = \\{\\{0,1\\},\\{2,3\\},\\{4\\}\\} % A_1 = \\{0,1\\} \\hspace{1cm} A_2 = \\{2,3\\} \\hspace{1cm} A_3 = \\{4\\}\\) \\) Let's consider the first subset of the partition, \\(A_1 = \\{0,1\\}\\) . Since both \\(0\\) and \\(1\\) are in the subset, we can form the following relation. ( \\(0 \\mathrel{R}1,\\ 1\\mathrel{R}0\\) \\) We can also do the following, since they are still in the same part of the partition. ( \\(0 \\mathrel{R}0,\\ 1 \\mathrel{R}1\\) \\) Thus, for the second subset, \\(A_2 = \\{2,3\\}\\) , we can form a similar set of relations. ( \\(2 \\mathrel{R}3,\\ 3 \\mathrel{R}2,\\ 2 \\mathrel{R}2,\\ 3 \\mathrel{R}3\\) \\) Finally for the third subset of the partition, \\(A_3 = \\{4\\}\\) , which only has one. ( \\(4 \\mathrel{R}4\\) \\) Hence, combining all of them makes up the binary relation \\(R_P\\) : ( \\(R_P = \\{\\underbrace{(0,1),(1,0),(0,0),(1,1)}_{\\{0,1\\}},\\underbrace{(2,3),(3,2),(2,2),(3,3)}_{\\{2,3\\}},\\underbrace{(4,4)}_{\\{4\\}}\\}\\) \\) ::: The fact is that a relation induced by a partition of a set satisfies all three properties, or in other words, \\(R_P\\) is an equivalence relation. -3ex -0.1ex -.4ex 0.5ex .2ex Partial Orders Partial orders provide one way of ranking objects. To define them, we define another property of relation. ::: dBox ::: definitionT Definition 2.8 (Antisymmetric). Let \\(R\\) be a binary relation on a set \\(A\\) . We say \\(R\\) is antisymmetric, if and only if, for every \\(a\\) and \\(b\\) in \\(A\\) , if \\(a \\mathrel{R}b\\) and \\(b \\mathrel{R}a\\) , where \\(a = b\\) . ::: ::: Equivalently, we are saying that the relation should not have the following: ::: center ::: For example, the relation on the left is not antisymmetric, whereas the relation on the right is antisymmetric. ::: center ::: ::: dBox ::: definitionT Definition 2.9 (Partial order relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is a partial order relation if it is reflexive, antisymmetric, and transitive. ::: ::: We call \\((A,R)\\) a partial ordered set or poset. We use \\(\\preceq\\) to represent the relation \\(R\\) . Two elements \\(x\\) and \\(y\\) are comparable if either \\(x \\mathrel{R}y\\) or \\(y \\mathrel{R}x\\) . Otherwise, the elements are incomparable. A set of pairwise incomparable is called an antichain . If every pair of element is comparable, then \\(R\\) is a total order , linear order , or chain . Back to previous example, the relation on the left has two elements, \\(0\\) and \\(1\\) , that are incomparable. The relation on the right is a linear order with three elements. ::: dBox ::: definitionT Definition 2.10 (Hasse diagram). A diagram used to represent partial order relations with sufficient information and an implied upward orientation. ::: ::: To obtain a Hasse diagram, proceed as follows: Construct a digraph (or directed graph) of the poset \\((A,R)\\) , so that all arrows point upward, except the loops. Eliminate all loops. Eliminate all directed edges that are redundant because of transitivity. Eliminate the arrows on the directed edge. Suppose we have a set \\(A = \\{1,2,3,9,18\\}\\) with the div relation, \\(x \\mid y\\) . The digraph of this poset has the following appearance on the left and the Hasse diagram on the right: ::: center ::: We can reference some extremal elements of posets using the following definitions. ::: dBox ::: definitionT Definition 2.11 . Let \\(R\\) be a partial order of on a set \\(A\\) : An element \\(u\\) is a least element if \\(\\forall x \\in A\\) , \\(u \\mathrel{R}x\\) . An element \\(v\\) is a greatest element if \\(\\forall x \\in A\\) , \\(x \\mathrel{R}v\\) . An element \\(u\\) is a minimal element if there does not exist an element \\(x \\in A \\smallsetminus \\{u\\}\\) , such that \\(x \\mathrel{R}u\\) . Alternatively, there exists no element \\\"below\\\" it. An element \\(u\\) is a maximal element if there does not exist an element \\(x \\in A \\smallsetminus \\{v\\}\\) , such that \\(v \\mathrel{R}x\\) . Alternatively, there exists no element \\\"above\\\" it. ::: ::: Note the difference between least and minimal element, likewise, with greatest and maximal element. A least element is a minimal, but a minimal element need not to be a least element. Similarly, a greatest element is a maximal, but a maximal element need not to be a greatest element. A poset can have at most one least and greatest element, but it may have more than one minimal or maximal element. Look at the following digraph of each poset: ::: center ::: -4ex -1ex -.4ex 1ex .2ex Introduction to Graph Theory Graph theory one of the most important topics in discrete mathematics. You may have heard of the famous bridge puzzle, known as \\\"The Seven Bridges of K\u00f6nigsberg\\\", which consists of the following problem. ::: problem Problem 2.1 . Is it possible to find a route through the graph that starts and ends at some vertex, one of \\(A\\) , \\(B\\) , \\(C\\) , or \\(D\\) , and traverses each edge exactly once? ::: center ::: ::: We can further simply this to the following graph. If you compare the two diagrams, they are equivalently the same. ::: center ::: If you aren't already aware of it, this problem is impossible to solve and it all relates back to graph theory. Let's start off by defining what a graph is. ::: dBox ::: definitionT Definition 2.12 (Graph). A graph \\(G\\) is a pair consisting of a vertex set \\(V(G)\\) and an edge set \\(E(G)\\) containing pairs of distinct vertices, such that \\(G = (V,E)\\) ::: ::: ::: list The bridge graph is an undirected graphthe order of the two connected vertices is not important, oppose to a directed graph. ::: The order of a graph \\(G\\) is \\(|V(G)|\\) and its size is \\(|E(G)|\\) . We can use the bridge example, to define our vertex and edge set, \\( \\(V(G) = \\{A,B,C,D\\} \\hspace{1cm} E(G) = \\{\\{A,B\\},\\{A,B\\},\\{B,D\\},\\{B,D\\},\\{A,C\\},\\{B,C\\},\\{C,D\\}\\}\\) \\) where \\(|V(G)| = 4\\) and \\(|E(G)| = 7\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Degrees There's no solution to the bridge problem because suppose we start and end at vertex \\(A\\) , then the degree of the other three vertices \\(B\\) , \\(C\\) , and \\(D\\) must be even. ::: dBox ::: definitionT Definition 2.13 (Degree). Given a graph with vertex \\(v\\) , the degree of \\(v\\) , denoted by \\(\\deg_G(v)\\) is the number of edges incident to \\(v\\) . ::: ::: From the graph earlier, we can define the degree of each vertex: \\(\\deg(A) = 3\\) , \\(\\deg(B) = 5\\) , \\(\\deg(C) = 3\\) , and \\(\\deg(D) = 3\\) . ::: dBox ::: definitionT Definition 2.14 (Neighbor set). The neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(\\{w \\in V(G) \\mid v,w \\in E(G)\\}\\) ; any \\(w \\in V(G)\\) is called a neighbor of \\(v\\) , equivalently \\(\\deg_G(v) = |N_G(v)|\\) . The closed neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(N_G(v)\\ \\cup\\ \\{v\\}\\) . ::: ::: ::: list As you'll see later on, the subscript \\(G\\) is often removed, if the graph is clear from context. ::: The definition may sound more confusing that what it is suppose to mean, but it is essentially the set of all vertices which are adjacent to \\(v\\) . Refer to the example below. Another set of terms will discuss in this section the minimum and maximum degree of a graph \\(G\\) . ::: dBox ::: definitionT Definition 2.15 (Minimum and Maximum Degree). The integer \\(\\delta(G)\\) is the minimum degree of \\(G\\) and the integer \\(\\Delta(G)\\) is the maximum degree of \\(G\\) . ::: ::: ::: exampleT Example 2.4 . Suppose we have the following graph \\(G\\) is defined as: ::: center ::: For each vertex: \\(\\deg(1) = 1\\) , \\(\\deg(2) = 2\\) , \\(\\deg(3) = 3\\) , \\(\\deg(4) = 2\\) , and \\(\\deg(5) = 2\\) . The vertex \\(3\\) has three vertices adjacent to it, which are \\(2\\) , \\(4\\) , and \\(5\\) , thus \\(N(3) = \\{2,4,5\\}\\) and \\(|N(3)| = \\deg(3) = 3\\) . The closed neighbor set includes \\(3\\) , since \\(N[3] = \\{2,4,5\\}\\ \\cup\\ \\{3\\} = \\{2,3,4,5\\}\\) . From 1. we can see the minimum degree is \\(1\\) and the maximum degree is \\(3\\) , thus \\(\\delta(G) = 1\\) and \\(\\Delta(G) = 3\\) . ::: The following theorem is pretty simple, but helpful in drawing conclusions when analyzing graphs. ::: tBox ::: theoremeT Theorem 2.1 (Handshake Theorem). Let \\(G\\) be a graph, then ( \\(\\sum_{u \\in V(G)} = \\deg_G(u) = 2|E(G)|\\) \\) ::: ::: The equivalent definition of this in text means, the sum of degree of all the vertices is twice the number of edges contained in it. Using the example from before, there's five vertices and a total degree of \\(10\\) . In general, any undirected graph has an even number of vertices of odd degree. -3ex -0.1ex -.4ex 0.5ex .2ex Walks, Paths, and Cycles Travel in a graph is accomplished by moving from one vertex to another along a sequence of adjacent edges. There various definitions used to describe the movement in a graph. ::: dBox ::: definitionT Definition 2.16 . Let \\(G\\) be a graph and let \\(u\\) and \\(v\\) be vertices in \\(G\\) . Walk: A walk from \\(u\\) to \\(v\\) is a finite sequence of adjacent vertices of \\(G\\) . Thus a walk has the form, \\(W = (v_0,v_1,v_2, \\cdots, v_n)\\) if \\(\\{v_i,v_{i+1}\\} \\in E(G)\\) for \\(0 \\leq i < n\\) , where \\(v_0 = u\\) and \\(v_n = v\\) . The length of a walk is the number of edges in \\(W\\) . Closed Walk: A closed walk is a walk that starts and ends at the same vertex, where \\(v_0 = v_n\\) . Path: A path is a walk without repeated vertices. The path of order \\(n \\geq 1\\) is denoted by \\(P_n\\) . Cycle: A cycle is a closed walk of at least \\(3\\) or more vertices. The cycle of order \\(n \\geq 3\\) is denoted by \\(C_n\\) . ::: ::: A path, \\(P_n\\) , where \\(n \\geq 1\\) , consist of \\(n\\) vertices and \\(n-1\\) edges. ::: center ::: A cycle, \\(C_n\\) , where \\(n \\geq 3\\) , consists of \\(n\\) vertices and \\(n\\) edges. ::: center ::: ::: exampleT Example 2.5 . Suppose we have the graph below, define the following walks: ::: center ::: ::: multicols 2 \\((v_0,v_1,v_2)\\) is a path, \\(P_3\\) , as neither vertices nor edges are repeated. ::: center ::: \\((v_2,v_6,v_4,v_5,v_1,v_2)\\) is a cycle, \\(C_5\\) , as we do not repeat a vertex nor edge, but started and ended at the same vertex. ::: center ::: ::: ::: We can now define what we mean by the diameter of a graph \\(G\\) . Note this will get a bit confusing. First, let's start with the distance between two vertices \\(u\\) and \\(v\\) . ::: dBox ::: definitionT Definition 2.17 (Graph distance). The distance between \\(u\\) and \\(v\\) is the minimum length of the paths in \\(G\\) connecting them, denoted by \\(d_G(u,v)\\) or \\(d(u,v)\\) , if \\(G\\) is clear from context. ::: ::: ::: list The distance between \\(u\\) and \\(v\\) is the same regardless of the start position, such that \\(d(u,v) = d(v,u)\\) . You can think of the distance as the number of edges traversed. ::: So in theory, what does this exactly mean? Let's use a simple graph for now. ::: center ::: The distance between \\(u\\) and \\(v\\) is \\(d(u,v) = 2\\) , as that's the only path to traverse. Another example, let's try and use the graph from before to see if you fully understand the definition. ::: center ::: As you can see, there are many possible paths from \\(u\\) to \\(v\\) , shown in red. Some examples are: ::: center ::: However, remember that we are only interested in the shortest path, which in this case is \\(d(u,v) = 3\\) . It's really important that you understand how to define the shortest path in \\(G\\) given two vertices \\(u\\) and \\(v\\) , as it will help in understand the next definition. ::: dBox ::: definitionT Definition 2.18 (Graph diameter). The diameter of a graph \\(G\\) is defined as ( \\(\\mathop{\\mathrm{diam}}(G) = \\max\\{d_G(v,w) \\mid v,w \\in V(G)\\}\\) \\) Equivalently, the largest number of vertices which must be traversed in order to travel from one vertex to another. ::: ::: ::: exampleT Example 2.6 . Consider the following graph \\(G\\) , determine the diameter of the graph: ::: center ::: We can start off by focusing on vertex \\(a\\) : ::: center ::: Then on vertex \\(b\\) : ::: center ::: Repeat for the rest of the vertices, for all \\(v,w \\in V(G)\\) . If you did it properly, you will see that \\(d(a,d)\\) and \\(d(f,e)\\) have the maximum distance. ( \\(\\mathop{\\mathrm{diam}}(G) = 3\\) \\) ::: You can also refer to this [video]{.underline} if you want a visual explanation of this example. -3ex -0.1ex -.4ex 0.5ex .2ex Subgraphs and Induced Subgraphs As the name suggests, the prefix \\\"sub\\\" refers to it being subsets of another graph. ::: dBox ::: definitionT Definition 2.19 (Subgraph). A graph \\(H\\) is said to be a subgraph of a graph \\(G\\) , written \\(H \\subseteq G\\) , if, and only if, \\(V(H) \\subseteq V(G)\\) and \\(E(H) \\subseteq E(G)\\) . The graph \\(H\\) is a spanning subgraph of \\(G\\) if \\(V(H) = V(G)\\) . ::: ::: It may be easier to understand visually. Suppose we have the following graph \\(G\\) : ::: center ::: As an example, each of the graphs are variations of graph \\(H\\) , which are subgraphs of graph \\(G\\) : ::: center ::: We only consider it to be spanning subgraph if and only if \\(V(H) = V(G)\\) , which in this case, if \\(V(H) = \\{a,b,c,d,e,f,g,h,i,j\\}\\) , which none of them are. On the other hand, these graphs are spanning subgraphs of \\(G\\) : ::: center ::: A final concept is induced subgraphs, which consists of the following property. ::: dBox ::: definitionT Definition 2.20 (Induced subgraph). If \\(S \\subseteq V(G)\\) , then the subgraph of \\(G\\) induced by \\(S\\) , denoted by \\(G[S]\\) , has vertices \\(S\\) and edges are those of \\(G\\) with endpoints in \\(S\\) . ::: ::: Note that none of the subgraphs shown so far are considered induced subgraphs of \\(G\\) . Though, we can modify it slightly to make it an induced subgraph, indicated by the red line. ::: center ::: Just focus on the induced subgraph in the far left. Notice how every possible edge that exists in graph \\(G\\) between the vertices, \\(\\{b,d,e,f,g,h,i,j\\}\\) , exists in this subgraph, thus making it an induced subgraph. Likewise, how the edge \\(a,b\\) is not here because \\(a\\) is not in the subset of vertices. -3ex -0.1ex -.4ex 0.5ex .2ex Special Graphs We consider some important examples of graphs. One important class of graphs consists of those that do not have any loops or parallel edges. ::: dBox ::: definitionT Definition 2.21 (Simple graph). A simple graph is a graph that does not have any loops or parallel edges. In a simple graph, an edge with endpoints \\(v\\) and \\(w\\) is denoted \\(\\{v,w\\}\\) . ::: ::: The following graphs can be depicted as simple graphs: ::: center ::: Another important class of graphs consists of those that are \"complete\" in the sense. ::: dBox ::: definitionT Definition 2.22 (Complete graph). A complete graph on \\(n\\) vertices, denoted \\(K_n\\) , is a simple graph with \\(n\\) vertices and exactly one edge connecting each pair of distinct vertices. ::: ::: This may sound confusing at first, but look at the following graphs below to get a general idea of how it works. ::: center ::: Then another class of graphs we can consider are complete bipartite graphs, which are as follows. ::: dBox ::: definitionT Definition 2.23 (Complete bipartite graph). A complete bipartite graph, denoted \\(K_{m.n}\\) , is a simple graph that has its vertex set partitioned into two subsets of \\(m\\) and \\(n\\) vertices. ::: ::: Note the difference between \\(K_5\\) and \\(K_{3,2}\\) , where there are no edges connected between \\(v_1\\) , \\(v_2\\) and \\(v_3\\) or similarly with \\(w_1\\) and \\(w_2\\) , which would otherwise just make it a complete graph. ::: center ::: The dashed-lines highlights the partition of two subsets. If there are some edges not present between the parts, then the graph is just a bipartite graph. -4ex -1ex -.4ex 1ex .2ex Functions If you think about it, functions are a special kind of binary relation. By definition: ::: dBox ::: definitionT Definition 2.24 (Function). A function \\(f\\) is a binary relation from sets \\(X\\) and \\(Y\\) . For each \\(x \\in X\\) , there is a unique \\(y \\in Y\\) , so that \\(x \\mathrel{f}y\\) . We write \\(f(x) = y\\) for \\(x \\mathrel{f}y\\) and say \\\" \\(f\\) of \\(x\\) equals \\(y\\) .\\\" Denoted as \\(f: X \\to Y\\) , it is a relation from \\(X\\) , the domain of \\(f\\) , to \\(Y\\) , the co-domain of \\(f\\) . ::: ::: Using an arrow diagrams, we can define a function by the following: Every element of \\(X\\) has an arrow that points to an element in \\(Y\\) . No element of \\(X\\) has two arrows that points to two different elements in \\(Y\\) . We can also define the range of a function \\(f\\) , which is a subset of the co-domain. ::: dBox ::: definitionT Definition 2.25 (Range). Let \\(f: X \\to Y\\) be a function. The range of \\(f\\) is: ( \\(\\{y \\in Y \\mid \\text{for some } x \\in X, f(x) = y\\}\\) \\) ::: ::: ::: exampleT Example 2.7 . Suppose a function \\(f\\) is defined from \\(X\\) to \\(Y\\) by the following arrow diagram: ::: center ::: The domain of \\(f = \\{a,b,c\\}\\) and co-domain of \\(f = \\{1,2,3,4\\}\\) . The range of \\(f\\) equals \\(\\{2,4\\}\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Functions Acting on Sets You can consider the set of images in \\(Y\\) of all the elements in a subset of \\(X\\) and the set of inverse images in \\(X\\) of all the elements in a subset of \\(Y\\) . ::: dBox ::: definitionT Definition 2.26 (Image and Inverse Image). Let \\(f: X \\to Y\\) be a function and \\(A \\subseteq X\\) and \\(C \\subseteq Y\\) . The image of \\(A\\) , denoted by \\(f(A)\\) , is ( \\(f(A) = \\{y \\in Y \\mid \\text{for some } x \\in A, f(x) = y\\}\\) \\) The inverse image of \\(C\\) , denoted by \\(f^{-1}(C)\\) , is ( \\(f^{-1}(C) = \\{x \\in X \\mid f(x) \\in C\\}\\) \\) ::: ::: Using the example from before, it might be easier to understand what these definition represent: \\(f(a) = 2\\) , \\(f(b) = 4\\) , and \\(f(c) = 2\\) \\(f^{-1}(1) = \\varnothing\\) , \\(f^{-1}(2) = \\{a,c\\}\\) , \\(f^{-1}(3) = \\varnothing\\) , and \\(f^{-1}(4) = \\{b\\}\\) Note that \\(C\\) is a subset of \\(Y\\) , so you maybe asked to find the inverse image of more than one element. For example, let \\(C = \\{2,4\\}\\) , then \\(f^{-1}(C) = \\{a,b,c\\}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex One-to-One, Onto and Inverse Functions We'll now discuss two important properties that functions may satisfy: the property of being one-to-one and the property of being onto. ::: dBox ::: definitionT Definition 2.27 (One-to-one). A function \\(f: X \\to Y\\) is one-to-one (or injective ) if for all \\(x_1,x_2 \\in X\\) , such that \\(x_1 \\neq x_2\\) , we have \\(f(x_1) \\neq f(x_2)\\) . If any two distinct elements of \\(X\\) are sent to two distinct elements of \\(Y\\) , then it is one-to-one. ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.28 (Onto). A function \\(f: X \\to Y\\) is one-to-one (or surjective ) if for all \\(y \\in Y\\) , there exists \\(x \\in X\\) , such that \\(f(x) = y\\) . If each elements of \\(Y\\) equals \\(f(x)\\) for at least one \\(x\\) in \\(X\\) , then it is onto. ::: center ::: ::: ::: For finite sets, it is pretty easy to determine whether a function is one-to-one or onto, just from the arrow diagram above. The tricky part comes when analyzing a function for an infinite set. ::: exampleT Example 2.8 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = 4x - 1\\) . Is \\(f\\) is onto? In order to prove something is onto, we need show that there exists a real number \\(x\\) , such that \\(y = 4x-1\\) . We can do so, by solving for \\(x\\) in this case, where \\(x = (y - 1)/4\\) . If you notice, \\(y\\) is not restricted to anything, meaning that \\(y\\) can be any \\(\\mathbb{R}\\) . Equivalently, we are saying \\\"There exists an \\(x\\) (which we determined from 3.), which is being mapped \\(\\forall y \\in \\mathbb{R}\\) .\\\" ::: ::: exampleT Example 2.9 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = x^2\\) . Is \\(f\\) one-to-one? Consider \\(f(x_1) = f(x_2)\\) , We need to show that \\(x_1 = x_2\\) . If we fail to prove this, then it is not one-to-one. Alternatively, \\((2)^2 = (-2)^2 = 4\\) , but \\(2 \\neq -2\\) , so the function is not one-to-one. ::: There also exist functions which satisfy both properties discussed. ::: dBox ::: definitionT Definition 2.29 (One-to-one correspondence). A function \\(f: X \\to Y\\) is bijective (or bijective) if it is both one-to-one and onto. ::: center ::: ::: ::: This will aid us in defining a type of function known as the inverse function, which undoes the action of \\(f\\) . It sends each element of \\(Y\\) back to the element of \\(X\\) where it came from. ::: dBox ::: definitionT Definition 2.30 (Inverse function). Let \\(f: X \\to Y\\) be a one-to-one correspondence. The inverse function of \\(f\\) is denoted by \\(f^{-1}\\) , where \\(f^{-1}: Y \\to X\\) . ::: center ::: ::: ::: Obtaining the inverse function should be something you are all familiar with, it just been described in a different setting, which is by solving for \\(x\\) . ::: exampleT Example 2.10 . Is \\(y = 4x-1\\) a bijection (or a one-to-one correspondence) from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) ? If so, find the inverse \\(f^{-1}(x)\\) . To check if it is one-to-one, show that \\(f(x_1) = f(x_2) \\Longrightarrow x_1 = x_2\\) . The function is one-to-one, since \\(4x_1 - 1 = 4x_2 - 1 \\Longrightarrow x_1 = x_2\\) . To check if it is onto, we must show that for every \\(y \\in \\mathbb{R}\\) , there is an \\(x \\in \\mathbb{R}\\) , such that \\(y = f(x)\\) . We can prove this, by solving for \\(x\\) . ( \\(x = \\frac{y+1}{4}\\) \\) There exists some \\(x \\in \\mathbb{R}\\) , such that \\(f(x) = y\\) , which makes the function is onto. To get the inverse function, we can use the equation we obtained in the previous one and interchange \\(x\\) and \\(y\\) : ( \\(y = f^{-1}(x) = \\frac{x+1}{4}\\) \\) ::: Number Theory and Combinatorics \u00b6 -4ex -1ex -.4ex 1ex .2ex Elementary Number Theory The underlying content of this section consists of properties of integers, rational numbers, and real numbers. -3ex -0.1ex -.4ex 0.5ex .2ex Rational Number Sums, differences, and products of integers are integers, but most quotients of integers are not integers, rather known as: ::: dBox ::: definitionT Definition 3.1 (Rational Number). A real number \\(r\\) is rational, if and only if, it can be expressed as a quotient of two integers with a nonzero denominator. ( \\(r \\text{ is rational } \\Leftrightarrow \\exists \\text{ integers } a \\text{ and } b \\text{ such that } r = \\frac{a}{b} \\text{ and } b \\neq 0\\) \\) ::: ::: Some examples of rational numbers are \\(0\\) , \\(1\\) , and \\(1/3\\) . While a real number that is not rational is called an irrational numbers, like \\(\\pi\\) , \\(e\\) , and \\(\\sqrt{2}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Parity Number The parity of an integer focuses on the property of a number being even or odd. ::: dBox ::: definitionT Definition 3.2 (Parity). An integer \\(x\\) is even if \\(x = 2a\\) , for some integer \\(a\\) . An integer \\(x\\) is odd if \\(x = 2b + 1\\) , for some integer \\(b\\) . ::: ::: ::: tBox ::: theoremeT Theorem 3.1 (Properties of parity). Let \\(x\\) and \\(y\\) be integers: If \\(x\\) and \\(y\\) are both, then so are \\(x + y\\) and \\(xy\\) . If \\(x\\) is even and \\(y\\) is odd, then \\(x + y\\) is odd. If \\(x\\) is odd and \\(y\\) is odd, then \\(x + y\\) is even. We have that \\(x\\) is even if and only if \\(x^2\\) is even. ::: ::: Alternatively, the arithmetic on the even and odd numbers can depicted as: Even \\(+\\) Even \\(\\to\\) Even Even \\(+\\) Odd \\(\\to\\) Odd Odd \\(+\\) Odd \\(\\to\\) Even Even \\(\\times\\) Even \\(\\to\\) Even Even \\(\\times\\) Odd \\(\\to\\) Even Odd \\(\\times\\) Odd \\(\\to\\) Odd -4ex -1ex -.4ex 1ex .2ex Divisors Divisors play a central role in number theory, as they help us define prime numbers and the Euclidean algorithm, which will discuss after. ::: dBox ::: definitionT Definition 3.3 (Divisibility). The notation \\(a \\mid b\\) is read \\\" \\(a\\) divides \\(b\\) \\\", if \\(b = ak\\) , for some integer \\(k\\) . We can say that ::: description \\(b\\) is a multiple of \\(a\\) , or \\(b\\) is divisible by \\(a\\) , or \\(a\\) is a factor of \\(b\\) , or \\(a\\) is a divisor of \\(b\\) ::: ::: ::: One useful trick for checking divisibility when it comes to large numbers is by checking if the sum of its individual digit is divisible by instead. Refer to the example below. ::: exampleT Example 3.1 . Is \\(94\\;417\\;898\\;732\\) divisible by \\(9\\) ? Let's start calculating the sum of its digits ( \\(9+4+4+1+7+8+9+8+7+3+2 = 62\\) \\) Since there exist no integers \\(k\\) which satisfy the following equation, \\(62 = 9k\\) , it is not divisible by \\(9\\) . ::: Following this, we can now define what the greatest common divisor of two integers is. ::: dBox ::: definitionT Definition 3.4 (Greatest common divisor). The greatest common divisor of nonzero integers \\(a\\) and \\(b\\) , denoted \\(\\gcd(a,b)\\) , is the largest integer that divides both \\(a\\) and \\(b\\) . ::: ::: Note that every integer divides \\(0\\) , since \\(0 = a \\times 0\\) where \\(k = 0\\) . ::: exampleT Example 3.2 . Find the \\(\\gcd(72,63)\\) : The divisor of \\(72\\) are \\(\\{1,2,3,6,8,9,12,18,24,36,72\\}\\) . The divisor of \\(63\\) are \\(\\{1,3,7,9,21,63\\}\\) . The largest integer that divides both integers is \\(9\\) , such that \\(9 \\mid 72\\) and \\(9 \\mid 63\\) . Hence, \\(\\gcd(72,63) = 9\\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Prime Numbers We can also use divisors as a way to define a prime number. ::: dBox ::: definitionT Definition 3.5 (Prime number). A number \\(p > 1\\) is prime if, and only if, its only positive integer divisors are \\(1\\) and itself. Otherwise, it's composite. ::: ::: The most comprehensive statement about divisibility of integers is contained in the factorization of integers theorem. ::: tBox ::: theoremeT Theorem 3.1 (Fundamental Theorem of Arithmetic). Every integer \\(n>1\\) equals a product of primes, which is unique up to the ordering of factors. ::: ::: For example, \\(72\\) can be written as, \\(2^33^2\\) , where \\(2\\) and \\(3\\) are prime numbers. In a way you can think of each number as made up of building blocks of prime number. -3ex -0.1ex -.4ex 0.5ex .2ex Euclidean Algorithm The Euclidean algorithm provides us a simpler method for deriving the greatest common divisor of two positive integers. It is based on these two key facts: If \\(r\\) is a positive integer, then \\(\\gcd(r,0) = r\\) . If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , and if \\(q\\) and \\(r\\) are integers such that \\(a = bq + r\\) , then \\(\\gcd(a,b) = gcd(b,r)\\) The first fact should be fairly straightforward to understand. The second fact might be harder to understand, so let's use an example. ::: exampleT Example 3.3 . Find the \\(\\gcd(72,63)\\) : Let \\(a = 72\\) and \\(b = 63\\) , then we rewrite it in the form of \\(72 = 63q + r\\) . You can think of \\(q\\) as the quotient and \\(r\\) as the remainder. ( \\(72 = 63(1) + 9\\) \\) Then \\(\\gcd(72,63) = \\gcd(63,9)\\) . Let \\(a = 63\\) and \\(b = 9\\) , where \\(63 = 9q + r\\) . ( \\(63 = 9(7) + 0\\) \\) Then \\(\\gcd(63,9) = \\gcd(9,0)\\) . Using the first key fact, we know \\(\\gcd(9,0) = 9\\) . ::: Alternatively, we can set \\(q = 1\\) , where \\(r = a - b\\) . ::: tBox ::: theoremeT Theorem 3.1 (Reducing gcd). If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , then: ( \\(\\gcd(a,b) = \\gcd(b, a-b)\\) \\) ::: ::: ::: list Keep in mind, \\(\\gcd(a,b) = \\gcd(b,a)\\) , so alternatively can be written as \\(\\gcd(a, b-a)\\) . Also note that \\(\\gcd(a,b) = \\gcd(|a|,|b|)\\) . ::: Using this theorem, we can continue to reduce greatest common divisor, until we either have an integer simple enough to work with or it results in the form of \\(\\gcd(r,0)\\) . -4ex -1ex -.4ex 1ex .2ex Linear Diophantine Equation We focus on equations with integer coefficients and integer solutions. ::: dBox ::: definitionT Definition 3.6 (Linear Diophantine equation (LDE)). An equation of the form \\(ax + by = c\\) , or can also be written as \\(ax = c\\ (\\mathrm{mod}\\ b)\\) , where \\(a\\) , \\(b\\) , \\(c\\) , \\(x\\) , \\(y \\in \\mathbb{Z}\\) . ::: ::: In this section, we try to answer the following problem. ::: problem Problem 3.1 . Given an integer \\(a\\) , \\(b\\) , and \\(c\\) , does a solution \\((x,y)\\) exist and how can you find a solution to an LDE? ::: So how exactly can we prove whether a solution exists? It all relates back to the greatest common divisor, more specifically \\(\\gcd(a,b)\\) . A useful theorem which we'll use in proving this is B\u00e9zout's identity. ::: tBox ::: theoremeT Theorem 3.1 (B\u00e9zout's identity). Let \\(a\\) and \\(b\\) be nonzero integers, and let \\(d = \\gcd(a,b)\\) . Then there exist integers \\(m\\) and \\(n\\) that satisfy: ( \\(ma + nb = d\\) \\) ::: ::: So for an LDE to have a solution, \\(c\\) must be a multiple of \\(d\\) , denoted as \\(d \\mid c\\) , which can be written more formally as: ::: tBox ::: theoremeT Theorem 3.1 (Check if solution exists for LDE). Let \\(d = \\gcd(a,b)\\) . The LDE \\(ax + by = c\\) has a solution if and only if \\(d \\mid c\\) . ::: ::: ::: exampleT Example 3.4 . Does a solution exists to the LDE \\(60x + 33y = 9\\) ? Compute the greatest common divisor of \\(60\\) and \\(33\\) . ( \\(\\gcd(60,33) = \\gcd(33,27) = \\gcd(27,6) \\gcd(6,3) = \\gcd(3,0) = 3\\) \\) Determine whether a solution exists, if \\(\\gcd(12,8) = 4 \\mid 68\\) or there exists an integer \\(k\\) , where ( \\(3 \\mid 9 \\Longleftrightarrow 9 = 3k\\) \\) which is true for \\(k = 3\\) , thus a solution exist. ::: Once it's determined a solution exists for the LDE, we want to way to derive the general solution \\((x,y)\\) , which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Solutions to LDE). Let \\(d = \\gcd(a,b)\\) where \\(a \\neq 0\\) and \\(b \\neq 0\\) . If \\((x,y) = (x_0,y_0)\\) is a solution to the LDE \\(ax + by = c\\) , then all solutions are given by ( \\(x = x_0 + \\frac{b}{d}t \\qquad y = y_0 - \\frac{a}{d}t\\) \\) for all \\(t \\in \\mathbb{Z}\\) . We may write the solution set as ( \\(\\Big\\{\\Big(x_0 + \\frac{b}{d}t\\Big), \\Big(y_0 - \\frac{a}{d}t\\Big) : t \\in \\mathbb{Z}\\Big\\}\\) \\) ::: ::: ::: exampleT Example 3.5 . Solve the following LDE \\(60x + 33y = 9\\) or \\(60x = 9\\ (\\mathrm{mod}\\ 33)\\) . As we proved before, a solution exists where \\(d = \\gcd(60,33) = 3\\) . First step is a finding a solution to B\u00e9zout's identity, where there exists integers \\(m\\) and \\(n\\) that satisfy \\(am + bn = d\\) . ( \\(60m + 33n = 3\\) \\) such that \\(m = 5\\) and \\(n = -9\\) satisfies this equation. Refer to the section after. Then to get \\(x_0\\) and \\(y_0\\) , we multiply \\(m\\) and \\(n\\) by \\(3\\) to get the original LDE equation. ( \\(3\\big[60n + 33n = 3\\big] = 60(3n) + 33(3n) = 9\\) \\) such that \\(x_0 = 15\\) and \\(y_0 = -27\\) satisfies the equation \\(60x_0 + 33y_0 = 9\\) . Finally, we just apply the theorem to get the general solution for all \\(t \\in \\mathbb{Z}\\) ( \\(x = 15 + \\frac{33}{3}t = 15 + 11t \\qquad y = -27 - \\frac{60}{3}t = -27 - 20t\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Solving for Initial Solution The trickiest part is solving for integers \\(m\\) and \\(n\\) which satisfies B\u00e9zout identity to get the initial solutions \\(x_0\\) and \\(y_0\\) . Let's use the previous example, \\(60m + 33n = 9\\) . The process involves by working backwards through your steps in the Euclidean Algorithm: \\(60 = 33(1) + 27\\) \\(33 = 27(1) + 6\\) \\(27 = 6(4) + 3\\) Reformat the Euclidean Algorithm, such that \\(r = a - bq\\) : \\(27 = 60 - 33(1) \\hfill (3.5)\\) \\(6 = 33 - 27(1) \\hfill (3.6)\\) \\(3 = 27 - 6(4) \\hfill (3.7)\\) Now use substitution. Refer to the text in red: \\( \\(3 = 27 - {\\color{red}6}(4)\\) \\) \\( (3 = 27 - \\big {\\color{red}33 - 27(1)}\\big \\tag*{Substitute \\(6\\) using Eq. \\(3.6\\) }\\) \\) \\( \\(3 = 27 - 33(4) + 27(4) \\tag*{Expand}\\) \\) \\( \\(3 = {\\color{red}27}(5) - 33(4) \\tag*{Combine like terms}\\) \\) \\( (3 = \\big {\\color{red}60 - 33(1)}\\big - 33(4) \\tag*{Substitute \\(27\\) using Eq. 3.5}\\) \\) \\( \\(3 = 60(5) - 33(5) - 33(4) \\tag*{Expand}\\) \\) \\( \\(3 = 60(5) - 33(9) \\tag*{Combine like terms}\\) \\) \\( \\(3 = 60(5) + 33(-9)\\) \\) -4ex -1ex -.4ex 1ex .2ex Congruence In number theory, congruence is nothing more than a statement about divisibility. ::: dBox ::: definitionT Definition 3.7 (Congruence). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) , if \\(a\\) is congruent to \\(b\\) modulo \\(n\\) , then we write ( \\(a \\equiv b\\ (\\mathrm{mod}\\ n)\\) \\) which provides that \\(n \\mid (a - b)\\) . ::: ::: So what information can we take away from \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) ? \\(a\\) and \\(b\\) have the same remainder when divided by \\(n\\) \\(a = kn + b\\) for some integer \\(k\\) \\(n \\mid (a-b)\\) There are also some useful algebraic properties of congruences. ::: tBox ::: theoremeT Theorem 3.1 . If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(c \\equiv d \\ (\\mathrm{mod}\\ n)\\) , then: \\(a + c \\equiv b + d \\ (\\mathrm{mod}\\ n)\\) \\(a - c \\equiv b - d \\ (\\mathrm{mod}\\ n)\\) \\(ac \\equiv bd \\ (\\mathrm{mod}\\ n)\\) ::: ::: The algebra of congruence is sometime referred to as clock arithmetic. For example, we can represent modulo \\(12\\) as a clock (where \\(0\\) represents \\(12\\) ). ::: center ::: The clock demonstrate that every integer is congruent to at least one of \\(0 \\dots 11\\) modulo \\(12\\) (row highlighted in pink). Just like a clock, when we go over \\(12\\) , we start over at \\(1\\) , and so the same thing applies with modulo, where \\(1 \\equiv 13 \\ (\\mathrm{mod}\\ 12)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Congruence Class Refer back to [section 2.1.2]{.underline} for a recap. Congruence is another type of equivalence relationsa relation that satisfies all three: Reflexive: \\(a \\equiv a \\ (\\mathrm{mod}\\ n)\\) Symmetric: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) , then \\(b \\equiv a \\ (\\mathrm{mod}\\ n)\\) Transitive: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(b \\equiv c \\ (\\mathrm{mod}\\ n)\\) , then \\(a \\equiv c \\ (\\mathrm{mod}\\ n)\\) As a result, we can form equivalence classes, or otherwise known as ::: dBox ::: definitionT Definition 3.8 (Congruence class). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . The congruence class of modulo \\(n\\) is ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b \\equiv a \\ (\\mathrm{mod}\\ n)\\}\\) \\) Note that ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b = a + kn \\text{ for } k \\in \\mathbb{Z}\\}\\) \\) ::: ::: For example, in congruence modulo \\(2\\) , we have \\([0]_2 = \\{0, \\pm 2, \\pm 4, \\pm 6, \\cdots\\}\\) \\([1]_2 = \\{\\pm 1, \\pm 3, \\pm 5, \\pm 7, \\cdots\\}\\) The congruence classes of \\(0\\) and \\(1\\) are, respectively, the sets of even and odd integers. ::: tBox ::: theoremeT Theorem 3.1 (Equality of congruence classes). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . We then have that \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) if and only if ( \\(_n = [b]_n\\) \\) ::: ::: Referring back to the clock diagram, in congruence modulo \\(12\\) , we have: \\( \\([0]_{12} = [12]_{12} = [24]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\} = \\{\\cdots -24, -12, 0, 12, 24, \\cdots\\}\\) \\) You may have notice that the distinct congruence classes are \\([0], [1], \\cdots, [11]\\) : \\([0]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\}\\) \\([1]_{12} = \\{1, 1 \\pm 12, 1 \\pm 24, \\cdots\\}\\) \\(\\vdots\\) \\([11]_{12} = \\{11, 11 \\pm 12, 11 \\pm 24, \\cdots\\}\\) In congruence modulo \\(n\\) , we can say for \\(n > 2\\) , the distinct congruence classes are \\([0], [1], \\cdots, [n-1]\\) . -4ex -1ex -.4ex 1ex .2ex Principles of Counting We'll start off with the fundamentals, that is counting. Of course, most people know how to count, but combinatorics applies mathematical operations to count quantities that are much too large to be counted the conventional way. -3ex -0.1ex -.4ex 0.5ex .2ex Sum Rule Combinatorics is often concerned with how things are arrangeda way objects could be grouped. One of the most basic rules regarding arrangements is the rule of sum. ::: tBox ::: theoremeT Theorem 3.1 (Sum rule). Suppose that we are given disjoint sets \\(X\\) and \\(Y\\) . If \\(|X| = m\\) and \\(|Y| = n\\) , then ( \\(|X\\ \\cup\\ Y | = m + n\\) \\) ::: ::: Then we can generalized this theorem for more than two disjoints sets. ::: tBox ::: theoremeT Theorem 3.1 (Generalized sum rule). If we are given pairwise disjoint sets \\(X_i\\) , where \\(1 \\leq i \\leq m\\) , so that \\(|X_i| = m\\) , then ( \\(\\bigg|\\bigcup\\limits_{i=1}^m X_i \\bigg| = \\sum_{i=1}^m |X_i|\\) \\) ::: ::: So what about sets that are not disjoint? For example, we have two sets \\(X = \\{1,2,3\\}\\) and \\(Y = \\{2,3,4\\}\\) . If we use the sum rule, where \\(|X| = 3\\) and \\(|Y| = 3\\) , we should get: \\( \\(|X| + |Y| = 3 + 3 = 6\\) \\) However, \\(X\\ \\cup\\ Y = \\{1,2,3,4\\}\\) , where \\(|X \\cup Y| = 4 \\neq 6\\) , such that \\( \\(|X\\ \\cup\\ Y| < |X| + |Y|\\) \\) which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Boole's inequality). For any sets \\(A_i\\) , we have that ( \\(\\bigg|\\bigcup\\limits_{i=1}^m A_i \\bigg| \\leq \\sum_{i=1}^m |A_i|\\) \\) ::: ::: Equivalently, we can rewrite the sum rule as \\(|A\\ \\cup\\ B| = |A| + |B| - |A\\ \\cap\\ B|\\) , where we subtract the cardinality of elements that are common. For disjoint sets that is \\(\\varnothing\\) , compared to joint sets, resulting in \\(\\leq\\) . And so this brings the final theorem which will cover in this section. ::: tBox ::: theoremeT Theorem 3.1 (Principle of Inclusion-Exclusion). Let \\(X_1, X_2, \\dots, X_n\\) be finite sets. We then have that ( \\(\\bigg|\\bigcup\\limits_{1\\leq1\\leq n} X_i \\bigg| = |X_1| + |X_2| + \\dots + |X_n|\\) \\) ( \\(\\qquad\\qquad \\:-\\;|X_1\\ \\cap\\ X_2| - |X_1\\ \\cap\\ X_3| - \\dots - |X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;|X_1\\ \\cap\\ X_2\\ \\cap\\ X_3| + |X_1\\ \\cap\\ X_2\\ \\cap\\ X_4| + \\dots + |X_{n-2}\\ \\cap\\ X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;(-1)^{n-1}|X_1\\ \\cap\\ X_2\\ \\cap\\ \\cdots\\ \\cap\\ X_n|\\) \\) ::: ::: We can break this principle line-by-line: Take the sum of the cardinalities of the sets, as you would in a disjoint union. Subtract off the cardinalities of the pairwise intersections of the sets Add the cardinalities of the triple intersections and so on. The signs \\((-1)^{n-1}\\) depend on the parity of the number of sets intersected. For example, if there is three set in the intersection ::: center ::: \\( \\(|A\\ \\cup\\ B\\ \\cup\\ C| = |A| + |B| + |C| - |A\\ \\cap\\ B| - |A\\ \\cap\\ C| - |B\\ \\cap\\ C| + |A\\ \\cap\\ B\\ \\cap C|\\) \\) You can also refer to the visualization below, if you have trouble understanding the reason behind it ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Product Rule Another basic rules regarding arrangements is the rule of product. ::: tBox ::: theoremeT Theorem 3.1 (Product rule). If a task \\(X\\) can be performed in \\(m\\) ways and a task \\(Y\\) can be performed in \\(n\\) ways, then we have that \\(X\\) and \\(Y\\) can be performed together in \\(mn\\) ways. ::: ::: One example is with cards. How many cards are in a standard deck of cards? ::: center ::: Equivalently, you can think of the suit of the card and the rank of the card as two tasksthere are \\(4\\) suits and \\(13\\) . The product rule, \\(4 \\times 13\\) , tells us there are \\(52\\) card. Likewise, we can generalize this theorem for more than two tasks. ::: tBox ::: theoremeT Theorem 3.1 (Generalized product rule). If tasks \\(X_i\\) can be performed in \\(m_i\\) ways where \\(i \\leq i \\leq n\\) , then we have ( \\(m_1m_2 \\cdots m_n\\) \\) way tasks can be performed together. ::: ::: ::: exampleT Example 3.6 . How many ways can you make a license plate with three-digit number (not including zero) and three letters? For starters, let's focus on the three-digit number. For each digit, there can be nine different ways, \\(1 \\dots 9\\) , we can choose a number. ( \\(9 \\times 9 \\times 9 = 9^3\\) \\) Then for the three letters, for each choice, there can be twenty-six different ways, \\(a \\dots z\\) , we can choose a letter. ( \\(26 \\times 26 \\times 26 =26^3\\) \\) In total, there are \\(9^326^3\\) different ways we can make them. ::: -3ex -0.1ex -.4ex 0.5ex .2ex The Pigeonhole Principle The Pigeonhole Principle is a simple, but powerful tool when counting objects. The metaphors used to describe the principle typically vary, but they all follow the same analogy of inserting a finite set into a smaller finite set. You can think of it like this, if \\(n\\) pigeons fly into \\(m\\) pigeonholes and \\(n > m\\) , then at least one hole must contain two or more pigeons. ::: center ::: ::: tBox ::: theoremeT Theorem 3.1 (The Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , then at least one container must contain more than one item. ::: ::: In hindsight, it is pretty is obvious, thus, we move immediately to applicationsranging from the totally obvious to the extremely subtle. ::: exampleT Example 3.7 . If you choose a set of three non-negative integers, must there be at least two who are both even or both odd. Yes, because we have three items and only two container (odd or even), therefore one container must contain more than one item, which could be odd or even. ::: ::: exampleT Example 3.8 . Let \\(A = \\{1,2,3,4,5,6,7,8\\}\\) . If five integers are selected form \\(A\\) , must at least one pair of the integers have a sum of \\(9\\) ? You can think of the five selected integers as the number of items, where \\(a_n\\) represent a distinct number in the set \\(A\\) . ( \\(a_1,\\ a_2,\\ a_3,\\ a_4, \\text{ and }\\ a_5\\) \\) Then, all the disjoint subsets that have a sum of \\(9\\) is our container. ( \\(\\{1,8\\},\\ \\{2,7\\},\\ \\{3,6\\}, \\text{ and}\\ \\{4,5\\}\\) \\) Applying the pigeonhole principle, because there are more items, \\(n = 5\\) , than there are containers, \\(m = 4\\) . Then at least one container must contain more than one item. In other words, at least one of the disjoint subsets will contain two distinct integers, which will have a sum of \\(9\\) . ::: A generalization of the pigeonhole principle states: ::: tBox ::: theoremeT Theorem 3.1 (Generalized Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , there is at least one container with \\(\\lceil n/m \\rceil\\) items. ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Permutation Before going over combinations, let's talk about permutation, where order matters. You can think of it as an ordered combinations. For example, the set of elements \\(a\\) , \\(b\\) , and \\(c\\) has six permutations: \\( \\(abc \\hspace{1cm} acb \\hspace{1cm} bac \\hspace{1cm} bca \\hspace{1cm} cab \\hspace{1cm} cba\\) \\) The number of permutations can be derived using the product rule. Suppose we have a set of \\(n\\) elements: For our first choice (or task), we have \\(n\\) ways of picking an element. For our second choice, we now have \\(n-1\\) ways of picking an element \\(\\vdots\\) For our \\(n\\) th choice, there's only one element left, so we only have \\(1\\) way of choosing an element. So by the product rule, there are \\( \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 = n!\\) \\) ways to perform the entire operation with no repetitions. In other words, there are \\(n!\\) permutations of a set of \\(n\\) elements. Suppose in the previous example, we want to know how many permutation there one only using two elements, instead of all three. We can define an ordered arrangement of \\(r\\) elements taken from the set of \\(n\\) elements as an \\(r\\) -permutation. ::: dBox ::: definitionT Definition 3.9 (Permutation). The number of \\(r\\) -permutations of a set of \\(n\\) elements is denoted \\(P(n,r)\\) . If \\(0 \\leq r \\leq n\\) , then ( \\(P(n,r) = n \\times (n-1) \\times \\cdots \\times (n - r + 1) = \\frac{n!}{(n-r)!}\\) \\) ::: ::: So now can calculate the \\(2\\) -permutation of \\(\\{a,b,c\\}\\) , resulting in \\( \\(P(3,2) = \\frac{3!}{(3-2)!} = \\frac{3!}{1!} = \\frac{6}{1} = 6\\) \\) which are \\( \\(ab \\hspace{1cm} ac \\hspace{1cm} ba \\hspace{1cm} bc \\hspace{1cm} ca \\hspace{1cm} cb\\) \\) -4ex -1ex -.4ex 1ex .2ex Combination We can now define an unordered arrangement of \\(r\\) elements of a set as an \\(r\\) -combination. ::: dBox ::: definitionT Definition 3.10 (Combination). Let \\(n\\) and \\(r\\) be non-negative integers, with \\(r \\leq n\\) . The symbol \\(\\binom{n}{r}\\) , read \\\" \\(n\\) chooses \\(r\\) \\\", denotes the number of subsets of size \\(r\\) that can be formed from a set of \\(n\\) elements. ::: ::: ::: list There's two ways to denote an \\(r\\) -combination, which is by \\(\\binom{n}{r}\\) or \\(C(n,r)\\) ::: Using the relation between permutation \\(P(n.r)\\) and combination, gives us an important formula: \\( \\(C(n.r) = \\binom{n}{r} = \\frac{n!}{(n-r)!r!}\\) \\) ::: exampleT Example 3.9 . Given a set of four people: Ann, Bob, Cyd, and Dan. List all the combinations that can be made with only three people. Note that order doesn't matter, so a subset consisting of \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) is the same as the subset consisting of \\(\\{\\) Dan, Cyd, Bob \\(\\}\\) . Following this fact, then the \\(3\\) -combination can be obtained by leaving one out of the elements of the set: \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Cyd \\(\\}\\) or alternatively ( \\(C(4,3) = \\binom{4}{3} = \\frac{4!}{(4-3)!3!} = \\frac{24}{6} = 4\\) \\) ::: As a follow up, there are special cases of combinations using this equation. ::: tBox ::: theoremeT Theorem 3.1 (Basic properties of combination). Let \\(n\\) be an integer: \\(\\displaystyle\\binom{n}{0} = \\binom{n}{n} = 1\\) \\(\\displaystyle\\binom{n}{1} = \\binom{n}{n-1} = n\\) \\(\\displaystyle\\binom{n}{2} = \\frac{n(n-1)}{2}\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Combinations There also some useful of identities that you can form using \\(C(n,r)\\) . They seem mysterious at first, but there's usually a good reason for them. Combinations have a recursive quality that is captured in the following theorem. ::: tBox ::: theoremeT Theorem 3.1 (Recursive property). For integers \\(n \\geq 1\\) and \\(r \\geq 1\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n - 1}{r - 1} + \\binom{n - 1}{r}\\) \\) ::: ::: You will often see this depicted as Pascal's formula. As an example, we can calculate \\(\\binom{6}{2}\\) using: \\( \\(\\binom{6}{2} = \\binom{5}{1} + \\binom{5}{2} = 5 + 10 = 15\\) \\) Another important property of combinations is their symmetry. ::: tBox ::: theoremeT Theorem 3.1 (Symmetry property). For integers \\(n \\geq 0\\) and \\(r \\geq 0\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n}{n - r}\\) \\) ::: ::: Using the previous example, we know that: \\( \\(\\binom{6}{2} = \\binom{6}{4}\\) \\) At first, it might not make sense, but it will prove to be useful in the next section, when we go over Pascal's trianglewhich is an arrangement of the combinations that makes them simple to remember. Lastly, we have this identity. ::: tBox ::: theoremeT Theorem 3.1 (Sum of squares combinations). For \\(n \\leq 0\\) , we have that ( \\(\\sum_{r=0}^n\\binom{n}{r}^2 = \\binom{2n}{n}\\) \\) ::: ::: As such, we can express the sum of squares as a single combination, shown below: \\( \\(\\sum_{r=0}^{25} \\binom{25}{r}^2 = \\binom{2 \\cdot 25}{25} = \\binom{50}{25}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Pascal's Triangle The rows of Pascal's triangle are indexed by non-negative integers: \\(0\\) , \\(1\\) , \\(2\\) , and etc. In the \\(n\\) th row, we have \\(\\binom{n}{r}\\) , where \\(1 \\leq r \\leq n\\) and the values of \\(r\\) increase from left to right. The following are the first seven rows of Pascal's triangle. Then, lastly we have this identity. ::: center ::: Though, they are more commonly represented as their integer counterpart. ::: center ::: You can see many patterns of how combinations are related in the triangle, such symmetry in a given row and the recursive property of combinations, which we discussed prior to this. One important use of combinations is in expanding polynomial expressions, such as \\((x + y)^n\\) . The binomial theorem generalizes this formula. ::: tBox ::: theoremeT Theorem 3.1 (Binomial theorem). Let \\(n\\) be non-negative integers and let \\(x\\) , \\(y\\) be variables. ( \\((x + y)^n = \\sum_{r = 0}^n\\binom{n}{r}x^{n-r}y^r\\) \\) ( \\(\\qquad\\quad\\ \\ = \\binom{n}{0}x^n + \\binom{n}{1}x^{n-1}y + \\binom{n}{2}x^{n-2}y^2 + \\cdots + \\binom{n}{n-1}xy^n + \\binom{n}{n}y^n\\) \\) ::: ::: In other words, the triangular arrangement of numbers gives us the coefficients in the expansion of any binomial expression. ::: exampleT Example 3.10 . Expand the following expression \\((x+y)^n\\) for \\(n=6\\) : Using the binomial theorem: ( \\((x + y)^6 = \\binom{6}{0}x^6 + \\binom{6}{1}x^5y + \\binom{6}{2}x^4y^2 + \\binom{6}{3}x^3y^3 + \\binom{6}{4}x^2y^4 + \\binom{6}{5}xy^5 + \\binom{6}{6}y^6\\) \\) If we refer back to Pascal's triangle, then we can easily substitute the binomial coefficient with its respective integers, as such: ( \\((x + y)^6 = x^6 + 6x^5y + 15x^4y^2 + 20x^3y^3 + 15x^2y^4 + 6xy^5 + y^6\\) \\) :::","title":"MTH314"},{"location":"W2022/MTH314/MTH314/#intro-to-sets-and-logic","text":"","title":"Intro to Sets and Logic"},{"location":"W2022/MTH314/MTH314/#the-language-of-sets","text":"One of the most important fundamentals revolves around sets. Definition 1.1 (Set). A set refers to a collection of objects, written in set-roster notation; using curly brackets \\(\\{ \\}\\) or set-builder notation, which will be discussed later. We use the following notation \\(\\in\\) to represent an element of a set and \\(\\notin\\) when it is not an element of a set. Example 1.1 . *Given \\(A = \\{1,2,3,4,5\\}\\) , we can write it as: \\(1 \\in A\\) \\(3 \\in A\\) \\(6 \\notin A\\) \\(\\pi \\notin A\\) There are certain sets of numbers referred to frequently, so they are given common set notations: Symbol Set of ... Example \\(\\mathbb{N}\\) Non-negative integers or natural numbers \\(\\{0,1,2,3, \\cdots\\}\\) \\(\\mathbb{Z}\\) Integers \\(\\{\\cdots, -2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q}\\) Rational numbers \\(\\{\\frac{p}{q} \\mid p,q \\in \\mathbb{Z}, q \\neq 0\\}\\) \\(\\mathbb{R}\\) Real numbers All of the above number sets Addition to a superscript \\(+\\) or \\(-\\) indicates that only positive or negative elements of the sets: \\(\\mathbb{Z}^+ = \\{1,2,\\cdots\\}\\) \\(\\mathbb{Z}^- = \\{-1,-2,\\cdots\\}\\) \\(\\mathbb{R}^+\\) is a set of positive real numbers \\(\\mathbb{R}^-\\) is a set of negative real numbers Another way to describe a set is using a set-builder notation, which characterize all the elements in the set by stating the property or properties they must have to be members. Definition 1.2 (Set-Builder Notation). *Let \\(S\\) denote a set and let \\(P(x)\\) be a property that elements of \\(S\\) may or may not satisfy. \\[\\{x \\in S \\mid P(x)\\}\\] We may define the following to be the set of all elements \\(x\\) in \\(S\\) such that \\(P(x)\\) is true. Example 1.2 . Let \\(A = \\{x \\in \\mathbb{Z} \\mid -2 < x < 5\\}\\) , the following set can be described as the set of all elements \\(x\\) are integers such that \\(-2 < x < 5\\) , where: \\( \\(A = \\{-1,0,1,2,3,4\\}\\) \\) * Definition 1.3 (Cardinality). *The cardinality of set denotes the number of elements of the set, usually denoted with a vertical bar on each side. Example 1.3 . Let \\(B = \\{2,5,7,9,12\\}\\) , then the cardinality of set \\(B\\) is: \\[|B| = 5\\] As defined earlier, a set is a collection of objects and so how do we define a set with no objects? Definition 1.4 (Empty Set). *A special set that contains no elements is called an empty set, which uses the notation \\(\\varnothing\\) . For an empty set, the cardinality would be \\(|\\varnothing| = 0\\) , since it contains no elements; \\(\\varnothing = \\{\\}\\) .","title":"The Language of Sets"},{"location":"W2022/MTH314/MTH314/#subsets","text":"A basic relation between sets is that of subset, which introduces a new notation \\(\\subseteq\\) for subset and \\(\\subsetneq\\) for proper subset. $$A \\subseteq B$$ $$A \\subsetneq B$$ Definition 1.5 (Subset). If \\(A\\) and \\(B\\) are sets, then \\(A\\) is a subset of \\(B\\) , written as \\(A \\subseteq B\\) , if and only if every element of \\(A\\) is also an element of \\(B\\) . Definition 1.6 (Proper subset). *If \\(A\\) is a subset of \\(B\\) , but not equal to \\(B\\) ; where there is at least one element of \\(B\\) not in \\(A\\) , then \\(A\\) is a proper subset of \\(B\\) , written as \\(A \\subsetneq B\\) . Some online sources or textbooks may use the following notation \\(\\subset\\) instead of \\(\\subsetneq\\) , to represent a proper subset. They are the same thing, but to avoid confusion, I'll be using this notation \\(\\subsetneq\\) . Note that every proper subset is a subset, but not every subset is a proper subset. So the diagram shown in (b) can also be used to demonstrate what a subset looks like. Example 1.4 . *Let \\(A = \\{1,2,3,4,5\\}\\) , \\(B = \\{1,3,5\\}\\) , \\(C = \\{2,3,5\\}\\) , and \\(D = \\{2,3,5\\}\\) then: \\(B \\subseteq A\\) and \\(B \\subsetneq A\\) \\(C \\subseteq A\\) and \\(C \\subsetneq A\\) \\(C\\subseteq D\\) and \\(D \\subseteq C\\) Two sets are equal when they share the exact element or in other words, subsets of each other: \\(C \\subseteq D\\) and \\(D \\subseteq C \\Longleftrightarrow C = D\\) .","title":"Subsets"},{"location":"W2022/MTH314/MTH314/#power-sets","text":"Previously, we went over what a subset is and so will use that to define the following set. Definition 1.7 (Power Set). Let \\(S\\) denote a set, the power set of \\(S\\) , denoted \\(\\mathcal{P}(S)\\) , is the set of all subsets of \\(S\\) . In general, for any finite set \\(S\\) , where \\(|S| = n\\) , we have that \\(|\\mathcal{P}(S)| = 2^n\\) . If you recall every proper subset is considered a subset and so we may use that in the following example. Any set is gonna have various subsets. Example 1.5 . Given a set \\(A = \\{1,3,5\\}\\) , we can define the following to be subsets of \\(A\\) : \\(\\{1\\} \\subseteq A\\) \\(\\{1,3\\} \\subseteq A\\) \\(\\{1,5\\} \\subseteq A\\) \\(\\{1,3,5\\} \\subseteq A\\) and so on The power set is basically all the possible subsets of \\(A\\) that can be formed given a set. The empty set \\(\\varnothing\\) and the set \\(A\\) is always included in the \\(\\mathcal{P}(A)\\) . Example 1.6 . Using the set \\(A = \\{1,3,5\\}\\) , then: \\[\\mathcal{P}(A) = \\{\\varnothing, \\{1\\}, \\{2\\}, \\{3\\}, \\{1,3\\}, \\{1,5\\}, \\{3,5\\}, \\{1,3,5\\}\\}\\] It becomes quite tricky when we use an empty set. The power set of an empty set, \\(\\mathcal{P}(\\varnothing) = \\{\\varnothing\\}\\) with cardinality of \\(1\\) . We can double check this by \\(|\\varnothing| = 0\\) and so \\(|\\mathcal{P}(\\varnothing)| = 2^0 = 1\\) or cardinality of \\(1\\) .","title":"Power Sets"},{"location":"W2022/MTH314/MTH314/#singleton-and-doubleton","text":"When a set only has one or two elements, it can be classified into one of two ways: Definition 1.8 (Singleton). A singleton is a set with a single element, \\(\\{x\\}\\) . Definition 1.9 (Doubleton). A doubleton (or unordered pair) is a set with two elements, \\(\\{x,y\\}\\) . An ordered pair uses round brackets instead of curly brackets to indicate that order matters, \\((x,y)\\) , which will be discussed more in later sections.","title":"Singleton and Doubleton"},{"location":"W2022/MTH314/MTH314/#operations-on-sets","text":"There are four main set operations to be discussed. Let \\(A\\) and \\(B\\) be subsets of a universal set \\(U\\) . Universal set, denoted \\(U\\) , is the collection of all objects that can occur as elements of the sets under consideration. All other sets are subsets of the universal set. Definition 1.10 (Union). The union of \\(A\\) and \\(B\\) , denoted \\(A \\cup B\\) , is the set of all elements that are at least in one of \\(A\\) or \\(B\\) . \\[A\\ \\cup\\ B = \\{x \\in U \\mid x \\in A \\text{ or } x \\in B\\}\\] Definition 1.11 (Intersection). The intersection of \\(A\\) and \\(B\\) , denoted \\(A \\cap B\\) , is the set of all elements that are common to both \\(A\\) and \\(B\\) . \\[A\\ \\cap\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\in B\\}\\] Definition 1.12 (Difference). The difference of \\(A\\) minus \\(B\\) , denoted \\(A \\smallsetminus B\\) , is the set of all elements that are in \\(A\\) and not in \\(B\\) . \\( \\(A\\ \\smallsetminus\\ B = \\{x \\in U \\mid x \\in A \\text{ and } x \\notin B\\}\\) \\) * Definition 1.13 (Complement). The complement of \\(A\\) , denoted \\(A^c\\) , is the set of all elements in \\(U\\) that are not in \\(A\\) . \\[A^c = \\{x \\in U | x \\notin A\\}\\] There's one other operation not included, which is the symmetric difference, denoted \\(A \\triangle B\\) , is the set of elements which are in either of the sets A and B, but are not common to both \\(A\\) and \\(B\\) .","title":"Operations on Sets"},{"location":"W2022/MTH314/MTH314/#set-properties","text":"The following theorem consists of set identities, some of which you might be familiar with: Theorem 1.1 . *Let \\(A\\) , \\(B\\) , and \\(C\\) be subsets of a universal set \\(U\\) : Commutative law: Associative law: Distributive law: Complement law: Double complement law: De Morgan's law: Identity law: Idempotent law: Set difference law:","title":"Set Properties"},{"location":"W2022/MTH314/MTH314/#indexed-collection-of-sets","text":"The definitions of unions and intersections for more than two sets are very similar to the definitions for two sets, which we may generalize in the following way. Definition 1.14 (Union and Intersection). *Let \\(A_i\\) be a subset of a universal set \\(U\\) where \\(i \\geq 1\\) and given a non-negative integer \\(n\\) . \\[\\bigcup\\limits_{i=1}^n\\ A_i = A_1 \\cup A_2 \\cup \\cdots \\cup A_n = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } 1 \\leq i \\leq n\\}\\] \\[\\bigcap\\limits_{i=1}^n\\ A_i = A_1 \\cap A_2 \\cap \\cdots \\cap A_n = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } 1 \\leq i \\leq n\\}\\] ... and generalize to infinite unions and intersections. \\[\\bigcup\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for some } i, \\text{ where } i \\geq 1 \\}\\] \\[\\bigcap\\limits_{i=1}^\\infty\\ A_i = \\{x \\in U \\mid x \\in A_i \\text{ for all } i, \\text{ where } i \\geq 1\\}\\] Recap of the interval notation, \\((\\ )\\) means the endpoints are excluded and \\([\\ ]\\) means they are included. There are three specific types you'll encounter when solving for infinite intersections of a set: \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\bigg[0, \\frac{1}{i}\\bigg)\\) \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (n, \\infty)\\) and \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ [n, \\infty)\\) There might be slight variations of the questions, but it should give you a general idea on how to solve for them. Example 1.7 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : Let's think logically, by drawing out the number lines for the first three sets. If you notice the number line slowly decreases in size. As it reaches infinity, it will eventually reach \\(0\\) , which will be the only thing they all share in common. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = \\{0\\}\\] Example 1.8 . *Solve the following set, \\(\\bigcup\\limits_{i=1}^\\infty \\Big(-\\frac{1}{i}, \\frac{1}{i}\\Big)\\) : 1. We are now interested in all of the possible elements of the set. Using the number line from before, if you notice, \\((-1,1)\\) already contains all the elements in \\(A_2\\) and \\(A_3\\) . So we can say that \\((-1,1)\\) contains all the elements shared in the infinite set. \\[\\bigcup\\limits_{i=1}^\\infty \\bigg(-\\frac{1}{i}, \\frac{1}{i}\\bigg) = (-1,1)\\] The notation for the open interval \\((a,b)\\) is identical to the notation for ordered pair \\((a,b)\\) , context makes it unlikely that the two will be confused. Example 1.9 . *Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty \\Big(0, \\frac{1}{i}\\Big)\\) : Let's draw out the number lines again for the first three sets. Likewise, as it reaches infinity, it will eventually reach \\(0\\) , however, note that \\(0\\) is not included, from the round brackets, so we say it's an empty set. \\[\\bigcap\\limits_{i=1}^\\infty \\bigg(0, \\frac{1}{i}\\bigg) = \\varnothing\\] If instead it was \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty \\Big[0, \\frac{1}{i}\\Big)\\) , then our answer would be \\(\\{0\\}\\) , as it is included in the set. ::: ::: exampleT Example 1.10 . Solve the following set, \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty)\\) : Let's draw out the first number line when \\(i = 1\\) and when \\(i = 5\\) . ::: center ::: As \\(i\\) goes from \\(1\\) to \\(5\\) , you notice that \\(\\{1,2,3,4\\}\\) is no longer common for all sets. As \\(i \\to \\infty\\) , logically there exists no real number which are common for all sets. We can conclude that the intersection of the infinite set is an empty set. ( \\(\\bigcap\\limits_{i=1}^\\infty\\ [i, \\infty) = \\varnothing\\) \\) ::: ::: list Note that the answer doesn't change even if we have \\(\\displaystyle\\bigcap\\limits_{i=1}^\\infty\\ (i,\\infty)\\) , our answer is still \\(\\varnothing\\) . ::: If you are interested in the mathematical proof, click on the following link: [Archimedean property]{.underline} . -3ex -0.1ex -.4ex 0.5ex .2ex Cartesian Products on Sets In this section, we'll first focus on the notion of ordered pairs and how they work. ::: dBox ::: definitionT Definition 1.15 (Ordered pair). The symbol \\((a,b)\\) denotes the ordered pair with the specification that \\(a\\) is the first element and \\(b\\) is the second element of the pair. ( \\((a,b) = (c,d) \\longrightarrow a = c \\text{ and } b = d\\) \\) ::: ::: ::: dBox ::: definitionT Definition 1.16 (Cartesian product). For sets \\(A\\) and \\(B\\) , the Cartesian product of \\(A\\) and \\(B\\) , denoted \\(A \\times B\\) , is the set of all ordered pairs \\((a,b)\\) . ( \\(A \\times B = \\{(a,b) \\mid a \\in A \\text{ and } b \\in B\\}\\) \\) ::: ::: ::: exampleT Example 1.11 . Let \\(A = \\{x,y\\}\\) and \\(B = \\{1,2,3\\}\\) , find \\(A \\times B\\) and \\(B \\times A\\) : \\(A \\times B = \\{(x,1), (y,1), (x,2), (y,2), (x,3), (y,3)\\}\\) \\(B \\times A = \\{(1,x), (1,y), (2,x), (2,y), (3,x), (3,y)\\}\\) ::: ::: list Note how the order matters, such that \\(A \\times B \\neq B \\times A\\) in the following example above. ::: -4ex -1ex -.4ex 1ex .2ex Logic It's important to first establish one thing, which we define as: ::: dBox ::: definitionT Definition 1.17 (Statement). A statement (or proposition) is a sentence that is true or false, but not both. ::: ::: There are different ways to express a statement as shown below. ::: exampleT Example 1.12 . Determine whether the following are statements and if so, are they true or false? \\\" \\(2\\) is greater than \\(5\\) \\\" is a statement and is logically false. \\\" \\(x > 5\\) \\\" is not a statement because of the variable \\(x\\) , it is undetermined. \\\" \\(\\sqrt{9}\\) is an integer\\\" is a statement and is logically true. \\\"There are \\(7\\) days in a week\\\" is a statement and is logically true. ::: We will now introduce five logical connectives, used to build more complicated logical expressions out of simpler ones. Let \\(P\\) and \\(Q\\) be statement variables. ::: dBox ::: definitionT Definition 1.18 (Negation). The statement \\\"not \\(P\\) \\\", denoted by \\(\\lnot\\ P\\) , is true when \\(P\\) is false. ::: ::: ::: dBox ::: definitionT Definition 1.19 (Conjunction). The statement \\\" \\(P\\) and \\(Q\\) \\\", denoted by \\(P\\ \\land\\ Q\\) , is true when, and only when, both \\(P\\) and \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.20 (Disjunction). The statement \\\" \\(P\\) or \\(Q\\) \\\", denoted by \\(P\\ \\lor\\ Q\\) , is true when at least one of \\(P\\) or \\(Q\\) are true. ::: ::: ::: dBox ::: definitionT Definition 1.21 (Conditional). The statement \\\"If \\(P\\) then \\(Q\\) \\\", denoted by \\(P\\ \\rightarrow\\ Q\\) , is false when \\(P\\) is true and \\(Q\\) is false; otherwise it is true. ::: ::: ::: dBox ::: definitionT Definition 1.22 (Biconditional). The statement \\\" \\(P\\) if and only if \\(Q\\) \\\", denoted by \\(P\\ \\leftrightarrow\\ Q\\) , is true exactly when either \\(P\\) and \\(Q\\) are both true, or when \\(P\\) and \\(Q\\) are both false. ::: ::: ::: list The order of operations goes from \\(\\lnot\\) , \\(\\land\\) , \\(\\lor\\) , \\(\\rightarrow\\) then \\(\\leftrightarrow\\) , if no parenthesis are present. ::: The five logical connectives has the following truth table: \\(P\\) \\(\\lnot\\ P\\) T F F T \\(P\\) \\(Q\\) \\(P\\ \\land\\ Q\\) \\(P\\ \\lor\\ Q\\) \\(P\\ \\rightarrow\\ Q\\) \\(P\\ \\leftrightarrow\\ Q\\) T T T T T T T F F T F F F T F T T F F F F F T T However, when statement is always true or false we can define to be the following: ::: dBox ::: definitionT Definition 1.23 (Tautology). A tautology is a statement form that is always true regardless of the truth values of the individual statement substituted for its statement variables ::: ::: ::: dBox ::: definitionT Definition 1.24 (Contradiction). A contradiction is a statement form that is always false regardless of the truth values of the individual statements substituted for its statement variables. ::: ::: ::: exampleT Example 1.13 . Show that \\(P\\ \\lor\\ \\lnot\\ P\\) is a tautology and that \\(P\\ \\land\\ \\lnot\\ P\\) is a contradiction. \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\lor\\ \\lnot\\ P\\) *T* *F* *T* *F* *T* *T* \\(P\\) \\(\\lnot\\ P\\) \\(P\\ \\land\\ \\lnot\\ P\\) *T* *F* *F* *F* *T* *F* ::: -3ex -0.1ex -.4ex 0.5ex .2ex Conditional Statements When asked to rewrite the following sentences using \\(\\rightarrow\\) , they would use the phrases \\\"necessary condition\\\" and \\\"sufficient condition\\\", which implies: The statement \\(P\\) is a necessary condition for \\(Q\\) means that \\(Q\\ \\rightarrow\\ P\\) . The statement \\(P\\) is a sufficient condition for \\(Q\\) means that \\(P\\ \\rightarrow\\ Q\\) . For a conditional statement, \\(P\\ \\rightarrow\\ Q\\) , we can form two related statements: ::: dBox ::: definitionT Definition 1.25 (Inverse). The inverse of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ P\\ \\rightarrow\\ \\lnot\\ Q\\) . ::: ::: ::: dBox ::: definitionT Definition 1.26 (Converse). The converse of \\(P\\ \\rightarrow\\ Q\\) is \\(Q\\ \\rightarrow\\ P\\) . ::: ::: ::: dBox ::: definitionT Definition 1.27 (Contrapositive). The contrapositive of \\(P\\ \\rightarrow\\ Q\\) is \\(\\lnot\\ Q\\ \\rightarrow\\ \\lnot\\ P\\) . ::: ::: ::: exampleT Example 1.14 . Rewrite the following statement: \\\"If \\(n\\) is prime, then \\(n\\) is odd or \\(n\\) is \\(2\\) \\\" using the: Inverse: If \\(n\\) is not prime, then \\(n\\) is not odd and \\(n\\) is not \\(2\\) . Converse: If \\(n\\) is odd or \\(n\\) is \\(2\\) , then \\(n\\) is a prime. Contrapositive: If \\(n\\) is not odd and \\(n\\) is not \\(2\\) , then \\(n\\) is not a prime. ::: ::: list When writing the negation, watch out for other logical connectives present in the sentences, like \\(\\land\\) and \\(\\lor\\) and make sure to apply De Morgan's laws properly. ::: -3ex -0.1ex -.4ex 0.5ex .2ex Logical Equivalences Two statements are logically equivalent, denoted by \\(\\equiv\\) , if they share the same truth table. ::: tBox ::: theoremeT Theorem 1.1 . Let \\(P\\) and \\(Q\\) be statement variables: Commutative law: Associative law: Distributive law: Double negation law: De Morgan's law: Idempotent law: Implication law: ::: ::: You might notice some similarities to the previous theorem for unions and intersections of a set. ::: tabu *2X[c] Logical Equivalences & Set Properties \\ ::: flushleft For all statements variables \\(P\\) , \\(Q\\) , and \\(R\\) : ::: & ::: flushleft For all sets \\(A\\) , \\(B\\) , and \\(C\\) : ::: \\ (a) \\(P\\ \\lor\\ Q \\equiv Q\\ \\lor\\ P\\) (b) \\(P\\ \\land\\ Q \\equiv Q\\ \\land\\ P\\) & (a) \\(A\\ \\cup\\ B = B\\ \\cup\\ A\\) (b) \\(A\\ \\cap\\ B = B\\ \\cap\\ A\\) \\ (a) \\(P\\ \\land\\ (Q \\land R) \\equiv (P\\ \\land Q)\\ \\land\\ R\\) (b) \\(P\\ \\lor\\ (Q \\lor R) \\equiv (P\\ \\lor Q)\\ \\lor\\ R\\) & (a) \\(A\\ \\cap\\ (B\\ \\cap\\ C) = (A\\ \\cap\\ B)\\ \\cap\\ C\\) (b) \\(A\\ \\cup\\ (B\\ \\cup\\ C) = (A\\ \\cup\\ B)\\ \\cup\\ C\\) \\ (a) \\(P\\ \\land\\ (Q\\ \\lor R) \\equiv (P\\ \\land Q)\\ \\lor\\ (P\\ \\land\\ R)\\) (b) \\(P\\ \\lor\\ (Q\\ \\land R) \\equiv (P\\ \\lor Q)\\ \\land\\ (P\\ \\lor\\ R)\\) & (a) \\(A\\ \\cap\\ (B\\ \\cup\\ C) = (A\\ \\cap\\ B)\\ \\cup\\ (A\\ \\cap\\ C)\\) (b) \\(A\\ \\cup\\ (B\\ \\cap\\ C) = (A\\ \\cup\\ B)\\ \\cap\\ (A\\ \\cup\\ C)\\) \\ (a) \\(\\lnot(\\lnot\\ P) \\equiv P\\) & (a) \\((A^c)^c = A\\) \\ (a) \\(P\\ \\lor\\ P \\equiv P\\) (b) \\(P\\ \\land\\ P \\equiv P\\) & (a) \\(A\\ \\cup\\ A = A\\) (b) \\(A\\ \\cap\\ A = A\\) \\ (a) \\(\\lnot(P\\ \\lor\\ Q) \\equiv \\lnot\\ P\\ \\land\\ \\lnot\\ Q\\) (b) \\(\\lnot(P\\ \\land\\ Q) \\equiv \\lnot\\ P\\ \\lor\\ \\lnot\\ Q\\) & (a) \\((A\\ \\cup\\ B)^c = A^c\\ \\cap\\ B^c\\) (b) \\((A\\ \\cap\\ B)^c = A^c\\ \\cup\\ B^c\\) \\ ::: ::: tabu *2X[c] (a) \\(P\\ \\lor\\ (P\\ \\land\\ Q) \\equiv P\\) (b) \\(P\\ \\land\\ (P\\ \\lor\\ Q) \\equiv P\\) & (a) \\(A\\ \\cup\\ (A\\ \\cap\\ B) = A\\) (b) \\(A\\ \\cap\\ (A\\ \\cup\\ B) = A\\) \\ ::: -3ex -0.1ex -.4ex 0.5ex .2ex Predicates and Quantified Statements We initially discussed that a logical statement is either true or false. So something like \\\" \\(x > 5\\) \\\" is not a statement, but what we define to be a predicate. ::: dBox ::: definitionT Definition 1.28 (Predicate). A predicate \\(P(x)\\) is a sentence that contains a finite number of variables and becomes a statement when specific values are substituted for variables. ::: ::: ::: dBox ::: definitionT Definition 1.29 (Domain). The domain \\(D\\) of a predicate variable is the set of all values that may be substituted in place of variable. ::: ::: A way to obtain statements from predicates is to add quantifiers. Let \\(P(x)\\) be a predicate and \\(D\\) the domain of \\(x\\) . ::: dBox ::: definitionT Definition 1.30 (Universal quantifier). The symbol \\(\\forall\\) is read as \\\"for every\\\" or \\\"for all.\\\" A universal statement is a statement of the form, \\(\\forall x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for every \\(x\\) in \\(D\\) . It is defined to be false if, and only if, \\(P(x)\\) is false for at least one \\(x\\) in \\(D\\) . ::: ::: ::: dBox ::: definitionT Definition 1.31 (Existential quantifier). The symbol \\(\\exists\\) is read as \\\"there exists\\\" or \\\"there is.\\\" An existential statement is a statement of the form, \\(\\exists x \\in D,\\ P(x)\\) . It is defined to be true if, and only if, \\(P(x)\\) is true for at least one \\(x\\) in \\(D\\) . It is false if, and only if, \\(P(x)\\) is false for all \\(x\\) in \\(D\\) . ::: ::: ::: exampleT Example 1.15 . Rewrite the following sentences using quantifiers. \\\"Every real number has a non-negative square\\\" rewritten as \\(\\forall x \\in \\mathbb{R},\\ x^2 \\geq 0\\) \\\"There is a positive integer whose square is equal to itself\\\" rewritten as \\(\\exists y \\in \\mathbb{Z}^+,\\ y^2 = y\\) ::: One final topic to discuss is the negation of universal and existential quantifiers. ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Universal statement). The negation of a universal statement is logically equivalent to an existential statement. Symbolically, ( \\(\\lnot(\\forall x \\in D,\\ P(x)) \\equiv \\exists x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: tBox ::: theoremeT Theorem 1.1 (Negation of a Existential statement). The negation of a existential statement is logically equivalent to an universal statement. Symbolically, ( \\(\\lnot(\\exists x \\in D,\\ P(x)) \\equiv \\forall x \\in D,\\ \\lnot\\ P(x)\\) \\) ::: ::: ::: exampleT Example 1.16 . Negate the following statements from the previous exercise. \\\"There exists a real number that has a negative square\\\" or \\(\\exists x \\in \\mathbb{R},\\ x^2 < 0\\) \\\"Every positive integer whose square is not equal to itself\\\" or \\(\\forall x \\in \\mathbb{Z}^+,\\ y^2 \\neq y\\) ::: ::: list In example 1.11, the first statement is true, while, in example 1.12, the first statement is false, since squaring a number will always be positive. ::: There are some cases where certain statements contain multiple quantifiers. ::: exampleT Example 1.17 . Negate the following statement: \\(\\forall x \\in \\mathbb{Z},\\ \\exists y \\in \\mathbb{Z},\\ y > x\\) . We can simply think of \\\" \\(\\exists y \\in \\mathbb{Z},\\ y > x\\) \\\" as the predicate \\(P(x)\\) . ( \\(\\lnot(\\forall x \\in \\mathbb{Z},\\ P(x)) \\equiv \\exists x \\in \\mathbb{Z},\\ \\lnot\\ P(x)\\) \\) Then we take the negation of \\(P(x)\\) . ( \\(\\lnot\\ P(x) \\equiv \\lnot(\\exists y \\in \\mathbb{Z},\\ y > x) \\equiv \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) And we put it all together. ( \\(\\exists x \\in \\mathbb{Z},\\ \\forall y \\in \\mathbb{Z},\\ y \\leq x\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Order of Quantifiers In a statement containing both \\(\\forall\\) and \\(\\exists\\) , changing the order of the quantifiers can significantly change the meaning of the statementyou read from left to right. For example, the following statements are equivalent: \\( \\(\\forall x,\\ \\forall y,\\ P(x) \\equiv \\forall y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\exists y,\\ P(x) \\equiv \\exists y,\\ \\exists x,\\ P(x)\\) \\) However, now consider mixed quantifier and they are no longer equivalent: \\( \\(\\forall x,\\ \\exists y,\\ P(x) \\not\\equiv \\exists y,\\ \\forall x,\\ P(x) \\hspace{2cm} \\exists x,\\ \\forall y,\\ P(x) \\not\\equiv \\forall y,\\ \\exists x,\\ P(x)\\) \\) For a more dramatic context. let's use the following analogy: \\(\\forall x,\\ \\exists y,\\ x \\text{ loves } y\\) and \\(\\exists y,\\ \\forall x,\\ x \\text{ loves } y\\) Note that both statements looks identical, except the order of quantifiers. However, the first statement means everybody loves somebodywhom that somebody could be a different person for each \\(x\\) , ... whereas the second statement means there is one individual who is loved by all people. We can also visualize this using a directed graph, as a shown below:","title":"Indexed Collection of Sets"},{"location":"W2022/MTH314/MTH314/#relations-and-functions","text":"-4ex -1ex -.4ex 1ex .2ex Binary Relations A relations is something that involves two different sets. A special kind of binary relation is a function. Suppose there are some elements inside \\(X\\) and \\(Y\\) , we can visualize an arrow diagram for our relation. The graph corresponds to the following ordered pairs: \\(\\{(x_1,y_1), (x_2, y_2), (x_3, y_3)\\}\\) ::: dBox ::: definitionT Definition 2.1 (Binary Relation). For sets \\(X\\) and \\(Y\\) , a binary relation \\(R\\) from \\(X\\) to \\(Y\\) is a subset of \\(X \\times Y\\) . Hence, \\(R\\) is a set of ordered pairs \\((x,y)\\) with \\(x \\in X\\) and \\(y \\in Y\\) . We write \\(x \\mathrel{R}y\\) if \\((x,y) \\in R\\) . We say that \\(R\\) is a binary relation on \\(X\\) if \\(X = Y\\) ; that is, \\(R\\subseteq X \\times X\\) . ::: ::: A relation can also be drawn as a directed graph which will prove to be more useful when explaining the properties of relation. Using the same set of ordered pairs from before, it can drawn as such: ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Relation There exists various properties that some, but not all, relations satisfy. Let \\(R\\) be a binary relation on a set \\(A\\) : ::: dBox ::: definitionT Definition 2.2 (Reflexive). The relation \\(R\\) is reflexive, if for all \\(x \\in A\\) , \\(x \\mathrel{R}x\\) . \\(R\\) is reflexive \\(\\Leftrightarrow\\) for every \\(x\\) in \\(A\\) , \\((x,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.3 (Symmetric). The relation \\(R\\) is symmetric, if for all \\(x,y \\in A\\) , if \\(x \\mathrel{R}y\\) , then \\(y \\mathrel{R}x\\) . \\(R\\) is symmetric \\(\\Leftrightarrow\\) for every \\(x\\) and \\(y\\) in \\(A\\) , if \\((x,y) \\in R\\) then \\((y,x) \\in R\\) ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.4 (Transitive). The relation \\(R\\) is transitive, if for all \\(x,y,z \\in A\\) , if \\(x \\mathrel{R}z\\) and \\(y \\mathrel{R}z\\) , then \\(x \\mathrel{R}z\\) . \\(R\\) is transitive \\(\\Leftrightarrow\\) for every \\(x\\) , \\(y\\) , and \\(z\\) in \\(A\\) , if \\((x,y) \\in R\\) and \\((y,z) \\in R\\) then \\((x,z) \\in R\\) ::: center ::: ::: ::: ::: exampleT Example 2.1 . Suppose we have a set \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0), (0,1), (0,3), (1,0), (1,1), (2,2), (3,0), (3,3)\\}\\) \\) Let's draw out the directed graph. ::: center ::: \\(R\\) is reflexive because there is a loop for each element, as shown for \\((0,0), (1,1), (2,2), (3,3)\\) . \\(R\\) is symmetric because there is an arrow going from one point then back to the other, as shown for \\((0,1), (1,0)\\) and \\((0,3), (3,0)\\) \\(R\\) is not transitive because there's no arrow from \\((1,3)\\) or \\((3,1)\\) which would otherwise make it transitive. ::: Sometimes the set of relation is not given, instead the definition is provided. For example, suppose we have set a \\(A = \\{1,2,3,4\\}\\) and the relation \\(R\\) defined as follows: Properties of Div: \\((x,y) \\in R \\text{ if } x \\mid y\\) The line \\\" \\(\\mid\\) \\\" means \\(x\\) divisible by \\(y\\) . Equivalently, you can think of it as \\(y = xk\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,2),(1,3),(1,4),(2,2),(2,4),(3,3),(4,4)\\}\\) \\) Properties of Congruence Modulo n: \\((x,y) \\in R \\text{ if } n \\mid (x-y)\\) Sometimes, it referred to as \\(x \\equiv y \\ (\\mathrm{mod}\\ n)\\) . Let use \\(n = 2\\) , as an example. We can use the expression before, \\(x-y = 2k\\) where \\(k \\in \\mathbb{Z}\\) . The set of relation would be: \\( \\(R = \\{(1,1),(1,3),(2,2),(2,4),(3,1),(3,3),(4,2),(4,4)\\}\\) \\) Properties of \\\"Greater Than\\\": \\((x,y) \\in R \\text{ if } x > y\\) In this case, we are only interested where \\(x\\) is greater than \\(y\\) . The set of relation would be: \\( \\(R = \\{(2,1),(3,1),(3,2),(4,1),(4,2),(4,3)\\}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Equivalence Relation There's are cases when the relation has all three properties discussed. ::: dBox ::: definitionT Definition 2.5 (Equivalence relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is an equivalence relation if it is reflexive, symmetric and transitive, \\((A, R)\\) . ::: ::: A notable example would be the equality sign, which might not make senses at first, but let's break it down. For all real numbers \\(x\\) and \\(y\\) , consider the relation \\(R\\) defined as follows \\( \\((x,y) \\in R \\text{ if } x = y\\) \\) Is \\(R\\) reflexive? Yes, because it is implying the statement \\(x = x\\) , which is true; every real number is equal to itself. Is \\(R\\) symmetric? Yes, because if \\(x = y\\) , then \\(y = x\\) is also true; if one number is equal to another, then the second is equal to the first. Is \\(R\\) transitive? Yes, because if \\(x = y\\) and \\(y = z\\) , then \\(x = z\\) is true; if one real number equals a second and the second equals a third, then the first must also equal the third. Let's introduce this new idea called the equivalence class, as an extension to equivalence relation. Suppose \\(A\\) is a set and \\(R\\) is an equivalence relation on \\(A\\) . ::: dBox ::: definitionT Definition 2.6 (Equivalence class). For each element \\(a\\) in \\(A\\) , the equivalence class of \\(a\\) , denoted \\([a]\\) is the set of all elements \\(x\\) in \\(A\\) such that \\(x\\) is related to \\(a\\) by \\(R\\) . ( \\(= \\{x \\in A \\mid x \\mathrel{R}a \\}\\) \\) ::: ::: ::: list Some textbooks may define it as \\([a] = \\{x \\in A \\mid a \\mathrel{R}x \\}\\) instead. Either one works, as you may recall, it's transitive. ::: ::: exampleT Example 2.2 . Let \\(A = \\{0,1,2,3,4\\}\\) and the relation as follows: ( \\(R = \\{(0,0),(0,4),(1,1),(1,3),(2,2),(3,1),(3,3),(4,0),(4,4)\\}\\) \\) Determine all the elements related to \\(0\\) or \\(x \\mathrel{R}0\\) , where \\(x \\in A\\) . ( \\(0 \\mathrel{R}0,\\ 4 \\mathrel{R}0\\) \\) The elements \\(0\\) and \\(4\\) are essentially what makes up the equivalence class of \\(0\\) , which more formally can be written as ( \\([0] = \\{x \\in A \\mid x \\mathrel{R}0\\} = \\{0,4\\}\\) \\) Thus, find the equivalence class for the rest of the elements of \\(A\\) . ( \\([1] = \\{x \\in A \\mid x \\mathrel{R}1\\} = \\{1,3\\}\\) \\) ( \\([2] = \\{x \\in A \\mid x \\mathrel{R}2\\} = \\{2\\}\\) \\) ( \\([3] = \\{x \\in A \\mid x \\mathrel{R}3\\} = \\{1,3\\}\\) \\) ( \\([4] = \\{x \\in A \\mid x \\mathrel{R}4\\} = \\{0,4\\}\\) \\) Note that \\([4] = [0]\\) and \\([1] = [3]\\) , so the distinct equivalent classes are ( \\(\\{0,4\\}, \\{1,3\\}, \\text{ and } \\{2\\}\\) \\) ::: If you notice, the distinct equivalence classes of an equivalence relation \\(R\\) on a set \\(A\\) form a partition of \\(A\\) . Likewise, the converse is also true, which will discuss in the next section. ::: tBox ::: theoremeT Theorem 2.1 . Let \\(R\\) be an equivalence relation on a set \\(A\\) , where we assume \\(A \\neq \\varnothing\\) . For all \\(x \\in A\\) , \\([x] \\neq \\varnothing\\) . If \\(x \\mathrel{R}y\\) , then \\([x] = [y]\\) . If \\((x,y) \\notin R\\) , then \\([x]\\ \\cap\\ [y] = \\varnothing\\) . ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Relation Induced by Partition As a recap, a partition of a set \\(A\\) is a finite or infinite collection of nonempty, mutually disjoint subsets whose union is \\(A\\) or illustratively, ::: center ::: where \\(A_i\\ \\cap\\ A_j = \\varnothing\\) whenever \\(i \\neq j\\) and \\(A_1\\ \\cup\\ \\cdots\\ \\cup\\ A_6 = A\\) . ::: dBox ::: definitionT Definition 2.7 . Let \\(P\\) be a partition of a set \\(A\\) and define the binary relation \\(R_P\\) , so that \\(x \\mathrel{R}y\\) if \\(x\\) and \\(y\\) are in the same part of the partition. ::: ::: Again, this definition might sound confusing, but let's use the set \\(A\\) from the previous example to get a better understanding. ::: exampleT Example 2.3 . Let \\(A = \\{0,1,2,3,4\\}\\) and consider the following be a partition \\(P\\) of \\(A\\) : ( \\(P = \\{\\{0,1\\},\\{2,3\\},\\{4\\}\\} % A_1 = \\{0,1\\} \\hspace{1cm} A_2 = \\{2,3\\} \\hspace{1cm} A_3 = \\{4\\}\\) \\) Let's consider the first subset of the partition, \\(A_1 = \\{0,1\\}\\) . Since both \\(0\\) and \\(1\\) are in the subset, we can form the following relation. ( \\(0 \\mathrel{R}1,\\ 1\\mathrel{R}0\\) \\) We can also do the following, since they are still in the same part of the partition. ( \\(0 \\mathrel{R}0,\\ 1 \\mathrel{R}1\\) \\) Thus, for the second subset, \\(A_2 = \\{2,3\\}\\) , we can form a similar set of relations. ( \\(2 \\mathrel{R}3,\\ 3 \\mathrel{R}2,\\ 2 \\mathrel{R}2,\\ 3 \\mathrel{R}3\\) \\) Finally for the third subset of the partition, \\(A_3 = \\{4\\}\\) , which only has one. ( \\(4 \\mathrel{R}4\\) \\) Hence, combining all of them makes up the binary relation \\(R_P\\) : ( \\(R_P = \\{\\underbrace{(0,1),(1,0),(0,0),(1,1)}_{\\{0,1\\}},\\underbrace{(2,3),(3,2),(2,2),(3,3)}_{\\{2,3\\}},\\underbrace{(4,4)}_{\\{4\\}}\\}\\) \\) ::: The fact is that a relation induced by a partition of a set satisfies all three properties, or in other words, \\(R_P\\) is an equivalence relation. -3ex -0.1ex -.4ex 0.5ex .2ex Partial Orders Partial orders provide one way of ranking objects. To define them, we define another property of relation. ::: dBox ::: definitionT Definition 2.8 (Antisymmetric). Let \\(R\\) be a binary relation on a set \\(A\\) . We say \\(R\\) is antisymmetric, if and only if, for every \\(a\\) and \\(b\\) in \\(A\\) , if \\(a \\mathrel{R}b\\) and \\(b \\mathrel{R}a\\) , where \\(a = b\\) . ::: ::: Equivalently, we are saying that the relation should not have the following: ::: center ::: For example, the relation on the left is not antisymmetric, whereas the relation on the right is antisymmetric. ::: center ::: ::: dBox ::: definitionT Definition 2.9 (Partial order relation). Let \\(R\\) be a binary relation on a set \\(A\\) . The relation \\(R\\) is a partial order relation if it is reflexive, antisymmetric, and transitive. ::: ::: We call \\((A,R)\\) a partial ordered set or poset. We use \\(\\preceq\\) to represent the relation \\(R\\) . Two elements \\(x\\) and \\(y\\) are comparable if either \\(x \\mathrel{R}y\\) or \\(y \\mathrel{R}x\\) . Otherwise, the elements are incomparable. A set of pairwise incomparable is called an antichain . If every pair of element is comparable, then \\(R\\) is a total order , linear order , or chain . Back to previous example, the relation on the left has two elements, \\(0\\) and \\(1\\) , that are incomparable. The relation on the right is a linear order with three elements. ::: dBox ::: definitionT Definition 2.10 (Hasse diagram). A diagram used to represent partial order relations with sufficient information and an implied upward orientation. ::: ::: To obtain a Hasse diagram, proceed as follows: Construct a digraph (or directed graph) of the poset \\((A,R)\\) , so that all arrows point upward, except the loops. Eliminate all loops. Eliminate all directed edges that are redundant because of transitivity. Eliminate the arrows on the directed edge. Suppose we have a set \\(A = \\{1,2,3,9,18\\}\\) with the div relation, \\(x \\mid y\\) . The digraph of this poset has the following appearance on the left and the Hasse diagram on the right: ::: center ::: We can reference some extremal elements of posets using the following definitions. ::: dBox ::: definitionT Definition 2.11 . Let \\(R\\) be a partial order of on a set \\(A\\) : An element \\(u\\) is a least element if \\(\\forall x \\in A\\) , \\(u \\mathrel{R}x\\) . An element \\(v\\) is a greatest element if \\(\\forall x \\in A\\) , \\(x \\mathrel{R}v\\) . An element \\(u\\) is a minimal element if there does not exist an element \\(x \\in A \\smallsetminus \\{u\\}\\) , such that \\(x \\mathrel{R}u\\) . Alternatively, there exists no element \\\"below\\\" it. An element \\(u\\) is a maximal element if there does not exist an element \\(x \\in A \\smallsetminus \\{v\\}\\) , such that \\(v \\mathrel{R}x\\) . Alternatively, there exists no element \\\"above\\\" it. ::: ::: Note the difference between least and minimal element, likewise, with greatest and maximal element. A least element is a minimal, but a minimal element need not to be a least element. Similarly, a greatest element is a maximal, but a maximal element need not to be a greatest element. A poset can have at most one least and greatest element, but it may have more than one minimal or maximal element. Look at the following digraph of each poset: ::: center ::: -4ex -1ex -.4ex 1ex .2ex Introduction to Graph Theory Graph theory one of the most important topics in discrete mathematics. You may have heard of the famous bridge puzzle, known as \\\"The Seven Bridges of K\u00f6nigsberg\\\", which consists of the following problem. ::: problem Problem 2.1 . Is it possible to find a route through the graph that starts and ends at some vertex, one of \\(A\\) , \\(B\\) , \\(C\\) , or \\(D\\) , and traverses each edge exactly once? ::: center ::: ::: We can further simply this to the following graph. If you compare the two diagrams, they are equivalently the same. ::: center ::: If you aren't already aware of it, this problem is impossible to solve and it all relates back to graph theory. Let's start off by defining what a graph is. ::: dBox ::: definitionT Definition 2.12 (Graph). A graph \\(G\\) is a pair consisting of a vertex set \\(V(G)\\) and an edge set \\(E(G)\\) containing pairs of distinct vertices, such that \\(G = (V,E)\\) ::: ::: ::: list The bridge graph is an undirected graphthe order of the two connected vertices is not important, oppose to a directed graph. ::: The order of a graph \\(G\\) is \\(|V(G)|\\) and its size is \\(|E(G)|\\) . We can use the bridge example, to define our vertex and edge set, \\( \\(V(G) = \\{A,B,C,D\\} \\hspace{1cm} E(G) = \\{\\{A,B\\},\\{A,B\\},\\{B,D\\},\\{B,D\\},\\{A,C\\},\\{B,C\\},\\{C,D\\}\\}\\) \\) where \\(|V(G)| = 4\\) and \\(|E(G)| = 7\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Degrees There's no solution to the bridge problem because suppose we start and end at vertex \\(A\\) , then the degree of the other three vertices \\(B\\) , \\(C\\) , and \\(D\\) must be even. ::: dBox ::: definitionT Definition 2.13 (Degree). Given a graph with vertex \\(v\\) , the degree of \\(v\\) , denoted by \\(\\deg_G(v)\\) is the number of edges incident to \\(v\\) . ::: ::: From the graph earlier, we can define the degree of each vertex: \\(\\deg(A) = 3\\) , \\(\\deg(B) = 5\\) , \\(\\deg(C) = 3\\) , and \\(\\deg(D) = 3\\) . ::: dBox ::: definitionT Definition 2.14 (Neighbor set). The neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(\\{w \\in V(G) \\mid v,w \\in E(G)\\}\\) ; any \\(w \\in V(G)\\) is called a neighbor of \\(v\\) , equivalently \\(\\deg_G(v) = |N_G(v)|\\) . The closed neighbor set of \\(v\\) , denoted \\(N_G(v)\\) , is \\(N_G(v)\\ \\cup\\ \\{v\\}\\) . ::: ::: ::: list As you'll see later on, the subscript \\(G\\) is often removed, if the graph is clear from context. ::: The definition may sound more confusing that what it is suppose to mean, but it is essentially the set of all vertices which are adjacent to \\(v\\) . Refer to the example below. Another set of terms will discuss in this section the minimum and maximum degree of a graph \\(G\\) . ::: dBox ::: definitionT Definition 2.15 (Minimum and Maximum Degree). The integer \\(\\delta(G)\\) is the minimum degree of \\(G\\) and the integer \\(\\Delta(G)\\) is the maximum degree of \\(G\\) . ::: ::: ::: exampleT Example 2.4 . Suppose we have the following graph \\(G\\) is defined as: ::: center ::: For each vertex: \\(\\deg(1) = 1\\) , \\(\\deg(2) = 2\\) , \\(\\deg(3) = 3\\) , \\(\\deg(4) = 2\\) , and \\(\\deg(5) = 2\\) . The vertex \\(3\\) has three vertices adjacent to it, which are \\(2\\) , \\(4\\) , and \\(5\\) , thus \\(N(3) = \\{2,4,5\\}\\) and \\(|N(3)| = \\deg(3) = 3\\) . The closed neighbor set includes \\(3\\) , since \\(N[3] = \\{2,4,5\\}\\ \\cup\\ \\{3\\} = \\{2,3,4,5\\}\\) . From 1. we can see the minimum degree is \\(1\\) and the maximum degree is \\(3\\) , thus \\(\\delta(G) = 1\\) and \\(\\Delta(G) = 3\\) . ::: The following theorem is pretty simple, but helpful in drawing conclusions when analyzing graphs. ::: tBox ::: theoremeT Theorem 2.1 (Handshake Theorem). Let \\(G\\) be a graph, then ( \\(\\sum_{u \\in V(G)} = \\deg_G(u) = 2|E(G)|\\) \\) ::: ::: The equivalent definition of this in text means, the sum of degree of all the vertices is twice the number of edges contained in it. Using the example from before, there's five vertices and a total degree of \\(10\\) . In general, any undirected graph has an even number of vertices of odd degree. -3ex -0.1ex -.4ex 0.5ex .2ex Walks, Paths, and Cycles Travel in a graph is accomplished by moving from one vertex to another along a sequence of adjacent edges. There various definitions used to describe the movement in a graph. ::: dBox ::: definitionT Definition 2.16 . Let \\(G\\) be a graph and let \\(u\\) and \\(v\\) be vertices in \\(G\\) . Walk: A walk from \\(u\\) to \\(v\\) is a finite sequence of adjacent vertices of \\(G\\) . Thus a walk has the form, \\(W = (v_0,v_1,v_2, \\cdots, v_n)\\) if \\(\\{v_i,v_{i+1}\\} \\in E(G)\\) for \\(0 \\leq i < n\\) , where \\(v_0 = u\\) and \\(v_n = v\\) . The length of a walk is the number of edges in \\(W\\) . Closed Walk: A closed walk is a walk that starts and ends at the same vertex, where \\(v_0 = v_n\\) . Path: A path is a walk without repeated vertices. The path of order \\(n \\geq 1\\) is denoted by \\(P_n\\) . Cycle: A cycle is a closed walk of at least \\(3\\) or more vertices. The cycle of order \\(n \\geq 3\\) is denoted by \\(C_n\\) . ::: ::: A path, \\(P_n\\) , where \\(n \\geq 1\\) , consist of \\(n\\) vertices and \\(n-1\\) edges. ::: center ::: A cycle, \\(C_n\\) , where \\(n \\geq 3\\) , consists of \\(n\\) vertices and \\(n\\) edges. ::: center ::: ::: exampleT Example 2.5 . Suppose we have the graph below, define the following walks: ::: center ::: ::: multicols 2 \\((v_0,v_1,v_2)\\) is a path, \\(P_3\\) , as neither vertices nor edges are repeated. ::: center ::: \\((v_2,v_6,v_4,v_5,v_1,v_2)\\) is a cycle, \\(C_5\\) , as we do not repeat a vertex nor edge, but started and ended at the same vertex. ::: center ::: ::: ::: We can now define what we mean by the diameter of a graph \\(G\\) . Note this will get a bit confusing. First, let's start with the distance between two vertices \\(u\\) and \\(v\\) . ::: dBox ::: definitionT Definition 2.17 (Graph distance). The distance between \\(u\\) and \\(v\\) is the minimum length of the paths in \\(G\\) connecting them, denoted by \\(d_G(u,v)\\) or \\(d(u,v)\\) , if \\(G\\) is clear from context. ::: ::: ::: list The distance between \\(u\\) and \\(v\\) is the same regardless of the start position, such that \\(d(u,v) = d(v,u)\\) . You can think of the distance as the number of edges traversed. ::: So in theory, what does this exactly mean? Let's use a simple graph for now. ::: center ::: The distance between \\(u\\) and \\(v\\) is \\(d(u,v) = 2\\) , as that's the only path to traverse. Another example, let's try and use the graph from before to see if you fully understand the definition. ::: center ::: As you can see, there are many possible paths from \\(u\\) to \\(v\\) , shown in red. Some examples are: ::: center ::: However, remember that we are only interested in the shortest path, which in this case is \\(d(u,v) = 3\\) . It's really important that you understand how to define the shortest path in \\(G\\) given two vertices \\(u\\) and \\(v\\) , as it will help in understand the next definition. ::: dBox ::: definitionT Definition 2.18 (Graph diameter). The diameter of a graph \\(G\\) is defined as ( \\(\\mathop{\\mathrm{diam}}(G) = \\max\\{d_G(v,w) \\mid v,w \\in V(G)\\}\\) \\) Equivalently, the largest number of vertices which must be traversed in order to travel from one vertex to another. ::: ::: ::: exampleT Example 2.6 . Consider the following graph \\(G\\) , determine the diameter of the graph: ::: center ::: We can start off by focusing on vertex \\(a\\) : ::: center ::: Then on vertex \\(b\\) : ::: center ::: Repeat for the rest of the vertices, for all \\(v,w \\in V(G)\\) . If you did it properly, you will see that \\(d(a,d)\\) and \\(d(f,e)\\) have the maximum distance. ( \\(\\mathop{\\mathrm{diam}}(G) = 3\\) \\) ::: You can also refer to this [video]{.underline} if you want a visual explanation of this example. -3ex -0.1ex -.4ex 0.5ex .2ex Subgraphs and Induced Subgraphs As the name suggests, the prefix \\\"sub\\\" refers to it being subsets of another graph. ::: dBox ::: definitionT Definition 2.19 (Subgraph). A graph \\(H\\) is said to be a subgraph of a graph \\(G\\) , written \\(H \\subseteq G\\) , if, and only if, \\(V(H) \\subseteq V(G)\\) and \\(E(H) \\subseteq E(G)\\) . The graph \\(H\\) is a spanning subgraph of \\(G\\) if \\(V(H) = V(G)\\) . ::: ::: It may be easier to understand visually. Suppose we have the following graph \\(G\\) : ::: center ::: As an example, each of the graphs are variations of graph \\(H\\) , which are subgraphs of graph \\(G\\) : ::: center ::: We only consider it to be spanning subgraph if and only if \\(V(H) = V(G)\\) , which in this case, if \\(V(H) = \\{a,b,c,d,e,f,g,h,i,j\\}\\) , which none of them are. On the other hand, these graphs are spanning subgraphs of \\(G\\) : ::: center ::: A final concept is induced subgraphs, which consists of the following property. ::: dBox ::: definitionT Definition 2.20 (Induced subgraph). If \\(S \\subseteq V(G)\\) , then the subgraph of \\(G\\) induced by \\(S\\) , denoted by \\(G[S]\\) , has vertices \\(S\\) and edges are those of \\(G\\) with endpoints in \\(S\\) . ::: ::: Note that none of the subgraphs shown so far are considered induced subgraphs of \\(G\\) . Though, we can modify it slightly to make it an induced subgraph, indicated by the red line. ::: center ::: Just focus on the induced subgraph in the far left. Notice how every possible edge that exists in graph \\(G\\) between the vertices, \\(\\{b,d,e,f,g,h,i,j\\}\\) , exists in this subgraph, thus making it an induced subgraph. Likewise, how the edge \\(a,b\\) is not here because \\(a\\) is not in the subset of vertices. -3ex -0.1ex -.4ex 0.5ex .2ex Special Graphs We consider some important examples of graphs. One important class of graphs consists of those that do not have any loops or parallel edges. ::: dBox ::: definitionT Definition 2.21 (Simple graph). A simple graph is a graph that does not have any loops or parallel edges. In a simple graph, an edge with endpoints \\(v\\) and \\(w\\) is denoted \\(\\{v,w\\}\\) . ::: ::: The following graphs can be depicted as simple graphs: ::: center ::: Another important class of graphs consists of those that are \"complete\" in the sense. ::: dBox ::: definitionT Definition 2.22 (Complete graph). A complete graph on \\(n\\) vertices, denoted \\(K_n\\) , is a simple graph with \\(n\\) vertices and exactly one edge connecting each pair of distinct vertices. ::: ::: This may sound confusing at first, but look at the following graphs below to get a general idea of how it works. ::: center ::: Then another class of graphs we can consider are complete bipartite graphs, which are as follows. ::: dBox ::: definitionT Definition 2.23 (Complete bipartite graph). A complete bipartite graph, denoted \\(K_{m.n}\\) , is a simple graph that has its vertex set partitioned into two subsets of \\(m\\) and \\(n\\) vertices. ::: ::: Note the difference between \\(K_5\\) and \\(K_{3,2}\\) , where there are no edges connected between \\(v_1\\) , \\(v_2\\) and \\(v_3\\) or similarly with \\(w_1\\) and \\(w_2\\) , which would otherwise just make it a complete graph. ::: center ::: The dashed-lines highlights the partition of two subsets. If there are some edges not present between the parts, then the graph is just a bipartite graph. -4ex -1ex -.4ex 1ex .2ex Functions If you think about it, functions are a special kind of binary relation. By definition: ::: dBox ::: definitionT Definition 2.24 (Function). A function \\(f\\) is a binary relation from sets \\(X\\) and \\(Y\\) . For each \\(x \\in X\\) , there is a unique \\(y \\in Y\\) , so that \\(x \\mathrel{f}y\\) . We write \\(f(x) = y\\) for \\(x \\mathrel{f}y\\) and say \\\" \\(f\\) of \\(x\\) equals \\(y\\) .\\\" Denoted as \\(f: X \\to Y\\) , it is a relation from \\(X\\) , the domain of \\(f\\) , to \\(Y\\) , the co-domain of \\(f\\) . ::: ::: Using an arrow diagrams, we can define a function by the following: Every element of \\(X\\) has an arrow that points to an element in \\(Y\\) . No element of \\(X\\) has two arrows that points to two different elements in \\(Y\\) . We can also define the range of a function \\(f\\) , which is a subset of the co-domain. ::: dBox ::: definitionT Definition 2.25 (Range). Let \\(f: X \\to Y\\) be a function. The range of \\(f\\) is: ( \\(\\{y \\in Y \\mid \\text{for some } x \\in X, f(x) = y\\}\\) \\) ::: ::: ::: exampleT Example 2.7 . Suppose a function \\(f\\) is defined from \\(X\\) to \\(Y\\) by the following arrow diagram: ::: center ::: The domain of \\(f = \\{a,b,c\\}\\) and co-domain of \\(f = \\{1,2,3,4\\}\\) . The range of \\(f\\) equals \\(\\{2,4\\}\\) . ::: -3ex -0.1ex -.4ex 0.5ex .2ex Functions Acting on Sets You can consider the set of images in \\(Y\\) of all the elements in a subset of \\(X\\) and the set of inverse images in \\(X\\) of all the elements in a subset of \\(Y\\) . ::: dBox ::: definitionT Definition 2.26 (Image and Inverse Image). Let \\(f: X \\to Y\\) be a function and \\(A \\subseteq X\\) and \\(C \\subseteq Y\\) . The image of \\(A\\) , denoted by \\(f(A)\\) , is ( \\(f(A) = \\{y \\in Y \\mid \\text{for some } x \\in A, f(x) = y\\}\\) \\) The inverse image of \\(C\\) , denoted by \\(f^{-1}(C)\\) , is ( \\(f^{-1}(C) = \\{x \\in X \\mid f(x) \\in C\\}\\) \\) ::: ::: Using the example from before, it might be easier to understand what these definition represent: \\(f(a) = 2\\) , \\(f(b) = 4\\) , and \\(f(c) = 2\\) \\(f^{-1}(1) = \\varnothing\\) , \\(f^{-1}(2) = \\{a,c\\}\\) , \\(f^{-1}(3) = \\varnothing\\) , and \\(f^{-1}(4) = \\{b\\}\\) Note that \\(C\\) is a subset of \\(Y\\) , so you maybe asked to find the inverse image of more than one element. For example, let \\(C = \\{2,4\\}\\) , then \\(f^{-1}(C) = \\{a,b,c\\}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex One-to-One, Onto and Inverse Functions We'll now discuss two important properties that functions may satisfy: the property of being one-to-one and the property of being onto. ::: dBox ::: definitionT Definition 2.27 (One-to-one). A function \\(f: X \\to Y\\) is one-to-one (or injective ) if for all \\(x_1,x_2 \\in X\\) , such that \\(x_1 \\neq x_2\\) , we have \\(f(x_1) \\neq f(x_2)\\) . If any two distinct elements of \\(X\\) are sent to two distinct elements of \\(Y\\) , then it is one-to-one. ::: center ::: ::: ::: ::: dBox ::: definitionT Definition 2.28 (Onto). A function \\(f: X \\to Y\\) is one-to-one (or surjective ) if for all \\(y \\in Y\\) , there exists \\(x \\in X\\) , such that \\(f(x) = y\\) . If each elements of \\(Y\\) equals \\(f(x)\\) for at least one \\(x\\) in \\(X\\) , then it is onto. ::: center ::: ::: ::: For finite sets, it is pretty easy to determine whether a function is one-to-one or onto, just from the arrow diagram above. The tricky part comes when analyzing a function for an infinite set. ::: exampleT Example 2.8 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = 4x - 1\\) . Is \\(f\\) is onto? In order to prove something is onto, we need show that there exists a real number \\(x\\) , such that \\(y = 4x-1\\) . We can do so, by solving for \\(x\\) in this case, where \\(x = (y - 1)/4\\) . If you notice, \\(y\\) is not restricted to anything, meaning that \\(y\\) can be any \\(\\mathbb{R}\\) . Equivalently, we are saying \\\"There exists an \\(x\\) (which we determined from 3.), which is being mapped \\(\\forall y \\in \\mathbb{R}\\) .\\\" ::: ::: exampleT Example 2.9 . Let \\(f: \\mathbb{R} \\to \\mathbb{R}\\) be defined by \\(f(x) = x^2\\) . Is \\(f\\) one-to-one? Consider \\(f(x_1) = f(x_2)\\) , We need to show that \\(x_1 = x_2\\) . If we fail to prove this, then it is not one-to-one. Alternatively, \\((2)^2 = (-2)^2 = 4\\) , but \\(2 \\neq -2\\) , so the function is not one-to-one. ::: There also exist functions which satisfy both properties discussed. ::: dBox ::: definitionT Definition 2.29 (One-to-one correspondence). A function \\(f: X \\to Y\\) is bijective (or bijective) if it is both one-to-one and onto. ::: center ::: ::: ::: This will aid us in defining a type of function known as the inverse function, which undoes the action of \\(f\\) . It sends each element of \\(Y\\) back to the element of \\(X\\) where it came from. ::: dBox ::: definitionT Definition 2.30 (Inverse function). Let \\(f: X \\to Y\\) be a one-to-one correspondence. The inverse function of \\(f\\) is denoted by \\(f^{-1}\\) , where \\(f^{-1}: Y \\to X\\) . ::: center ::: ::: ::: Obtaining the inverse function should be something you are all familiar with, it just been described in a different setting, which is by solving for \\(x\\) . ::: exampleT Example 2.10 . Is \\(y = 4x-1\\) a bijection (or a one-to-one correspondence) from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) ? If so, find the inverse \\(f^{-1}(x)\\) . To check if it is one-to-one, show that \\(f(x_1) = f(x_2) \\Longrightarrow x_1 = x_2\\) . The function is one-to-one, since \\(4x_1 - 1 = 4x_2 - 1 \\Longrightarrow x_1 = x_2\\) . To check if it is onto, we must show that for every \\(y \\in \\mathbb{R}\\) , there is an \\(x \\in \\mathbb{R}\\) , such that \\(y = f(x)\\) . We can prove this, by solving for \\(x\\) . ( \\(x = \\frac{y+1}{4}\\) \\) There exists some \\(x \\in \\mathbb{R}\\) , such that \\(f(x) = y\\) , which makes the function is onto. To get the inverse function, we can use the equation we obtained in the previous one and interchange \\(x\\) and \\(y\\) : ( \\(y = f^{-1}(x) = \\frac{x+1}{4}\\) \\) :::","title":"Relations and Functions"},{"location":"W2022/MTH314/MTH314/#number-theory-and-combinatorics","text":"-4ex -1ex -.4ex 1ex .2ex Elementary Number Theory The underlying content of this section consists of properties of integers, rational numbers, and real numbers. -3ex -0.1ex -.4ex 0.5ex .2ex Rational Number Sums, differences, and products of integers are integers, but most quotients of integers are not integers, rather known as: ::: dBox ::: definitionT Definition 3.1 (Rational Number). A real number \\(r\\) is rational, if and only if, it can be expressed as a quotient of two integers with a nonzero denominator. ( \\(r \\text{ is rational } \\Leftrightarrow \\exists \\text{ integers } a \\text{ and } b \\text{ such that } r = \\frac{a}{b} \\text{ and } b \\neq 0\\) \\) ::: ::: Some examples of rational numbers are \\(0\\) , \\(1\\) , and \\(1/3\\) . While a real number that is not rational is called an irrational numbers, like \\(\\pi\\) , \\(e\\) , and \\(\\sqrt{2}\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Parity Number The parity of an integer focuses on the property of a number being even or odd. ::: dBox ::: definitionT Definition 3.2 (Parity). An integer \\(x\\) is even if \\(x = 2a\\) , for some integer \\(a\\) . An integer \\(x\\) is odd if \\(x = 2b + 1\\) , for some integer \\(b\\) . ::: ::: ::: tBox ::: theoremeT Theorem 3.1 (Properties of parity). Let \\(x\\) and \\(y\\) be integers: If \\(x\\) and \\(y\\) are both, then so are \\(x + y\\) and \\(xy\\) . If \\(x\\) is even and \\(y\\) is odd, then \\(x + y\\) is odd. If \\(x\\) is odd and \\(y\\) is odd, then \\(x + y\\) is even. We have that \\(x\\) is even if and only if \\(x^2\\) is even. ::: ::: Alternatively, the arithmetic on the even and odd numbers can depicted as: Even \\(+\\) Even \\(\\to\\) Even Even \\(+\\) Odd \\(\\to\\) Odd Odd \\(+\\) Odd \\(\\to\\) Even Even \\(\\times\\) Even \\(\\to\\) Even Even \\(\\times\\) Odd \\(\\to\\) Even Odd \\(\\times\\) Odd \\(\\to\\) Odd -4ex -1ex -.4ex 1ex .2ex Divisors Divisors play a central role in number theory, as they help us define prime numbers and the Euclidean algorithm, which will discuss after. ::: dBox ::: definitionT Definition 3.3 (Divisibility). The notation \\(a \\mid b\\) is read \\\" \\(a\\) divides \\(b\\) \\\", if \\(b = ak\\) , for some integer \\(k\\) . We can say that ::: description \\(b\\) is a multiple of \\(a\\) , or \\(b\\) is divisible by \\(a\\) , or \\(a\\) is a factor of \\(b\\) , or \\(a\\) is a divisor of \\(b\\) ::: ::: ::: One useful trick for checking divisibility when it comes to large numbers is by checking if the sum of its individual digit is divisible by instead. Refer to the example below. ::: exampleT Example 3.1 . Is \\(94\\;417\\;898\\;732\\) divisible by \\(9\\) ? Let's start calculating the sum of its digits ( \\(9+4+4+1+7+8+9+8+7+3+2 = 62\\) \\) Since there exist no integers \\(k\\) which satisfy the following equation, \\(62 = 9k\\) , it is not divisible by \\(9\\) . ::: Following this, we can now define what the greatest common divisor of two integers is. ::: dBox ::: definitionT Definition 3.4 (Greatest common divisor). The greatest common divisor of nonzero integers \\(a\\) and \\(b\\) , denoted \\(\\gcd(a,b)\\) , is the largest integer that divides both \\(a\\) and \\(b\\) . ::: ::: Note that every integer divides \\(0\\) , since \\(0 = a \\times 0\\) where \\(k = 0\\) . ::: exampleT Example 3.2 . Find the \\(\\gcd(72,63)\\) : The divisor of \\(72\\) are \\(\\{1,2,3,6,8,9,12,18,24,36,72\\}\\) . The divisor of \\(63\\) are \\(\\{1,3,7,9,21,63\\}\\) . The largest integer that divides both integers is \\(9\\) , such that \\(9 \\mid 72\\) and \\(9 \\mid 63\\) . Hence, \\(\\gcd(72,63) = 9\\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Prime Numbers We can also use divisors as a way to define a prime number. ::: dBox ::: definitionT Definition 3.5 (Prime number). A number \\(p > 1\\) is prime if, and only if, its only positive integer divisors are \\(1\\) and itself. Otherwise, it's composite. ::: ::: The most comprehensive statement about divisibility of integers is contained in the factorization of integers theorem. ::: tBox ::: theoremeT Theorem 3.1 (Fundamental Theorem of Arithmetic). Every integer \\(n>1\\) equals a product of primes, which is unique up to the ordering of factors. ::: ::: For example, \\(72\\) can be written as, \\(2^33^2\\) , where \\(2\\) and \\(3\\) are prime numbers. In a way you can think of each number as made up of building blocks of prime number. -3ex -0.1ex -.4ex 0.5ex .2ex Euclidean Algorithm The Euclidean algorithm provides us a simpler method for deriving the greatest common divisor of two positive integers. It is based on these two key facts: If \\(r\\) is a positive integer, then \\(\\gcd(r,0) = r\\) . If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , and if \\(q\\) and \\(r\\) are integers such that \\(a = bq + r\\) , then \\(\\gcd(a,b) = gcd(b,r)\\) The first fact should be fairly straightforward to understand. The second fact might be harder to understand, so let's use an example. ::: exampleT Example 3.3 . Find the \\(\\gcd(72,63)\\) : Let \\(a = 72\\) and \\(b = 63\\) , then we rewrite it in the form of \\(72 = 63q + r\\) . You can think of \\(q\\) as the quotient and \\(r\\) as the remainder. ( \\(72 = 63(1) + 9\\) \\) Then \\(\\gcd(72,63) = \\gcd(63,9)\\) . Let \\(a = 63\\) and \\(b = 9\\) , where \\(63 = 9q + r\\) . ( \\(63 = 9(7) + 0\\) \\) Then \\(\\gcd(63,9) = \\gcd(9,0)\\) . Using the first key fact, we know \\(\\gcd(9,0) = 9\\) . ::: Alternatively, we can set \\(q = 1\\) , where \\(r = a - b\\) . ::: tBox ::: theoremeT Theorem 3.1 (Reducing gcd). If \\(a\\) and \\(b\\) are any integers, such that \\(a,b \\neq 0\\) , then: ( \\(\\gcd(a,b) = \\gcd(b, a-b)\\) \\) ::: ::: ::: list Keep in mind, \\(\\gcd(a,b) = \\gcd(b,a)\\) , so alternatively can be written as \\(\\gcd(a, b-a)\\) . Also note that \\(\\gcd(a,b) = \\gcd(|a|,|b|)\\) . ::: Using this theorem, we can continue to reduce greatest common divisor, until we either have an integer simple enough to work with or it results in the form of \\(\\gcd(r,0)\\) . -4ex -1ex -.4ex 1ex .2ex Linear Diophantine Equation We focus on equations with integer coefficients and integer solutions. ::: dBox ::: definitionT Definition 3.6 (Linear Diophantine equation (LDE)). An equation of the form \\(ax + by = c\\) , or can also be written as \\(ax = c\\ (\\mathrm{mod}\\ b)\\) , where \\(a\\) , \\(b\\) , \\(c\\) , \\(x\\) , \\(y \\in \\mathbb{Z}\\) . ::: ::: In this section, we try to answer the following problem. ::: problem Problem 3.1 . Given an integer \\(a\\) , \\(b\\) , and \\(c\\) , does a solution \\((x,y)\\) exist and how can you find a solution to an LDE? ::: So how exactly can we prove whether a solution exists? It all relates back to the greatest common divisor, more specifically \\(\\gcd(a,b)\\) . A useful theorem which we'll use in proving this is B\u00e9zout's identity. ::: tBox ::: theoremeT Theorem 3.1 (B\u00e9zout's identity). Let \\(a\\) and \\(b\\) be nonzero integers, and let \\(d = \\gcd(a,b)\\) . Then there exist integers \\(m\\) and \\(n\\) that satisfy: ( \\(ma + nb = d\\) \\) ::: ::: So for an LDE to have a solution, \\(c\\) must be a multiple of \\(d\\) , denoted as \\(d \\mid c\\) , which can be written more formally as: ::: tBox ::: theoremeT Theorem 3.1 (Check if solution exists for LDE). Let \\(d = \\gcd(a,b)\\) . The LDE \\(ax + by = c\\) has a solution if and only if \\(d \\mid c\\) . ::: ::: ::: exampleT Example 3.4 . Does a solution exists to the LDE \\(60x + 33y = 9\\) ? Compute the greatest common divisor of \\(60\\) and \\(33\\) . ( \\(\\gcd(60,33) = \\gcd(33,27) = \\gcd(27,6) \\gcd(6,3) = \\gcd(3,0) = 3\\) \\) Determine whether a solution exists, if \\(\\gcd(12,8) = 4 \\mid 68\\) or there exists an integer \\(k\\) , where ( \\(3 \\mid 9 \\Longleftrightarrow 9 = 3k\\) \\) which is true for \\(k = 3\\) , thus a solution exist. ::: Once it's determined a solution exists for the LDE, we want to way to derive the general solution \\((x,y)\\) , which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Solutions to LDE). Let \\(d = \\gcd(a,b)\\) where \\(a \\neq 0\\) and \\(b \\neq 0\\) . If \\((x,y) = (x_0,y_0)\\) is a solution to the LDE \\(ax + by = c\\) , then all solutions are given by ( \\(x = x_0 + \\frac{b}{d}t \\qquad y = y_0 - \\frac{a}{d}t\\) \\) for all \\(t \\in \\mathbb{Z}\\) . We may write the solution set as ( \\(\\Big\\{\\Big(x_0 + \\frac{b}{d}t\\Big), \\Big(y_0 - \\frac{a}{d}t\\Big) : t \\in \\mathbb{Z}\\Big\\}\\) \\) ::: ::: ::: exampleT Example 3.5 . Solve the following LDE \\(60x + 33y = 9\\) or \\(60x = 9\\ (\\mathrm{mod}\\ 33)\\) . As we proved before, a solution exists where \\(d = \\gcd(60,33) = 3\\) . First step is a finding a solution to B\u00e9zout's identity, where there exists integers \\(m\\) and \\(n\\) that satisfy \\(am + bn = d\\) . ( \\(60m + 33n = 3\\) \\) such that \\(m = 5\\) and \\(n = -9\\) satisfies this equation. Refer to the section after. Then to get \\(x_0\\) and \\(y_0\\) , we multiply \\(m\\) and \\(n\\) by \\(3\\) to get the original LDE equation. ( \\(3\\big[60n + 33n = 3\\big] = 60(3n) + 33(3n) = 9\\) \\) such that \\(x_0 = 15\\) and \\(y_0 = -27\\) satisfies the equation \\(60x_0 + 33y_0 = 9\\) . Finally, we just apply the theorem to get the general solution for all \\(t \\in \\mathbb{Z}\\) ( \\(x = 15 + \\frac{33}{3}t = 15 + 11t \\qquad y = -27 - \\frac{60}{3}t = -27 - 20t\\) \\) ::: -3ex -0.1ex -.4ex 0.5ex .2ex Solving for Initial Solution The trickiest part is solving for integers \\(m\\) and \\(n\\) which satisfies B\u00e9zout identity to get the initial solutions \\(x_0\\) and \\(y_0\\) . Let's use the previous example, \\(60m + 33n = 9\\) . The process involves by working backwards through your steps in the Euclidean Algorithm: \\(60 = 33(1) + 27\\) \\(33 = 27(1) + 6\\) \\(27 = 6(4) + 3\\) Reformat the Euclidean Algorithm, such that \\(r = a - bq\\) : \\(27 = 60 - 33(1) \\hfill (3.5)\\) \\(6 = 33 - 27(1) \\hfill (3.6)\\) \\(3 = 27 - 6(4) \\hfill (3.7)\\) Now use substitution. Refer to the text in red: \\( \\(3 = 27 - {\\color{red}6}(4)\\) \\) \\( (3 = 27 - \\big {\\color{red}33 - 27(1)}\\big \\tag*{Substitute \\(6\\) using Eq. \\(3.6\\) }\\) \\) \\( \\(3 = 27 - 33(4) + 27(4) \\tag*{Expand}\\) \\) \\( \\(3 = {\\color{red}27}(5) - 33(4) \\tag*{Combine like terms}\\) \\) \\( (3 = \\big {\\color{red}60 - 33(1)}\\big - 33(4) \\tag*{Substitute \\(27\\) using Eq. 3.5}\\) \\) \\( \\(3 = 60(5) - 33(5) - 33(4) \\tag*{Expand}\\) \\) \\( \\(3 = 60(5) - 33(9) \\tag*{Combine like terms}\\) \\) \\( \\(3 = 60(5) + 33(-9)\\) \\) -4ex -1ex -.4ex 1ex .2ex Congruence In number theory, congruence is nothing more than a statement about divisibility. ::: dBox ::: definitionT Definition 3.7 (Congruence). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) , if \\(a\\) is congruent to \\(b\\) modulo \\(n\\) , then we write ( \\(a \\equiv b\\ (\\mathrm{mod}\\ n)\\) \\) which provides that \\(n \\mid (a - b)\\) . ::: ::: So what information can we take away from \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) ? \\(a\\) and \\(b\\) have the same remainder when divided by \\(n\\) \\(a = kn + b\\) for some integer \\(k\\) \\(n \\mid (a-b)\\) There are also some useful algebraic properties of congruences. ::: tBox ::: theoremeT Theorem 3.1 . If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(c \\equiv d \\ (\\mathrm{mod}\\ n)\\) , then: \\(a + c \\equiv b + d \\ (\\mathrm{mod}\\ n)\\) \\(a - c \\equiv b - d \\ (\\mathrm{mod}\\ n)\\) \\(ac \\equiv bd \\ (\\mathrm{mod}\\ n)\\) ::: ::: The algebra of congruence is sometime referred to as clock arithmetic. For example, we can represent modulo \\(12\\) as a clock (where \\(0\\) represents \\(12\\) ). ::: center ::: The clock demonstrate that every integer is congruent to at least one of \\(0 \\dots 11\\) modulo \\(12\\) (row highlighted in pink). Just like a clock, when we go over \\(12\\) , we start over at \\(1\\) , and so the same thing applies with modulo, where \\(1 \\equiv 13 \\ (\\mathrm{mod}\\ 12)\\) . -3ex -0.1ex -.4ex 0.5ex .2ex Congruence Class Refer back to [section 2.1.2]{.underline} for a recap. Congruence is another type of equivalence relationsa relation that satisfies all three: Reflexive: \\(a \\equiv a \\ (\\mathrm{mod}\\ n)\\) Symmetric: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) , then \\(b \\equiv a \\ (\\mathrm{mod}\\ n)\\) Transitive: If \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) and \\(b \\equiv c \\ (\\mathrm{mod}\\ n)\\) , then \\(a \\equiv c \\ (\\mathrm{mod}\\ n)\\) As a result, we can form equivalence classes, or otherwise known as ::: dBox ::: definitionT Definition 3.8 (Congruence class). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . The congruence class of modulo \\(n\\) is ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b \\equiv a \\ (\\mathrm{mod}\\ n)\\}\\) \\) Note that ( \\(_n = \\{b \\in \\mathbb{Z} \\mid b = a + kn \\text{ for } k \\in \\mathbb{Z}\\}\\) \\) ::: ::: For example, in congruence modulo \\(2\\) , we have \\([0]_2 = \\{0, \\pm 2, \\pm 4, \\pm 6, \\cdots\\}\\) \\([1]_2 = \\{\\pm 1, \\pm 3, \\pm 5, \\pm 7, \\cdots\\}\\) The congruence classes of \\(0\\) and \\(1\\) are, respectively, the sets of even and odd integers. ::: tBox ::: theoremeT Theorem 3.1 (Equality of congruence classes). Let \\(a\\) , \\(b\\) , \\(n \\in \\mathbb{Z}\\) with \\(n > 0\\) . We then have that \\(a \\equiv b \\ (\\mathrm{mod}\\ n)\\) if and only if ( \\(_n = [b]_n\\) \\) ::: ::: Referring back to the clock diagram, in congruence modulo \\(12\\) , we have: \\( \\([0]_{12} = [12]_{12} = [24]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\} = \\{\\cdots -24, -12, 0, 12, 24, \\cdots\\}\\) \\) You may have notice that the distinct congruence classes are \\([0], [1], \\cdots, [11]\\) : \\([0]_{12} = \\{0, 0 \\pm 12, 0 \\pm 24, \\cdots\\}\\) \\([1]_{12} = \\{1, 1 \\pm 12, 1 \\pm 24, \\cdots\\}\\) \\(\\vdots\\) \\([11]_{12} = \\{11, 11 \\pm 12, 11 \\pm 24, \\cdots\\}\\) In congruence modulo \\(n\\) , we can say for \\(n > 2\\) , the distinct congruence classes are \\([0], [1], \\cdots, [n-1]\\) . -4ex -1ex -.4ex 1ex .2ex Principles of Counting We'll start off with the fundamentals, that is counting. Of course, most people know how to count, but combinatorics applies mathematical operations to count quantities that are much too large to be counted the conventional way. -3ex -0.1ex -.4ex 0.5ex .2ex Sum Rule Combinatorics is often concerned with how things are arrangeda way objects could be grouped. One of the most basic rules regarding arrangements is the rule of sum. ::: tBox ::: theoremeT Theorem 3.1 (Sum rule). Suppose that we are given disjoint sets \\(X\\) and \\(Y\\) . If \\(|X| = m\\) and \\(|Y| = n\\) , then ( \\(|X\\ \\cup\\ Y | = m + n\\) \\) ::: ::: Then we can generalized this theorem for more than two disjoints sets. ::: tBox ::: theoremeT Theorem 3.1 (Generalized sum rule). If we are given pairwise disjoint sets \\(X_i\\) , where \\(1 \\leq i \\leq m\\) , so that \\(|X_i| = m\\) , then ( \\(\\bigg|\\bigcup\\limits_{i=1}^m X_i \\bigg| = \\sum_{i=1}^m |X_i|\\) \\) ::: ::: So what about sets that are not disjoint? For example, we have two sets \\(X = \\{1,2,3\\}\\) and \\(Y = \\{2,3,4\\}\\) . If we use the sum rule, where \\(|X| = 3\\) and \\(|Y| = 3\\) , we should get: \\( \\(|X| + |Y| = 3 + 3 = 6\\) \\) However, \\(X\\ \\cup\\ Y = \\{1,2,3,4\\}\\) , where \\(|X \\cup Y| = 4 \\neq 6\\) , such that \\( \\(|X\\ \\cup\\ Y| < |X| + |Y|\\) \\) which brings up the next theorem. ::: tBox ::: theoremeT Theorem 3.1 (Boole's inequality). For any sets \\(A_i\\) , we have that ( \\(\\bigg|\\bigcup\\limits_{i=1}^m A_i \\bigg| \\leq \\sum_{i=1}^m |A_i|\\) \\) ::: ::: Equivalently, we can rewrite the sum rule as \\(|A\\ \\cup\\ B| = |A| + |B| - |A\\ \\cap\\ B|\\) , where we subtract the cardinality of elements that are common. For disjoint sets that is \\(\\varnothing\\) , compared to joint sets, resulting in \\(\\leq\\) . And so this brings the final theorem which will cover in this section. ::: tBox ::: theoremeT Theorem 3.1 (Principle of Inclusion-Exclusion). Let \\(X_1, X_2, \\dots, X_n\\) be finite sets. We then have that ( \\(\\bigg|\\bigcup\\limits_{1\\leq1\\leq n} X_i \\bigg| = |X_1| + |X_2| + \\dots + |X_n|\\) \\) ( \\(\\qquad\\qquad \\:-\\;|X_1\\ \\cap\\ X_2| - |X_1\\ \\cap\\ X_3| - \\dots - |X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;|X_1\\ \\cap\\ X_2\\ \\cap\\ X_3| + |X_1\\ \\cap\\ X_2\\ \\cap\\ X_4| + \\dots + |X_{n-2}\\ \\cap\\ X_{n-1}\\ \\cap\\ X_n|\\) \\) ( \\(\\qquad\\qquad \\:+\\;(-1)^{n-1}|X_1\\ \\cap\\ X_2\\ \\cap\\ \\cdots\\ \\cap\\ X_n|\\) \\) ::: ::: We can break this principle line-by-line: Take the sum of the cardinalities of the sets, as you would in a disjoint union. Subtract off the cardinalities of the pairwise intersections of the sets Add the cardinalities of the triple intersections and so on. The signs \\((-1)^{n-1}\\) depend on the parity of the number of sets intersected. For example, if there is three set in the intersection ::: center ::: \\( \\(|A\\ \\cup\\ B\\ \\cup\\ C| = |A| + |B| + |C| - |A\\ \\cap\\ B| - |A\\ \\cap\\ C| - |B\\ \\cap\\ C| + |A\\ \\cap\\ B\\ \\cap C|\\) \\) You can also refer to the visualization below, if you have trouble understanding the reason behind it ::: center ::: -3ex -0.1ex -.4ex 0.5ex .2ex Product Rule Another basic rules regarding arrangements is the rule of product. ::: tBox ::: theoremeT Theorem 3.1 (Product rule). If a task \\(X\\) can be performed in \\(m\\) ways and a task \\(Y\\) can be performed in \\(n\\) ways, then we have that \\(X\\) and \\(Y\\) can be performed together in \\(mn\\) ways. ::: ::: One example is with cards. How many cards are in a standard deck of cards? ::: center ::: Equivalently, you can think of the suit of the card and the rank of the card as two tasksthere are \\(4\\) suits and \\(13\\) . The product rule, \\(4 \\times 13\\) , tells us there are \\(52\\) card. Likewise, we can generalize this theorem for more than two tasks. ::: tBox ::: theoremeT Theorem 3.1 (Generalized product rule). If tasks \\(X_i\\) can be performed in \\(m_i\\) ways where \\(i \\leq i \\leq n\\) , then we have ( \\(m_1m_2 \\cdots m_n\\) \\) way tasks can be performed together. ::: ::: ::: exampleT Example 3.6 . How many ways can you make a license plate with three-digit number (not including zero) and three letters? For starters, let's focus on the three-digit number. For each digit, there can be nine different ways, \\(1 \\dots 9\\) , we can choose a number. ( \\(9 \\times 9 \\times 9 = 9^3\\) \\) Then for the three letters, for each choice, there can be twenty-six different ways, \\(a \\dots z\\) , we can choose a letter. ( \\(26 \\times 26 \\times 26 =26^3\\) \\) In total, there are \\(9^326^3\\) different ways we can make them. ::: -3ex -0.1ex -.4ex 0.5ex .2ex The Pigeonhole Principle The Pigeonhole Principle is a simple, but powerful tool when counting objects. The metaphors used to describe the principle typically vary, but they all follow the same analogy of inserting a finite set into a smaller finite set. You can think of it like this, if \\(n\\) pigeons fly into \\(m\\) pigeonholes and \\(n > m\\) , then at least one hole must contain two or more pigeons. ::: center ::: ::: tBox ::: theoremeT Theorem 3.1 (The Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , then at least one container must contain more than one item. ::: ::: In hindsight, it is pretty is obvious, thus, we move immediately to applicationsranging from the totally obvious to the extremely subtle. ::: exampleT Example 3.7 . If you choose a set of three non-negative integers, must there be at least two who are both even or both odd. Yes, because we have three items and only two container (odd or even), therefore one container must contain more than one item, which could be odd or even. ::: ::: exampleT Example 3.8 . Let \\(A = \\{1,2,3,4,5,6,7,8\\}\\) . If five integers are selected form \\(A\\) , must at least one pair of the integers have a sum of \\(9\\) ? You can think of the five selected integers as the number of items, where \\(a_n\\) represent a distinct number in the set \\(A\\) . ( \\(a_1,\\ a_2,\\ a_3,\\ a_4, \\text{ and }\\ a_5\\) \\) Then, all the disjoint subsets that have a sum of \\(9\\) is our container. ( \\(\\{1,8\\},\\ \\{2,7\\},\\ \\{3,6\\}, \\text{ and}\\ \\{4,5\\}\\) \\) Applying the pigeonhole principle, because there are more items, \\(n = 5\\) , than there are containers, \\(m = 4\\) . Then at least one container must contain more than one item. In other words, at least one of the disjoint subsets will contain two distinct integers, which will have a sum of \\(9\\) . ::: A generalization of the pigeonhole principle states: ::: tBox ::: theoremeT Theorem 3.1 (Generalized Pigeonhole Principle). If \\(n\\) items are put into \\(m\\) containers, with \\(n > m\\) , there is at least one container with \\(\\lceil n/m \\rceil\\) items. ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Permutation Before going over combinations, let's talk about permutation, where order matters. You can think of it as an ordered combinations. For example, the set of elements \\(a\\) , \\(b\\) , and \\(c\\) has six permutations: \\( \\(abc \\hspace{1cm} acb \\hspace{1cm} bac \\hspace{1cm} bca \\hspace{1cm} cab \\hspace{1cm} cba\\) \\) The number of permutations can be derived using the product rule. Suppose we have a set of \\(n\\) elements: For our first choice (or task), we have \\(n\\) ways of picking an element. For our second choice, we now have \\(n-1\\) ways of picking an element \\(\\vdots\\) For our \\(n\\) th choice, there's only one element left, so we only have \\(1\\) way of choosing an element. So by the product rule, there are \\( \\(n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1 = n!\\) \\) ways to perform the entire operation with no repetitions. In other words, there are \\(n!\\) permutations of a set of \\(n\\) elements. Suppose in the previous example, we want to know how many permutation there one only using two elements, instead of all three. We can define an ordered arrangement of \\(r\\) elements taken from the set of \\(n\\) elements as an \\(r\\) -permutation. ::: dBox ::: definitionT Definition 3.9 (Permutation). The number of \\(r\\) -permutations of a set of \\(n\\) elements is denoted \\(P(n,r)\\) . If \\(0 \\leq r \\leq n\\) , then ( \\(P(n,r) = n \\times (n-1) \\times \\cdots \\times (n - r + 1) = \\frac{n!}{(n-r)!}\\) \\) ::: ::: So now can calculate the \\(2\\) -permutation of \\(\\{a,b,c\\}\\) , resulting in \\( \\(P(3,2) = \\frac{3!}{(3-2)!} = \\frac{3!}{1!} = \\frac{6}{1} = 6\\) \\) which are \\( \\(ab \\hspace{1cm} ac \\hspace{1cm} ba \\hspace{1cm} bc \\hspace{1cm} ca \\hspace{1cm} cb\\) \\) -4ex -1ex -.4ex 1ex .2ex Combination We can now define an unordered arrangement of \\(r\\) elements of a set as an \\(r\\) -combination. ::: dBox ::: definitionT Definition 3.10 (Combination). Let \\(n\\) and \\(r\\) be non-negative integers, with \\(r \\leq n\\) . The symbol \\(\\binom{n}{r}\\) , read \\\" \\(n\\) chooses \\(r\\) \\\", denotes the number of subsets of size \\(r\\) that can be formed from a set of \\(n\\) elements. ::: ::: ::: list There's two ways to denote an \\(r\\) -combination, which is by \\(\\binom{n}{r}\\) or \\(C(n,r)\\) ::: Using the relation between permutation \\(P(n.r)\\) and combination, gives us an important formula: \\( \\(C(n.r) = \\binom{n}{r} = \\frac{n!}{(n-r)!r!}\\) \\) ::: exampleT Example 3.9 . Given a set of four people: Ann, Bob, Cyd, and Dan. List all the combinations that can be made with only three people. Note that order doesn't matter, so a subset consisting of \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) is the same as the subset consisting of \\(\\{\\) Dan, Cyd, Bob \\(\\}\\) . Following this fact, then the \\(3\\) -combination can be obtained by leaving one out of the elements of the set: \\(\\{\\) Bob, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Cyd, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Dan \\(\\}\\) \\(\\{\\) Ann, Bob, Cyd \\(\\}\\) or alternatively ( \\(C(4,3) = \\binom{4}{3} = \\frac{4!}{(4-3)!3!} = \\frac{24}{6} = 4\\) \\) ::: As a follow up, there are special cases of combinations using this equation. ::: tBox ::: theoremeT Theorem 3.1 (Basic properties of combination). Let \\(n\\) be an integer: \\(\\displaystyle\\binom{n}{0} = \\binom{n}{n} = 1\\) \\(\\displaystyle\\binom{n}{1} = \\binom{n}{n-1} = n\\) \\(\\displaystyle\\binom{n}{2} = \\frac{n(n-1)}{2}\\) ::: ::: -3ex -0.1ex -.4ex 0.5ex .2ex Properties of Combinations There also some useful of identities that you can form using \\(C(n,r)\\) . They seem mysterious at first, but there's usually a good reason for them. Combinations have a recursive quality that is captured in the following theorem. ::: tBox ::: theoremeT Theorem 3.1 (Recursive property). For integers \\(n \\geq 1\\) and \\(r \\geq 1\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n - 1}{r - 1} + \\binom{n - 1}{r}\\) \\) ::: ::: You will often see this depicted as Pascal's formula. As an example, we can calculate \\(\\binom{6}{2}\\) using: \\( \\(\\binom{6}{2} = \\binom{5}{1} + \\binom{5}{2} = 5 + 10 = 15\\) \\) Another important property of combinations is their symmetry. ::: tBox ::: theoremeT Theorem 3.1 (Symmetry property). For integers \\(n \\geq 0\\) and \\(r \\geq 0\\) , with \\(r \\leq n\\) , we have that ( \\(\\binom{n}{r} = \\binom{n}{n - r}\\) \\) ::: ::: Using the previous example, we know that: \\( \\(\\binom{6}{2} = \\binom{6}{4}\\) \\) At first, it might not make sense, but it will prove to be useful in the next section, when we go over Pascal's trianglewhich is an arrangement of the combinations that makes them simple to remember. Lastly, we have this identity. ::: tBox ::: theoremeT Theorem 3.1 (Sum of squares combinations). For \\(n \\leq 0\\) , we have that ( \\(\\sum_{r=0}^n\\binom{n}{r}^2 = \\binom{2n}{n}\\) \\) ::: ::: As such, we can express the sum of squares as a single combination, shown below: \\( \\(\\sum_{r=0}^{25} \\binom{25}{r}^2 = \\binom{2 \\cdot 25}{25} = \\binom{50}{25}\\) \\) -3ex -0.1ex -.4ex 0.5ex .2ex Pascal's Triangle The rows of Pascal's triangle are indexed by non-negative integers: \\(0\\) , \\(1\\) , \\(2\\) , and etc. In the \\(n\\) th row, we have \\(\\binom{n}{r}\\) , where \\(1 \\leq r \\leq n\\) and the values of \\(r\\) increase from left to right. The following are the first seven rows of Pascal's triangle. Then, lastly we have this identity. ::: center ::: Though, they are more commonly represented as their integer counterpart. ::: center ::: You can see many patterns of how combinations are related in the triangle, such symmetry in a given row and the recursive property of combinations, which we discussed prior to this. One important use of combinations is in expanding polynomial expressions, such as \\((x + y)^n\\) . The binomial theorem generalizes this formula. ::: tBox ::: theoremeT Theorem 3.1 (Binomial theorem). Let \\(n\\) be non-negative integers and let \\(x\\) , \\(y\\) be variables. ( \\((x + y)^n = \\sum_{r = 0}^n\\binom{n}{r}x^{n-r}y^r\\) \\) ( \\(\\qquad\\quad\\ \\ = \\binom{n}{0}x^n + \\binom{n}{1}x^{n-1}y + \\binom{n}{2}x^{n-2}y^2 + \\cdots + \\binom{n}{n-1}xy^n + \\binom{n}{n}y^n\\) \\) ::: ::: In other words, the triangular arrangement of numbers gives us the coefficients in the expansion of any binomial expression. ::: exampleT Example 3.10 . Expand the following expression \\((x+y)^n\\) for \\(n=6\\) : Using the binomial theorem: ( \\((x + y)^6 = \\binom{6}{0}x^6 + \\binom{6}{1}x^5y + \\binom{6}{2}x^4y^2 + \\binom{6}{3}x^3y^3 + \\binom{6}{4}x^2y^4 + \\binom{6}{5}xy^5 + \\binom{6}{6}y^6\\) \\) If we refer back to Pascal's triangle, then we can easily substitute the binomial coefficient with its respective integers, as such: ( \\((x + y)^6 = x^6 + 6x^5y + 15x^4y^2 + 20x^3y^3 + 15x^2y^4 + 6xy^5 + y^6\\) \\) :::","title":"Number Theory and Combinatorics"}]}